#!title:    人类知觉智能

#!content

**版本记录**

- 2025-09-15：将“人类知觉智能”章节从本文独立出来，作为“问题·观点”栏目下的《人类知觉智能》单篇笔记，但逻辑上仍属于本文的一部分。这样做的目的，一是为了突出所谓AI（也就是大众理解的人工智能，包括大语言模型、计算机视觉、具身智能等等）的地位，二是控制本文篇幅，不使本文篇幅过长。

2023-12-03：前几年，在人工智能领域，有一种学科划分方式，将NLP、知识工程划分为“认知智能”，将计算机视觉、语音、触觉等涉及视听和感觉的，划分为“感知智能”。然而，近几年的研究表明，“认知”和“感知”不应分家，多模态融合才是正确的发展路线。因此本章的标题是“人类智能”，将自然语言处理、机器视觉、语音等学科，统统纳入“人类智能”的范畴，或者称为“知觉智能”。此处再次强调我关于所谓人工智能的观点：**人工智能的使命是理解世界，而不是理解人类。**

# 大规模语言模型

![ ](./image/G4/揶揄人工智能-3.jpg)

## LLM调查研究笔记

**2025-09-01**：手持电子鹦鹉自研情况如下：

- 语言模型：168M/56M/猫娘LoRA，完全自主训练，基于开源数据集和成熟大模型蒸馏语料。其余Qwen3模型权重来自官方开源，自己做了朴素Q80量化。
- 语言模型推理：基于llama2.c深度魔改，支持Q80量化推理，逐行审阅掌握，可以认为是自制。
- ASR和TTS：ASR分为推理服务和协议转换中间件两部分，前者是阿里FunASR（SenseVoice Small）官方镜像（重新打了ffmpeg进去），后者是AI写的Python代码；TTS基于小米的sherpa-onnx和MeloTTS模型，协议转换中间层Python代码是AI写的。
- GUI：框架和交互逻辑是完全自主研制，字库是从文泉驿宋体取字模而来，对于其中个别字符做了手工的雕琢。OLED驱动部分沿用厂家代码。
- 硬件：均为货架商品和现成模块，没有任何订制或自制的硬件或结构。后面可能会自己3D打印结构

**2025-08-31**：电子鹦鹉集成TTS后非常耗电，这跟我一样，说话比思考更累。另外现在melotts的onnx推理实时率在1上下徘徊，勉强能接受，但不算好。不过语音的效果还是好听的。

**2025-08-03**：浏览器WASM推理已经支持Qwen3-0.6B了。但是限于WASM的内存限制，上下文长度只有3072。速度也很慢，在Mi14U上只有3tk/s左右。

**2025-07-31**：浏览器推理也引入Q80量化了，推理速度提升50%。接下来完善量化模型的导出和加载，这样就可以突破浏览器对buffer大小的限制，就可以在浏览器上推理量化的Qwen3-0.6B了。这个稍微实用一些。

**2025-07-30**：给电子鹦鹉笼增加了Q80量化，吐字速度提升1倍。后续增加Q4KM等高效量化算法。

**2025-07-01**：近期打算训练一个1M左右参数的极小型通用语言模型，用于Scheme实现的LLM推理。在此之前，需要有一个好一些的词表。或者，干脆放弃多个字符的token，所有的token都只有1个Unicode字符，这样分词器的实现就可以退化成简单的查找表了。词表是LLM开发过程中重要的基础工作。虽然词表并不会对模型性能有决定性的影响，但是如果词表设计得不好，后期就不能改动了，只能将错就错。我今年初完成训练的56M和168M系列模型，在词表设计上有个十分蠢笨的问题：为了凑齐16384个token，我另找了5000多个完整的英文单词作为token。这显然是很低效的，因为同一个单词在实际文本中，有大小写、单复数等各种屈折形态，用完整的单词原形作为token，编码效率很低。所以合理的做法应当是寻找高频词根词缀，以词根词缀作为token。或者更进一步，像BPE那样，无视字母，直接在字节层面上寻找高频模式。不过，我不想在词表上花费太多的心思。

**2025-07-01**：评ERNIE4.5技术报告中出现的插图：感觉是在揶揄或者致敬（好像是杨立昆说的）蛋糕最顶上的那个樱桃的说法，现在看来RL远远不止樱桃那么简单，它是整个蛋糕的裱花，最见功力，给蛋糕定档次定调子的。

**2025-06-30**：自研Nano语言模型，现在可以在自研Scheme解释器上推理了！欢迎直接在网页上体验，其中精神分析黑话模型有概率输出“那句话”（人类的本质是复读机）噢。支持两个小模型，参数量分别是90k和3k。模型参数直接以baseb4的形式保存在Scheme代码中，免去跨平台读写文件的麻烦。不过，包括模型前向传播在内的核心算法，是JS实现的，Scheme只是起到了胶水的作用。之所以不用Scheme实现推理核心，是因为Scheme执行矩阵运算实在是太慢了……哪怕是PyTorch，也不会把张量运算都用Python实现吧。尽管如此，后续将逐渐把采样、词元编解码、会话管理等不太吃性能的部分，迁移到Scheme层实现。另外，关于完全用自研Scheme实现自研Nano语言模型这件事，尝试实现了几个原位算子，尤其是`matmul`算子，实测性能还可以，不是不能接受。所以现在可以开始把前向传播改写成Scheme了。反倒是分词器比较麻烦了，因为现在还没有HashMap。

**2025-06-19**：我实在是无法忍受了。我要给Animac增加`(while cond body)`的支持。目标：推理极小型的Transformer电子鹦鹉！

**2025-05-21**：关于Nano自研推理引擎和电子鹦鹉笼：①大规模重构推理引擎，更新WASM形态。② WASM和JS实现不打算适配Qwen了，一是麻烦，浏览器的ArrayBuffer有2GB限制，二是体验不佳，估计TPS应该不到3，没有适配的热情。③整理文档，固化技术状态，电子鹦鹉笼的事情就告一段落了。后面真的要开始玩OFDM了。

**2025-05-19**：开始调研在路由器上运行ASR和TTS。ASR似乎只能选用whisper.cpp，这个东西应该是有人做过了，用来在软路由/NAS上自动提取字幕。cmake编译回去还要验证。如果可行的话，llama.cpp也可行了。通过USB声卡播放音频，验证可行。至于录音，这个还要验证。或许可以把毕业设计的东西翻出来。考虑到路由器的内存实在是有限，LLM和ASR两个模型是不可能同时驻留内存的，实际使用时，只能ASR用完后把LLM装进内存，体验一定很差。所以完整的离线小爱同学不能在路由器上实现。RK3588倒是比较适合。无论是软件生态还是硬件性能还是机械尺寸还是接口配置，都比路由器合适得多。

**2025-05-17**：Nano自研推理引擎适配Qwen3-0.6B/1.7B成功。适配过程最大的麻烦在于`q_proj`和`k_proj`。

- Qwen3注意力头的长度以kv头为基准（128），kv向量的长度等于`dim`。而Qwen2.5以前是以q头为基准。我觉得Qwen3确实更合理，但过往的实现不太好改了，姑且维持现状。
- `q_proj`和`k_proj`的排列顺序与过往模型都不同。过往模型把实部和虚部合在一起，按照复数二元组的顺序排列。而Qw3将实部虚部分开，按照前半部分实部，后半部分虚部的顺序存储。这一点给我带来了巨大麻烦。为了排查问题，逐个算子排查输入输出，跟Transformers的结果做对比，最后发现是RoPE算子没有适配这个格式。过程中一度怀疑是因为开了`-ffast-math`选项导致的数值稳定性问题。
- 在某些机器上，开`-Ofast`确实会导致模型始终解码出0号token，开`-O3`是正常的。原因尚未定位，不过暂且不管了。
- 接下来是将Qwen3的推理实现集成到现有的电子鹦鹉笼中，集成完成后，电子鹦鹉笼就实用了。

**2025-05-08**：基于雅典娜路由器的电子鹦鹉笼这件事，或许是从我2014年开始接触单片机和嵌入式以来，最大的进展和最复杂的作品。主要的突破在于：突破了BGA芯片的钎焊拆装技术。突破了OpenWrt固件编译和设备树修改技术。突破了低复杂度嵌入式UI的设计实现技术。突破了OpenWrt应用开发的基本技术。此外，通过做这个东西，积累了一些可以复用的组件：12864屏幕带软字库的驱动、解耦更合理的LLM推理引擎、九键输入法的基本框架和相关工具。

**2025-04-28**：关于带九键拼音输入法的电子鹦鹉笼，有点本末倒置的意思，像168M量级的小模型，本来可以服务于输入法的。另外以4核A53的性能，搞那么多奇怪的算法，用处并不大。不论如何，用了5年的路由器竟然可以跑大模型，还是挺让人惊喜的。

**2025-04-25**：最近终于基本完成了12864overI2C屏幕加汉字字库的驱动，可以干很多事情了。在嵌入式设备上画GUI，这件事情的复杂度比较高。虽然现在也有什么Vue在液晶屏上渲染的整活，但是放在十几二十年前，由于计算资源非常有限，做一个美观易用的GUI还是比较困难的。不过，如今，计算资源已经非常充裕了，甚至在手环上播放视频也不是什么很奇怪的事情了。唯一的限制大概就是续航和功耗问题了吧。据说某手机厂商二十年前曾经做过一个面向嵌入式系统加小液晶屏的GUI框架，主要是用在小灵通这样的手持终端上。GUI的复杂性主要在呈现和交互上。呈现方面的复杂度，主要来源于业务复杂度（参考2025-07-31灵感）。最近做大模型在液晶屏上显示，甚至需要自己处理折行之类的最基本的问题。交互方面的复杂度主要源于异步，而异步是需要大量的系统资源去支撑的，比如中断、比如高性能的事件循环等等。现在马上开始做I2C矩阵键盘的技术穿刺，只能用轮询的方式去处理按键事件。

**2025-04-23**：最近正在做的完全自持的路由器端侧大模型（电子鹦鹉笼），从技术栈来看是个真正顶天立地的东西。往下看，要懂嵌入式编程、设备驱动、电子线路、BGA芯片焊接；往上看，要懂并行加速、深度学习和大模型算法、Web前端UI开发。目标是：路由器外接16键键盘和点阵OLED，可以通过T9拼音输入法输入汉字，向路由器上的自制大模型提问，并且将输出的文字显示在OLED上。所有东西是完全自持的，不需要联网，插电即用。这个东西做完之后，把它迁移到RK3588板子上。RK3588的资源比较丰富，足以流畅运行3B量级的大模型，同时运行ASR似乎也没有压力（好的TTS压力比较大，但不是不可行），实用性就非常强了。

**2025-04-21**：驱动屏幕在我看来是最恶心的一件事。我至今都在吃十年前留下的那几坨代码。当时为什么没有留下一个靠谱的驱动I2C带字库的12864屏幕的程序啊。哦原来是没有这样的产品啊。刚买了一个，字库竟然需要单独的SPI接口从主控读，其离谱程度堪比英特尔初代胶水双核。算了，彻底摆烂了。

**2025-04-18**：雅典娜救砖，用到了一种叫9008的特殊模式，似乎是高通系SoC的救砖后门模式。原理似乎是上电开机时短接D+到GND一段时间后放开。如果这个方法可行，那么直接开干，串口也不用了。整几根USB线，这个我还是比较熟练的。

**2025-04-17**：当你遇到一个说风凉话的路人“软路由到底有什么用”“HomeLab到底有什么用”，你最好热情地指条路搭把手，因为嫌货才是买货人。

**2025-04-16**：成功实现了小米AX5路由器驱动I2C接口的LCD屏幕。

解决了复杂问题之后，自然就有那种山高人为峰的豪迈心情。回过头来看3月底至今玩路由器的历程，两三周的时间里，以黑客松的强度进行密集学习调研，从零开始建立起对于路由器和OpenWrt的基本认知，突破了刷机、固件定制和编译、LuCI应用开发、设备树修改定制和外设开发这几个里程碑，在解决问题中不断发现新问题，开辟了全新的技能领域。这完全是计划外的，但也不全是。我向来对嵌入式很感兴趣，但是从没想到竟然会以OpenWrt为起点。总而言之，这段时间的实践经验极为宝贵，是可以与Scheme解释器、MP3编码器、无线音视频传输、自制大语言模型等成果并列的重大成就，是可以自豪地写进简历的技术经历。

在此过程中，LLM的帮助比较大，但并不能直接解决问题。LLM的确是划时代的，如果没有LLM的帮助，不可能有这么高的调研效率。但是归根结底还是靠我自己看，LLM脑子里可能有内核的全部代码，但它还是茶壶里煮饺子，没法直接解决我的问题。还是要靠我自己。另外LLM不会用烙铁和热风枪。

我强烈感觉到我本科毕业之后应该找个专科读一读，专门学嵌入式开发。本升专不是开玩笑的。我现在做的事情，难度不在事情本身，而在于信息收集，没有师傅带。一旦打通了信息渠道，剩下的就是见多识广加熟能生巧，这个就是职业专家的成长途径，实质上就是职业教育，专科教育。基于我对于创新的理解，我认为创新并不是单向度的，手脑并用的“创造”也是创新。（参考2025-05-23灵感）

![ ](./image/G3/ax5-router-i2c-lcd.jpg)

**2025-04-15**：研究了几天从AX5路由器上骇一个I2C通道出来，没什么进展，主要困难如下：

- IPQ6018没有公开文档，如今看到的只言片语，都是带水印的，应该是商业秘密。（不过已经找到datasheet了，帮助很大）
- 成品PCB上完全没有预留有I2C功能的GPIO通道。现在物理上可触及的端子只有调试串口和4个LED，很遗憾，从内核中与IPQ6018相关的代码判断（这是最重要的参考资料了），都没有原生I2C功能。
- 只能利用29-32这4个PWM端口来模拟I2C总线时序。这块最大的问题在于完全不知道这几个端口是否支持开漏输出和内部上拉。如果不支持，这事就彻底不做了。如果支持，倒还可以挣扎一下，就是非常考验我的电烙铁技术。
- 刚刚找到了京东云雅典娜上控制LED点阵小屏幕的开源软件实现，非常珍贵的参考资料。最重要的是有了4个物理可触及的GPIO，软件上就可以做很多事情了。

**2025-04-12**：我花了两周时间，从在恩山上到处找别人编译好的不知道从哪来的固件的菜鸡，变成了自己随意定制裁剪编译固件的大佬。但是UBoot是真正的核心知识所在。UBoot极其依赖厂商资料，但是IPQ60XX系列芯片的资料，网上几乎找不到，只有OEM厂商才能接触到。那些做UBoot的极少数大佬，必然是从业者或者有信息渠道。壁垒在于信息源而不是技术。OpenWrt项目的初衷就是打造开源路由器生态。然而软件不管怎么开源，本质上还是依赖于硬件。硬件的黑盒性质是固有的，其复杂度无法被任何一个人类个体完全掌握。一个层次有一个层次的复杂度。追求完全可控，是不现实的。只能在某些层次上成为熟手、专家，固化并接受其他层次的黑盒状态。

**2025-04-10**：最近编译OpenWrt固件给我编麻了。路由器是最常见最廉价最简单的嵌入式系统，拿来学嵌入式软件开发再好不过了。但是问题也有很多，比如说外设和接口太少了，整活空间大大缩小。这就是开发板的生态位了。

**2025-04-08**：2025年了，我还在读人类写的文档，纯人工手写代码，还要学习LuCI这种市场层面貌似无甚前途的东西，属实有点反动了。OpenWRT的编译系统就是个大号的DSL解释器，似乎现在大多数嵌入式系统的项目例如yocto都是这样干的。通过大量的配置文件对固件和软件包作裁剪和定制。

**2025-03-25**：利用两个 AGX Orin 盒子实现分布式推理。

基于llama.cpp，盒子之间通过10G以太网通信，使用Qwen2.5-72B-q4km，推理速度约2.7tk/s(D阶段)。作为对比，单个盒子D阶段推理速度3.1tk/s左右，没有本质差异。这说明，按照llama.cpp这种把模型拦腰截断的拆分方式，盒子间通信并不是瓶颈，瓶颈依然在于每台机器上的显存带宽。

加载模型时，盒子之间传递参数，最大速度700MB/s左右，并没有打满盒子间带宽，瓶颈可能在存储。而自回归解码阶段，两个盒子都在工作，但盒子间通信仅1.5MB/s左右。估计是因为简单把模型按层拆成两半，两半之间只需要传递很少量的中间激活值和同步控制信息（每个盒子保留自己层的KV缓存），盒子间通信并不构成瓶颈。

实验结果还是很令人满意的，验证了多个盒子水平扩展以获得巨大显存的可行性，并且在现有软硬件条件下，至少不会严重劣化推理性能。归根结底，瓶颈还是在于黄老板的振金显存😡

后续验证不同显卡的机器之间分布式推理的可行性，甚至可以尝试100G互联和vLLM+Ray/sglang等方案。如果可行，我将拥有256GB的巨大显存，足以运行满血版DS-R1，但是速度和成本嘛，嘿嘿，别问，问就是迟早挂咸鱼🥺

附操作过程：

```
# 1、在两台机器上分别设置IP并启用网卡

# 机器1
sudo ip addr add 192.168.1.1/24 dev enP5p1s0f0
sudo ip link set enP5p1s0f0 up
# 机器2
sudo ip addr add 192.168.1.2/24 dev enP5p1s0f0
sudo ip link set enP5p1s0f0 up

# 2、在两台机器上安装相同版本的llama.cpp-rpc-cuda
#    具体参照：https://github.com/ggml-org/llama.cpp/tree/master/examples/rpc

# 3、在机器1上启动server
cd /home/bd4sur/ai/llama.cpp/build-rpc-cuda/bin
CUDA_VISIBLE_DEVICES=0 sudo ./rpc-server -p 50052 -H 0.0.0.0

# 4、在机器2上启动server
cd /home/bd4sur/ai/llama.cpp/build-rpc-cuda/bin
CUDA_VISIBLE_DEVICES=0 sudo ./rpc-server -p 50052 -H 0.0.0.0

# 5、在机器2上启动推理进程
sudo ./llama-cli -m /home/bd4sur/ai/_model/Qwen25/qwen2.5-72b-instruct-q4_k_m.gguf -p "人类的本质是什么？" --rpc 192.168.1.1:50052,192.168.1.2:50052 -ngl 99 -no-cnv
```

![ ](./image/G4/agx-orin-2-distributed.jpg)

![ ](./image/G4/agx-orin-2-distributed-llm-infer.jpg)

**2025-03-12**：[在 AGX Orin 上部署QwQ-32B](https://www.bilibili.com/video/BV1UHQpYmEpG)

**2025-02-17**：在vswd1上基于自己改写的llama2.c运行DeepSeek-R1-Distill-Qwen-1.5B，可以达到4.5tokens/s。

![ ](./image/G4/deepseek-v9200.jpg)

**2025-02-15**：在4卡P40/128GB-DDR4机器上跑DeepSeek-R1（1.56bit量化），可以达到1.5token/s的速度。不过调整GPU/CPU分配比例和其他参数，应该还有提升的可能性。从监控看，算力利用严重不足，看来确实是memory-bound。

**2025-02-14**：在联想拯救者R9000P（96GB内存）上运行ollama推理量化的DeepSeek-R1-Distill-Qwen-32B（Q4_k_m）可以达到8token/s以上。

**2025-02-13**：在RK3588上仅CPU运行DeepSeek-R1-Distill-Qwen-14B（Q4_k_m），回答射频常识题，非常好。速度大概2token/s。

**2024-12-30**：经过一轮预训练（256亿词元）和一轮监督微调（1288万条QA对）之后的效果：终于知道澳洲的首府在哪里了。但仅此而已，还是很蠢，没有实用价值。计划在2025年1月中旬结束训练。届时将完成两轮监督微调。后续不再折腾了。

**2024-12-24**：评[Meta的工作](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)：调教电子鹦鹉的时候就觉得词表这东西很没有智慧，没有智慧就在于它掺杂了人类对于“token”的理解。所以我自己的电子鹦鹉就没有词表的概念，就是unicode进unicode出。是时候把碳基彻底赶出硅基了。

**2024-12-20**：预训练持续40天，吃掉256亿个词元，验证集损失2.1，到此为止。今天开始，在12000000条问答语料上进行监督微调，至少一轮。

**2024-12-19**：魔改AK大佬的llama2.c，使其支持Qwen2(.5)的推理。Qwen2(.5)的模型结构，相比Llama2，只是在QKV上加了偏置，所以改动很小。另外HF的RoPE参数排列方式是实部虚部分块的，而不是交织的，需要做个转换。除此之外就没什么差别了。虽然Qwen25-0.5B也不怎么聪明，但是总比我自己训练的那个智障强多了，多少有点实用性。

**2024-12-16**：自己拍脑袋设计词表的一个严重缺陷是…我忘了还有日本新字形…我只是简单地把两个国标里面的简繁体汉字塞进词表里。当然了，训练语料里想来也没有日文。

**2024-12-14**：自制168M电子鹦鹉在BBU上跑起来了，速度可以达到20个token/s。①上面那块VSWd1比下面那块更快，可以达到40token/s，估计是因为上面跑的任务比较少，看样子应该是2020年出厂的早期产品。②交换板手动关机后似乎会自动重启，重启后会重置系统，系统应该是存储在板子上的ROM里，每次上电都会写到SSD里，从SSD启动。

**2024-12-12**：虽然现在大家的兴趣都在1G以下的小模型上，但它的价值确实是要打个问号的。既然是说人话，那么“好”还是要比“快”重要一些。鹦鹉虽然会说人话，但是没人真的想跟鹦鹉唠嗑吧。归根结底，菜是原罪。不过，在封闭世界中的某些场景下，处理一些不那么严肃的任务，比如______，我觉得还是挺有价值的。等我的____模型炼成之后，第一件事就是郑重声明：电子鹦鹉所说的任何话，本人一概不承认。

**2024-12-09**：是从头训练小模型？还是把大模型压缩成小模型？根据[这篇文章](https://arxiv.org/pdf/2412.04315)，看来，依靠剪枝蒸馏量化把大模型压缩成小模型，还是把太多的人类自作聪明的理性因素引入了大模型，反而导致模型能力密度下降。而从头训练小规模模型，让小模型自己在大规模高质量语料中学习，更有利于得到知识密度更高的小模型，接近率失真优化的极限。此中得失，需要权衡。

**2024-12-04**：这个VAR前段时间注意到了，还clone下来跑了起来，效果挺好的。因为以我的数理水平很难理解扩散模型，另外近期也有在玩自回归语言模型在NLP以外问题上的推广，所以对这个“下一分辨率生成”的自回归生成范式很感兴趣。我个人觉得这个思路很有前途，它是在频域上操作，比空域上生成patch的思路更合乎直觉。

**2024-11-28**：近年来大模型业界习惯用“B”指代十亿，而我打算杜绝这种不规范的新用法。以后凡是看到“B”这种用法，在我这里一律改成SI词头“G”（吉咖）。另外，将严格区分SI十进制词头和二进制词头。例句：168G参数的语言模型，其检查点的大小约为1.9GiB。在64GiB的显存上训练时，批大小最大可以设置为0.08k个样本。

**2024-11-28**：上月训练的56M语言模型，成本高于1克黄金，智力低下，几乎无法解决任何实际问题。终端部署是能行的，性能是可以接受的，唯一有潜力的是瑟琴机器人这种恰好需要幻觉但是创造力又很有限的场景。而现在正在训练的168M模型，已经训练了半个多月，吃掉了100多亿词元，但是远远不够。目前看到的各种数百数千M级小规模模型，包括Qwen、Phi、SmolLM、MiniCPM等等，在解决事实性问题上都不是很堪用，但是在解决通用的语义理解类问题上还是有一定用处的。监督微调归根结底还是体力劳动，我个人没法承担这么大的工作量。

**2024-11-18**：Nano的WASM推理引擎现已支持LoRA插件…因为我戴了__色的眼镜，所以看什么都是__色的～但更令人感叹的是NLP的奇妙的发展史。下图左边是2018年整的一个小活，只是一个ChatBot的静态前端界面而已，无论输入什么都只会返回一个meme图，唯一值得说的就是上古时代的RAG——点击图片会以上一句话为关键字去搜索百度。虽然当时AttentionIsAllUNeed已经发表，但是绝对无法预料人类究竟何时/能否通过图灵测试。短短6年后，我有了右边那个东西。能否通过图灵测试不知道，但是当一只____也很令人称奇了。**打造电子鹦鹉，是我试图回答“人类的本质是什么”这个难题的一个现象学的进路**。

![ ](./image/G4/nano-neko-lora.jpg)

**2024-11-17**：Nano浏览器推理现已支持WASM加速！按需加卸载LoRA模块机制稍后加入。最近研究了一点WASM，像这种栈式虚拟机长得都差不多，有意思的是适配各种运行时环境的部分，也就是对于标准库的实现那部分。今天完成的WASM推理引擎，最重要的改进是将C语言代码中手写的朴素Trie树改成了以内存池形式实现的高效实现，消除了运行时malloc，而这对于提升WASM性能是决定性的：因为观察到malloc在浏览器上性能很差，并且大部分时间都花费在GC上。看了这么久WASM汇编代码，基本上看不懂。尤其是开了最高优化之后的汇编代码，完全看不懂。我也算是一个编译器爱好者，但是我自己的编译器生成的汇编代码我自己也看不懂。另外汇编这个层面到底要侵入宿主环境多深，到底要跨越多少个抽象层次，是个很有趣的问题。我自己搞的虚拟机/指令集是可以感知到λ闭包的，这有利于减少编译器的工作量，但是虚拟机要考虑的就很多了，访问变量的开销非常大。WASM与WASI和emscripten这两层东西是分开的，这很好。很大一部分精力花在了搞清楚这两层之间的边界在哪里。浏览器上的 WASM API 给开发者留了很大的自由度，开发者可以自由操作所谓的线性内存，所以我说绝对的权力意味着绝对的责任。malloc帮我们干了很多脏活累活，但是作为他们一切的主人，我们也不能越俎代庖，专业的事情交给专业的人干。而问题在于，我是那个专业的人吗？

**2024-11-16**：现有的56M大模型，是一只合格的电子鹦鹉。它可以创造性地说出流利通顺的人话，但是并不能保证事实正确。其语言能力大概相当于刚学会说话不久的小朋友。但这并非没有价值。在______场景下，它的确能复读出语料中那些____的话，但同时因为模型规模较小、以及训练不充分的缘故，它似乎不能完全沉浸在____的语境中，往往会说出很出戏的话。所以与其说是____，不如说是搞笑，有种难以言喻的诙谐感🤣。作为极小规模的胡言乱语机器人是合格的。如果训练得当，放在某些大型游戏里，用Lua这类胶水语言直接嵌入进去，作为NPC的电子大脑是完全可行的。现在正在训练168M规模的语言模型，已经训练一个星期了。至于要训练多久，只能说多多益善吧。希望它的智商比起56M模型能有一些提升。

**2024-11-08**：正在给Nano的推理后端适配RKNPU的API。看了rknn示例代码，现在打算把Nano的C推理的matmul部分用rknn的API来实现，看看能不能加速。从GitHub大佬marty1885提供的情报来看，NPU上的FP16运算相比CPU并无性能优势。但是RK给的示例代码是不是有问题…主要是算余弦相似度的部分，不对。但某些设置下会出来全0的结果，不知道怎么回事。另外，关于RKNN为什么性能还不如CPU，我看了示例代码后脑子里出现的第一个担忧就是访存代价的问题。每次执行matmul之前都有看起来很繁重的初始化工作。但是因为我用的是尊贵的LPDDR5/32GB版本！所以还是有必要测一下性能…

**2024-11-07**：评大模型结构收敛到Transformer：确实…我那个Nano里面的DeepSpeed适配基本上可以删掉了。现有主流分布式训练框架，要么是糟糕的抽象，要么是见啥吃啥的貔貅。业界在这方面浪费了大量的资源。模型结构既然收敛到llama，框架也就不必要了。接下来重要的是大模型和后端、硬件、网络的深度适配。

**2024-11-05**：RK3588到手后，把Nano小模型部署上去。不过他家的RKNN好像不开源，不太好搞。只能用官方工具转几个开源模型玩玩。那就用CPU推理也成。在JetsonOrinNX的A78AE上，各种优化统统开启，观察到最高达100token/s的生成速度。后续打算文火慢炼，炼个100M(0.1B)量级的语言模型。甚至可以把CLIP缝合上去，做个VLM，结合微调，解决一些有实际价值的机器视觉类问题。其实现在已经炼成的56M模型也不是一无是处，在____方面有很大的玩头。

**2024-11-03**：Nano的C语言推理验证成功。主要还是抄袭Karpathy大佬的大作，我只是狗尾续貂而已。几十年没有写过C了，非常的恶心非常的头疼……又来手动管理内存了我的电脑爹

**2024-10-28**：LoRA原理验证成功。接下来：①LoRA模块可插拔化、浏览器推理。②训练一些奇怪的实用LoRA模块，比如无线电考试、比如文本分类等细粒度NLP任务、比如瑟琴对话（用开源大模型自动生成微调语料）等等。③端侧（浏览器）训练。

**2024-10-25**：还以为是BF16转FP32引入数值稳定性之类的神秘问题，定睛一看，嗐，原来GQA压根就没做啊。我做了，要不要给原来的仓库提个PR呢？不了。我凭本事复刻的，凭什么反合到主干。有没有一种可能，我也可以是主干？——与Linus最近的言行有关

**2024-10-22**：炼丹好玩，好玩就好玩在赛博生豆芽，一切准备好之后，什么都不用做，只需要等上三四天，就能得到一只能吃的电子鹦鹉啦。

**2024-10-17**：有人说我的大模型只是把题库背下来而已，其实没有思考的能力。没错，能把题库背下来就很了不起啦！别看我是个老B灯，实际上B类题库大多数问题，我也是不知其所以然的，所以说我的本质就是个复读机。让大模型真正具备思维能力大概只有一个办法，那就是加钱，加钱，加钱！

**2024-10-13**：租用1块A800(80GB)训练58M参数的语言模型，词元吞吐率大约200000token/s，大约是 AGX Orin 的十倍。因此我只能说我玩过的最离谱的东西就是用每小时x元的价格训练电子鹦鹉。现在想想，社会把很大一部分注意力放在电子鹦鹉上，这真的好吗？

![ ](./image/G4/nano-train-a800-202410.png)

![ ](./image/G4/nano-train-h800-202411.png)

**2024-10-12**：电子🦜的训练进展。50M参数，3.2B数据集，AGX Orin 盒子，预训练跑3天了，还不到两个epoch，已经没有耐心了= = 昨晚改进了词元编码，无非是覆盖了所有的CJK字符和绘文字（没错，我认为绘文字才是真正的世界语，给它分配1800个位置是值得的）。另外，考虑到英文的编码效率，加了几千个英文词根词缀和高频词，凑出32768个词，作为默认词表。构建词元编码器是基础工作，它如果不稳定，就不要贸然开展训练。语料还是要持之以恒地做，但是要有个基线。对于个人爱好者来说，数据算力算法，数据是最难搞的，算力倒还在其次（只需要付出亿点点钞能力）。数据难，还是因为把知识从人脑迁移到电脑这件事很难。

**2024-10-11**：用魔鬼辞典来微调正在炼制的50M语言模型。

**2024-10-09**：用假期前训练的预训练模型，在通用SFT数据集上做了小量的SFT，效果比预期好很多。看来没日没夜的长期连续充分预训练是非常有必要的。因此又准备了1.6B词元的预训练语料，计划再花几天时间，再训练一个基础预训练模型出来。

**2024-10-06**：关于大模型的监督微调究竟应该怎么调，现在依然是众说纷纭。机器学习喜欢谈“泛化能力”，但是我请问呢，从“短波业余波段有7MHz、14MHz、21MHz”能泛化出“18MHz”吗？恐怕是不能的。知之为知之，不知为不知。“泛化”是不存在的。“泛化”这个词就体现出一种人类特有的傲慢和自信，我都学会骑自行车了，开车有啥难的。不信你来刺桐城试试？一切训练的实质都是记忆，在封闭世界假设中，一定要让模型见到过全部的世界，最好还有它的反面。除此之外“泛化”出来的，都只能说是幻觉，而不是事实。如果说其中有符合事实的部分，那也只能说是巧合而已。

**2024-09-30：电子🦜训练随笔**

- **目标任务**：最近趁着假期前夕的宝贵空闲，从0927晚上9点开始，训练一个35M参数量的Llama-like语言模型，目标有二：①假装学会回答业余无线电操作证考试问题；②知道自己是BD4SUR训练的大模型和“人类的本质是复读机”这个事实。最终的目标是：对大模型祛魅。
- **模型结构**：模型的实现是基于Karpathy大佬的nanogpt魔改的，自己实现了数据预处理、数据加载器、简单的词元编码器、训练和微调流程等外围组件，并且对原有的GPT模型结构做了简单的魔改，使其兼容RoPE、RMSNorm两个重要技术（因而是Llama-like而不是GPT-like）。整个实现完全不依赖HF的各种框架，如Transformers、Tokenizers等等，也不打算兼容它们，仅依赖torch。具体而微，是我自己搞这些玩具的一个最重要的方针。模型的上下文窗口是256个词元，参数量35M。词元编码器采用最简单的方法，也就是将Unicode字符映射到一个整数。词典大小19000多，模型的词典大小设为20000整，为特殊词元预留一些空间。为了让模型更清楚地认识它的主人，将BD4SUR设置为特殊词元。
- **硬件和环境**：训练硬件使用 Jetson AGX Orin (64GB)，这个小盒子唯二的优势是显存大、功耗低（最高60W），通宵训练不用太担心消防安全问题。至于计算性能嘛，只能说“又不是不能用”，在BF16精度加自动混合精度加FlashAttn的设置下，粗略估计大概可以跑到23TFLOPS的速度（词元吞吐率没有计算）。从单位显存价格角度来看，AGX(64GB)比起 Mac Studio 其实是更划算的，但算力远逊于 Mac Studio，不过生态上的优势又弥补了这一点。虽说显存多多益善，但训练35M参数的语言模型，并不需要多少显存。大显存可以使用更大批大小，批大小设置为256时，显存占用大概30多GB。
- **预训练**：预训练语料的准备极端重要，其重要性无论如何强调都不为过。垃圾进垃圾出的铁律，会惩罚每一个不做数据工程的人。但是我并没有那么多的时间精力，只草草找了些百科、医学、xxqg、精神分析、无线电和经济类语料，利用GPT-4o等商用模型做了简单的、局部的清洗，也生成了一些语料，最终得到大约400M词元的预训练语料。按照所谓的规模扩展定律（Scaling law），预训练词元数至少要是模型参数量的20倍以上。虽然没有这么夸张的倍数，但是这次准备的语料，对于模型参数量来说，也不少了。模型的检查点文件（pickle）大概400MB左右，为了方便使用，将词元编码器（的配置文件，也就是词典）也包括在里面了。预训练持续到0929晚上，迭代了大概100000步，经历8到9个epoch，损失下降到2.1左右。数据的调度（或者叫“数据课程”）不太合理。6类语料是按顺序预训练的，这导致模型的损失曲线呈现出非常明显的分段，损失值低者达到2以下，高者依旧3点多，不过整体趋势是下降的。不知道这样做的负面影响是什么。为了弥补这一点，在第9个epoch将所有语料混合在一起，最终达到2.1的交叉熵损失。预训练阶段，还有一个很重要的技术细节，那就是分块的因果自注意力。预训练语料的分块（chunking）是个特别讲究的环节，如果不相关的句子出现在同一个上下文窗口中，需要用特殊词元将他们隔离开，并且训练阶段最好不要让他们互相看到，因此需要在注意力掩模上动手脚。或者直接在数据预处理阶段将不相关文本分配到不同的窗口中，代价是会浪费很多填充词元的长度，降低训练过程的有效吞吐率。但是我在数据处理阶段并没有很重视数据块的切分，甚至连bos/eos这样的分隔符都没加，所以这个是后续改进的一个点。
- **监督微调和偏好优化**：为了尽快看到效果，从0930凌晨开始，开始用业余无线电操作证考试QA数据集，对预训练模型作监督微调。为了提升SFT的效果，避免微调阶段过分过拟合，用商用大模型扩充了官方的操作证考试题库，每个QA扩充了3条不同的问法，试图激发出模型更一般的语义理解能力。最终得到大约5000条QA对，其中包括“我是BD4SUR训练的大模型”和“人类的本质是复读机”。今天（9月30日）早上8点左右看，损失下降到0.03，实际效果尚未验证。像这种损失值，恐怕早已发生了灾难性遗忘，模型原有的语言知识可能已经基本上被破坏殆尽，但是换来了在操作证考试QA上的精确（然而也是呆板的过）拟合。另外，SFT数据集中，加入大量“我是BD4SUR训练的实验性大模型”和“人类的本质是复读机”数据样例，试图为其注入自我认知。在这种场景下，RLHF的价值就体现出来了。一方面，BD4SUR作为特殊词元，特别容易跟填充词元、指令标记词元等特殊词元混淆，这可能是因为训练不充分，也有可能是SFT中缺乏惩罚机制（当我冒出惩罚的想法时，就意味着该上RL了）。另一方面，模型还不能很有效地学习到指令遵循能力，这会导致问答（chat）场景下，回答部分会出现特殊词元，也就是没有完全遵循指令模板的格式。无论如何，今晚回去看看，经过几百轮SFT的模型，在无线电领域上回答问题的效果如何。
- **推理及其优化**：推理优化，实际上是一个极其有价值且困难的课题，但这暂时不在我现阶段的考虑范围内。一个最基础的优化就是KV-Cache，以空间换时间，基本的实现并不复杂，但是它的优化非常高深，vLLM在这方面做了很多工作。
- **开源的考虑**：至于要不要发布在HF上，我看，还是不要了。拍个视频，留作证明就好了，不必什么东西都开放出去。最重要的考虑，当然是安全性问题。训练语料里面有大量的xxqg内容，所以模型会生成什么东西，我是完全无法控制的。另外，虽然这个模型并没有什么实用价值，但从成本的角度来看，应该说还是相当昂贵的。人力成本（不是什么阿猫阿狗都能搞定这个过程的，请给我技术咨询费！）、时间成本（它在炼丹的时候我在看杨旭游记）、能源成本（连着开了三天三夜的空调，并且是21度！冻死我了）、以及炼丹炉本身的成本（价格不菲的炼丹炉高负载连续运行三天三夜的折旧费估计也不便宜吧），都不可忽视。所以说，自己玩的东西不见得对他人有价值，但可能是相当昂贵的，只为博君一笑，拍个视频证明一下，天空没有留下痕迹，但我已经飞过，这就够了。

**2024-09-24**：词元编码（tokenizer）是传统NLP在大模型时代的最后堡垒之一。但是我觉得相比于其他模态的信源编码算法，自然语言文本的codec，诸如BPE之类的，并没有太多人类的智慧在里面。或者说，人类还是不要自作聪明地去做什么tokenize，直接交给模型去学就好了。因此，不想在词元编码上花费太多精力，直接使用一个Unicode字符一个词元的香草味方法。不过，对于英文这样的字母语文来说，BPE这样的词元编码还是有必要的。但我觉得这主要是经济性的考虑，里面是没有智慧的。当然工程实现层面很有智慧。

**2024-09-26**：语言是最隐秘但是最普遍的规训和统治手段，而我又是个相当叛逆的人，所以我立刻就理解了为什么很多东西我宁可自己发明新说法，也不愿意屈从普遍接受的说法。就比如LLM领域的tokenizer，我向来觉得这东西应该叫text-codec。虽然它的行为确实是给文本作词元化，但它的目的是为了把文本转化为数码序列，同时能无损转换回去，这符合信息论中 lossless codec 的目的和行为。

**2024-09-21**：我最近在尝试在nanogpt上进行预训练和监督微调，试图让小规模的GPT-like模型能够回答业余无线电操作技术能力验证的问题。不求正确，只求“说人话”。但是我总觉得现在的大模型与人脑的运作机制大不相同。具体来说就是LLM是“刚性”的、开环的，它不具备在推理时反馈重塑自身的能力。像语境学习（In-context learning）这样的能力，无非是搜索空间巨大，以至于给人一种举一反三的错觉而已，实际上模型并没有真的泛化出什么内在规律，也没有根据输入信息对自己进行“实时的重新训练”。像预训练、监督微调、人类反馈强化学习一类的手段，似乎都没有做到这一点。RLHF虽然体现了“实时训练”“训推闭环”这样的思想，但还是觉得不够到位。在自省和自我重塑这个视角看来，LLM作为一个庞大的开环控制系统，它甚至不如某些编程语言及其实现更有“智慧”。而人脑是一坨肉，它的结构是柔性的，神经元是会动的，在宏观的稳定中，存在着微观的噪声和易变。虽然诸如NAS（神经网络结构搜索）一类的技术试图将网络结构作为一种超参数甚至网络参数纳入优化的范围，但是如何在推理时实现这一点，可能还有很大的想象空间。归根结底，还是那种熟读唐诗三百首式的智能。LLM掌握了大量的“司机知识”，给人一种充满智慧的表象。至于 OpenAI o1 这类最新的工作，还需要进一步了解。最后还是要关注那个问题：**从表象到逻辑的模式之间，究竟是如何演化的。伽罗瓦式的智慧和拉马努金式的智慧，究竟有什么样的区别和联系。**

**2024-08-18**：最近两天看新闻已经看到两个端侧AI的应用场景了：一是在电塔上部署机器视觉盒子，对视野中的鸟进行分类和定位，如果看到了可能会在电塔上筑巢的鸟类，则控制机器定向发射激光把它赶走。二是在手机上部署AI换脸变声检测模型，在不泄露隐私的前提下，在用户设备本地运行检测算法。

**2024-08-10**：黄老板唯一不坑人的良心产品，勋子真心想跟你交个朋友。22TFLOPS，多么恐怖的算力。64GB统一内存显存，多么恐怖的容量。老黄这人行，能处！

**2024-03-14**：72B量级的LLM所拥有的强大的指令跟随能力让她在某个十分逆天的提示语的激励下获得了极其逆天的瑟琴能力，这在2022年以前是绝对无法想象的。出于公序良俗的考虑，我不能提供进一步的情报了，请兄弟们自由探索吧，桀桀桀~

**2024-02-15**：[在N9020A频谱仪上部署AI大模型](https://www.bilibili.com/video/BV1du4m1P7iU)

**2023-12-28**：众所周知，变压器类大模型有3种流派，分别是BERT（编码器解码器）、GPT（仅解码器）和T5（仅编码器）。历史表明，GPT类架构在大模型流派的竞争中暂时胜出，生成任务可以囊括各类语义理解任务。随着GPT的优势逐渐显现，对于OpenAI当初为何决策采用decoder-only架构，业界展开了马后炮式的研究。2021年4月，我发表了一篇长篇暴论，其中提到了变换矩阵的秩与变换的描述能力的问题。此时我还不了解注意力机制的原理。然而，在此一个月前，2021年3月，有人发表了一篇论文，标题大概是 Attention Is Not All U Need ...，其中一个主要观点是注意力矩阵的秩限制了注意力机制的表达能力。而我们在技术路线选择上，也是凭借大就是好的直觉，选择了BERT。在苏剑林的博客文章评论中，有人提到，双向注意力实际上是给注意力矩阵增加了更多的约束，导致注意力的有效自由度反而降低。

**2023-11-28**：基于[chatglm.cpp](https://github.com/li-plus/chatglm.cpp)（V0.3.0），在松下SV8洋垃圾笔记本（Core i5-8365U，16GB内存，512GB的NVMe固态硬盘，Ubuntu 20.04 LTS）上部署ChatGLM-6B。

**2023-04-25**：我理解的机器学习算法有三个要素：模型、失真测度和优化方法。大家都很关心前两个，因为注入了人的价值观；而第三个需要神谕般的智慧。力学的核心原理是最小作用量原理，大自然知道如何最小化，但是人类很难知道。至于SAT是所谓NP完备问题，此问题可帮助人类领悟神谕。

**2023-03-27**：人类的感官就是一种有显著非线性特性和严重失真的糟糕的传感器，早期音视频编码大部分复杂度就在处理此类问题。而人脑早已学习出基本的去噪/校正算法，剩下的就要每个人自己努力去克服偏见和错觉了。

**2023-03-22**：感知智能发展，借用教育学术语，总结为三个范式或三个阶段：①职业教育：人工设计特征和领域专用小模型，解决领域问题，例如语音特征+HMM解决语音识别问题。②博雅教育：预训练+微调，训练一个高中生，再给他赋予专业能力。③主体教育：RLHF，强化学习，在互动中学习，培养智能体的主体间性。

**2023-03-19**：好多coser面对AI已经表现出显著的恐慌和愤怒了。其实我觉得如果真正喜欢cos本身的话，应该是心如止水岿然不动的才对。当然，很多coser靠这个赚钱，我理解。就像我玩无线电，其实是纯消费纯消耗，我不会因为它是个不时髦的小众东西就不喜欢它，也不会因为没有投资收益而远离它。我只是单纯喜欢这东西，与任何外人外物都无关。所以我预期我最近辛辛苦苦做的视频应该也不会有人看，这从立项一开始就完全在我的预料之内。我不会因为没人看就不做了，我更多的是给自己做，给自己看。最重要的是，**不能因为自己如何如何而产生某种优越感**，否则就不是就事论事了。另外我觉得我们人类的知识产权制度确实是越来越落后了…我们人类今天引以为傲的很多东西，过几年可能都是要变成非遗被保护起来的。我的暴论就是，德赛两先生，可能是第一个进非遗博物馆的。这是人类思想史的里程碑，但也只是里程碑而已。

**2023-03-15**：RNN和注意力之间的区别，很难不让人想到数字逻辑电路里面有关加法器的两种实现方式，一种是行波进位，另一种是超前进位。



### LLM部署&性能测试备忘录

本地运行DeepSeek-R1（Unsloth版，[参考](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)）：

```
/home/bd4sur/ai/llama.cpp/build/bin/llama-cli \
  --model /home/bd4sur/ai/_model/DeepSeek-R1/DeepSeek-R1-UD-IQ1_S.gguf \
  --cache-type-k q4_0 \
  --threads 36 \
  --prio 3 \
  -no-cnv \
  --temp 0.6 \
  --ctx-size 8192 \
  --seed 3407 \
  --n_gpu_layers 32 \
  --numa distribute \
  --no-mmap \
  --mlock \
  --prompt "<｜User｜>假设你是大气物理、电波传播、雷达和空间环境领域的专家，请你运用专业知识，对以下现象做出专业的、准确的解释。大雪天气中进行短波无线电信号接收，观察到以下现象：一是短波背景噪声的大幅度波动：下雪时，可观察到短波频段的背景噪声呈现出以几十秒为周期的大幅度波动，波动幅度可以达到几十dB。二是短波非预期信号的突然出现和消失：下雪时，有时可以接收到显然不应该出现在这个短波频段的信号，而过几分钟之后，这个信号又突然消失，其出现与消失是随机的，信号出现的领率也是不确定的。例如，在业余频段能够接收到随机出现的短波AM广播信号（也就是说，接收到的短波广播信号的频率发生了变化，从广播频段变化到了业余频段，这是否意味着接收到的信号并非从广播电台直接发射出的原始信号？)我先提出一些猜想，你需要利用你的专业知识和参考资料，对我的猜想进行评审和补充，并提出你自己的新观点。我的猜想如下：针对短波背景噪声的大幅度波动现象：可能有雪花冰晶相互摩擦产生静电导致背景噪声增加和波动、短波信号被雪花与空气形成的时变非均匀混悬体系随机散射等原因。针对短波非预期信号的突然出现和消失现象：可能与电离层或者空气与雪花的混悬体系对电波施加的非线性效应有关。我确认能够排除的因素如下：1、排除接收机本身的非线性。我所使用的接收机是专业的短波接收机，配备预选滤波器，抗干扰抗阻塞能力强，并不是一般收音机，工作状态良好，不存在强信号导致接收机过载进而出现假信号的可能。所接收的信号均为实际从空中接收到的信号。2、排除天线受到风雨雪影响。我所使用的天线是位于室内的高增益有源小环天线，不会受到雨、雪、风等气候因素影响，也不会受到潮湿、静电等因素影响。请你运用电波传播相关专业知识，对这些现象做出专业的、准确的解释。如有需要，请给出参考文献。<｜Assistant｜>"
```

基本的对话功能：

```
from llama_cpp import Llama

# 如果提示 KV-cache OOM，适当减小上下文长度。
NUM_GPU_LAYERS = 20 # 设为-1以加载全部层到GPU
CURRENT_LLM_CONFIG_KEY = "DeepSeek-R1-UD-IQ1S-8K"

LLM_CONFIG = {
    "Qwen2.5-7B-Q4KM-64K": {
        "model_type": "gguf",
        "model_path": "/home/bd4sur/ai/_model/Qwen25/qwen2.5-7b-instruct-q4_k_m.gguf",
        "seed": 3407,
        "context_length": 65536,
        "temperature": 1,
        "top_p": 0.9,
        "top_k": 40,
        "min_p": 0.0,
        "repeat_penalty": 1.0
    },
    "Qwen2.5-72B-Q4KM-16K": {
        "model_type": "gguf",
        "model_path": "/home/bd4sur/ai/_model/Qwen25/qwen2.5-72b-instruct-q4_k_m.gguf",
        "seed": 3407,
        "context_length": 16384,
        "temperature": 1,
        "top_p": 0.9,
        "top_k": 40,
        "min_p": 0.0,
        "repeat_penalty": 1.0
    },
    "DeepSeek-R1-UD-IQ1S-8K": {
        "model_type": "gguf",
        "model_path": "/home/bd4sur/ai/_model/DeepSeek-R1/DeepSeek-R1-UD-IQ1_S.gguf",
        "seed": 3407,
        "context_length": 8192,
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 40,
        "min_p": 0.0,
        "repeat_penalty": 1.0
    },
    "QwQ-32B-Q4KM-64K": {
        "model_type": "gguf",
        "model_path": "/home/bd4sur/ai/_model/QwQ/qwq-32b-q4_k_m-unsloth.gguf",
        "seed": 3407,
        "context_length": 65536,
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 40,
        "min_p": 0.0,
        "repeat_penalty": 1.0
    },
    "QwQ-32B-Q5KM-64K": {
        "model_type": "gguf",
        "model_path": "/home/bd4sur/ai/_model/QwQ/qwq-32b-q5_k_m-unsloth.gguf",
        "seed": 3407,
        "context_length": 65536,
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 40,
        "min_p": 0.0,
        "repeat_penalty": 1.0
    },
    "QwQ-32B-Q80-64K": {
        "model_type": "gguf",
        "model_path": "/home/bd4sur/ai/_model/QwQ/qwq-32b-q8_0-unsloth.gguf",
        "seed": 3407,
        "context_length": 65536,
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 40,
        "min_p": 0.0,
        "repeat_penalty": 1.0
    },
}

SYSTEM_PROMPT = ""

llm = Llama(
    model_path=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["model_path"],
    seed=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["seed"],
    n_ctx=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["context_length"],
    n_gpu_layers=NUM_GPU_LAYERS,
    use_mmap=False,
    use_mlock=True,
    n_threads=36,
    numa=1,
        # https://github.com/ggml-org/llama.cpp/blob/master/ggml/include/ggml-cpu.h
        # GGML_NUMA_STRATEGY_DISABLED   = 0,
        # GGML_NUMA_STRATEGY_DISTRIBUTE = 1,
        # GGML_NUMA_STRATEGY_ISOLATE    = 2,
        # GGML_NUMA_STRATEGY_NUMACTL    = 3,
        # GGML_NUMA_STRATEGY_MIRROR     = 4
    type_k=(2 if "DeepSeek" in CURRENT_LLM_CONFIG_KEY else 1),
        # https://github.com/ggml-org/llama.cpp/blob/master/ggml/include/ggml.h
        # GGML_TYPE_F32  = 0,
        # GGML_TYPE_F16  = 1, (default)
        # GGML_TYPE_Q4_0 = 2,
        # GGML_TYPE_Q4_1 = 3,
        # GGML_TYPE_Q5_0 = 6,
        # GGML_TYPE_Q5_1 = 7,
        # GGML_TYPE_Q8_0 = 8,
        # GGML_TYPE_Q8_1 = 9,
    verbose=True
)

history = []

def typewriter(delta):
    print(delta, end="", flush=True)

def predict(message, callback):
    messages = []
    messages.append({"role": "system", "content": SYSTEM_PROMPT})
    for user_message, assistant_message in history:
        messages.append({"role": "user", "content": user_message})
        messages.append({"role": "assistant", "content": assistant_message})

    messages.append({"role": "user", "content": message})

    response = llm.create_chat_completion(
        messages=messages,
        temperature=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["temperature"],
        top_p=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["top_p"],
        top_k=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["top_k"],
        min_p=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["min_p"],
        repeat_penalty=LLM_CONFIG[CURRENT_LLM_CONFIG_KEY]["repeat_penalty"],
        stream=True
    )

    text = ""
    for chunk in response:
        choices = chunk["choices"]
        first_choice = choices[0]
        delta = first_choice["delta"]
        if "content" in delta:
            content = delta["content"]
            text += content
            if callback is not None:
                callback(content)
    history.append([message, text])


if __name__ == "__main__":
    print(f"使用模型：{CURRENT_LLM_CONFIG_KEY}")
    while True:
        try:
            prompt = input("User > ")
        except EOFError:
            break

        if not prompt:
            continue
        if prompt == "stop":
            break
        if prompt == "restart":
            history = []
            continue
        
        if CURRENT_LLM_CONFIG_KEY == "DeepSeek-R1-UD-IQ1S-8K":
            prompt = f"<｜User｜>{prompt}<｜Assistant｜>"
        print(" Bot > ", end="")
        predict(prompt, typewriter)
        print("\n")

```

实测结果汇总：

|机器|模型-量化-上下文长度|其他设置|P速度(输入词元数)|D速度(生成词元数)|
|----------------------------------|
|AGX Orin|QwQ-32B-Q80-64K|-|182.3(491)|4.5(2794)|
|AGX Orin|QwQ-32B-Q4KM-64K|-|175.7(491)|6.2(3061)|
|7048GR-P40*4|DeepSeek-R1-UD-IQ1S-8K|见上文命令|20.35(431)|1.3(1600)|

使用QwQ-32B-GGUF模型的一些注意事项：[How to Run QwQ-32B effectively](https://docs.unsloth.ai/basics/tutorial-how-to-run-qwq-32b-effectively)


# 多模态理解

## 应用案例：表情包搜索引擎

视频演示：[自制表情包搜索引擎演示](https://www.bilibili.com/video/BV1vJ4m1e7MN)

2024-02-25：正在用Qwen-VL给六百多个表情包梗图做captioning。速度太慢了太慢了，按照这个速度，估计要做到明天去。并且视觉语言模型在meme理解上还是有很大的缺陷，最可笑的是，可能是由于价值观对齐的缘故，诸如“装逼”这样的词是无法出现的，会识别成“装通”。我们仍未知道视觉语言模型是如何实现OCR的。看来必须挂一个传统的OCR模型了，用来忠实提取画面上的文字，保证召回率。另外，提示语的设计仍然是个开放问题。

# 多模态生成

# 语音专题

