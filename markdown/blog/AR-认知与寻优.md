
#!title:    认知与寻优
#!date:     2023-02-23

#!content

**版本记录**

- 2019-08-04：开始起草《视频编码》专题。
- 2019-11-03：开始起草《音频编码》专题。
- 2023-02-23：建立《视听技术》专题，由《视频编码》和《音频编码》两篇文章合并，关注成体系的视听技术。
- 2023-03-28：博客增加《机器学习》专题。为什么不叫《人工智能》：首先这个专题并不关注“人工”的部分，只关注“机器”的部分；其次我不知道什么是“智能”，很难讨论一个没有良好定义的东西，而“学习”是个相对明确的东西，是一种数据驱动的方法、途径。这个专题不太关注目标ML能干什么，也不关注ML能否到达“智能”（这在其他专题中讨论），主要关注的是方法和技术的中微观细节。
- 2025-02-07：将《机器学习》专题合并到本文，并将本文更名为《认知与寻优》，旨在探索让机械系统“理解”世界的途径，这一途径基本上可以视为寻优过程。

![城市天际线（其实是Aqua在64kbps(CBR)下压缩某一首歌得到的声谱图。由于还没有实现心理声学模型，编码器随缘量化，中高频部分不仅损失，而且引入了很强的可闻量化噪声，能量分散在很宽的频段上，表现在声谱图上就是这个样子了。2023-08-30截图](./image/G3/audio-encoding/aqua-noisy-spectrum.jpg)

# 视听技术

- 采集·感知：成像和摄影 / HVS / 光学 / 光机电系统
- 处理·通信：图像处理 / [[#ff0000:**高效编解码**#]] / 视频通信广播 / 存储 / 版权
- 建模·认知：计算机图形学 / AR/VR / 计算机视觉 / 语义化和信息检索
- 呈现·交互：显示和打印 / 图形计算 / Web前端/GUI / 人机交互 / 字体排印

## 成像技术

CMOS尺寸：**所谓“1英寸CMOS”的对角线长度为16mm，而不是25.4mm。**这大概要追溯到二十世纪五、六十年代电子成像技术刚开始的时代吧。那时早期的电视摄像机使用的感光元件是真空管。真空管的外面是有个玻璃罩子的，真空管外径是把玻璃厚度也算进去的，玻璃管当然是不能用于成像的，外径1英寸的真空管，实际成像区域只有16mm左右，于是16mm就成了电子摄像照相行业一个约定俗成的度量单位。这种度量方式一直继承了下来，我们现在所接触到的CCD尺寸的说法是参考传统摄像机内的真空摄像管的对角线长短来衡量的，它严格遵守了 Optical Format 规范，其数值称为OF值，单位为英寸。（整理于2020.5.2，[来源](https://blog.csdn.net/u010653400/article/details/78050494/)，[参考](https://en.wikipedia.org/wiki/Image_sensor_format)）

## 显示技术

**显像管电视的高频声音**

- 国内的电视机遵循PAL-D制式标准。
- PAL制式的视频分辨率是720×576，每秒25帧。由于隔行扫描的原因，每一帧画面分为两场进行扫描，每场耗费312.5行扫描时间，正程287.5行扫描时间，逆程25行扫描时间，所以扫描575行需要625行扫描时间，场频正好是两倍帧率。所以对于PAL制式CRT电视来说，行频=312.5×2×25=15.625kHz。
- 由于线圈等元件的工艺问题，在这个频率下免不了发生一些机械振动，听到的便是这个频率的声音。
- 电脑CRT显示器也会有类似的噪音，但显示器分辨率和刷新率较高，比如17寸CRT普遍的1024×768@85Hz，行频已经超过了65kHz，人耳是不可能听到的。（[来源](https://www.zhihu.com/question/55612873/answer/151940491)）

## 计算机图形学

### 水波纹特殊效果

<iframe class="MikumarkIframe" src="./html/ripple.html" height="350px"></iframe>

水波纹特效是比较常见的一类特效。前段时间突然想知道这个特效是怎样实现的，便有了这篇文章。

最开始看到这个问题的时候，第一想法是使用正弦波的传播去模拟水波的传播，同时加上一定的衰减（指数衰减）去模拟真实的物理过程。然而运行的时候就傻眼了——动画极其卡顿，完全没有流畅的感觉。这显然是因为正弦、指数、乃至乘法操作耗费了大量的时间，如果是做实时渲染的特效，断然不可以使用这些复杂的函数。

这次失败的尝试，充分暴露出我毫无图形学素养的事实。因此，我开始搜索现成的解决方案[[1]](#参考资料)。

**建模**

水波是日常生活里司空见惯的物理现象，但实际上是非常复杂的。像我最开始那样，简单使用衰减的正弦波去模拟水波，尽管效果上比较类似，但那并不是物理事实。

从运动的效果入手去模拟物理现象，我认为这是一种运动学的方法。这只是在结果的层次上进行模拟，并不具有通用性，甚至在有些情况下很可能是错误的。

因此，合理的方法应该是对需要模拟的物理现象进行动力学的建模，从“原因”的层面上去模拟。由于只是模拟视觉效果，因此可以忽略许多物理细节，在保证效果的前提下，减少实现上的复杂性。

水波效果可以分成两个相继的问题去研究：一是对水波本身进行模拟，二是对水波折射水底景物的效果进行模拟。

**水波的动力学**

首先需要明确的一个常识是：波是能量的传播，而不是水中质点的传播（实际上水波的质点会沿着波的方向运动）。水中振动着的质点，由于液体内部的作用力，会带动其周围的质点一起运动，导致振动的能量以波源为中心向外传播。从宏观角度看，就体现为一圈水波。

如果把宽度为$W$的Canvas画布上的每个像素看成是振动着的质点，那么可以用$P_i$来表示从左上角开始的第$i$个质点，其对应的振幅记为$A_i$。$P_i$上下左右四个临点的振幅记为$A_{i1}$、$A_{i2}$、$A_{i3}$和$A_{i4}$，其中$i1 = i - W$、$i2 = i - 1$、$i3 = i + 1$、$i4 = i + W$。

假定①：经过一个时间单位后，$P_i$的振幅可以由以下递推方程所确定（其中$a$和$b$是待定系数）：

$$A_i' = a(A_{i1} + A_{i2} + A_{i3} + A_{i4}) + bA_i$$

假定②：不考虑阻尼的情况下，水波传播过程满足振幅守恒：

$$\sum{A_i'} = \sum{A_i}$$

需要注意的是，这并非能量守恒，而是为了计算方便而做的合理简化。实际上，按照以上两个假设，计算出的水波振幅实际上是不收敛的，即振幅虽然会上下振动，但绝对值会越来越大，直至无穷。

（待续）

**性能优化**

- 化乘除为移位。
- 避免频繁读写全局变量。
- 使用WebGL。

**参考资料**

+ [Canvas实现水波纹效果](http://uusama.com/643.html) | [GitHub](https://github.com/youyouzh/water_ripple)，本文主要借鉴了这篇文章，文章开头的演示沿用此文作者的代码。

## 光存储技术

![光盘光斑尺寸对比](./image/G3/光盘光斑尺寸对比-Wiki.png)

![光盘激光头光路](./image/G3/激光头光路图.png)

2015年VCD改造：详见`image/G3/vcd-hacking`目录下图片。

## 音乐盒（2018-05-20）

<iframe class="MikumarkIframe" src="./html/jukebox.html" height="300px"></iframe>

## Project Vita

### 公共数据类型定义

图像矩阵`Imat`：用于表示图像单个通道的像素矩阵。其结构为**由每一行构成的数组**，如果以左上角为坐标原点(0,0)，那么访问位于(x,y)的像素的方法是`Imat[y][x]`。矩阵的每个元素为`number`类型，一般为[0,255]的整数。

```
Imat: [
    [第0列, 第1列, ... , 第(width-1)列], // 第0行
    [第0列, 第1列, ... , 第(width-1)列], // 第1行
    ......
    [第0列, 第1列, ... , 第(width-1)列], // 第(height-1)行
]
```

### 静止图像Codec

**▶ [可视化演示](https://bd4sur.com/html/jpeg.html)**

定义在`jpeg.js`中。可视化页面`jpeg.html`。

编解码器由一个模块`JPEG_Codec`类构成，使用ES5编写，Node和浏览器皆可使用，但是浏览器使用之前要注释掉`require`相关的代码。

仅实现了变换编码和游程编码，未实现熵编码（哈夫曼编码）和码流封装部分，因此并非完整的JPEG编解码器。

2019年7月7日首次实现；2020年6月重构并纳入本仓库。

依赖：

- [fourier.js](https://github.com/bd4sur/Fourier)：提供离散余弦变换的实现
- image.js：用于可视化，提供RGB与YUV之间转换的函数
- canvas.js：用于可视化，提供绘图函数

使用方法：

```javascript
// 建立Codec实例
let jcodec = new JPEG_Codec(); // 无参数

// 编码
let streams = jcodec.encode(Y, U, V, width, height, quality);

// 解码
let yuv420 = jcodec.decode(stream_Y, stream_U, stream_V, quality);
```

接口定义：

`JPEG_Codec.prototype.encode`：编码函数。此函数将**YUV420**格式的三个图像矩阵编码为三个码流。参数如下：

- `matrix_Y: Imat`、`matrix_U: Imat`、`matrix_V: Imat`：YUV三个通道。输入的YUV矩阵**必须是YUV420格式的**，即U、V矩阵的长宽都是Y的一半。矩阵的元素为8位无符号整数，即[0,255]区间内的整数。
- `width: number`、`height: number`：输入图像的宽度和高度。这两个参数未必与输入矩阵的尺寸完全符合，如果小于实际尺寸，则只会处理一部分；如果大于实际尺寸，则范围外的部分可能是`NaN`。Codec不会检查这两个参数是否符合实际图像的尺寸。
- `quality: number`：图像质量参数，值域为(0,100]。该参数用于控制压缩程度。值越大，画质越高，压缩程度越低。值为50时相当于采用标准推荐的量化表。

`JPEG_Codec.prototype.decode`：字节码流解码函数。此函数将`encode`函数生成的字节码流解码为YUV420格式的三个图像矩阵。参数如下：

- `bytestream_Y: Array<number>`、`bytestream_U: Array<number>`、`bytestream_V: Array<number>`：三个通道的码流。
- `quality: number`：图像质量参数，需要与编码时使用的参数一致。

待办：

- 熵编码接口（待后续按需要实现）。
- 边缘补零会导致严重的振铃效应。
- 成员函数静态化（`JPEG_Codec`类仅仅起到名称空间的作用），并且尽可能去除语言相关的部分，以利于C/C++移植。

参考资料：

- [ITU T.81](https://www.w3.org/Graphics/JPEG/itu-t81.pdf)

### 抖动

**▶ [可视化演示](https://bd4sur.com/html/jpeg.html)**

定义在`dither.js`中。可视化页面`jpeg.html`。

实现了 [Floyd-Steinberg 抖动算法](https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering)。

接口格式：`dither(matrix, width, height, quant_function)`

- `matrix: Imat`：单通道图像矩阵。
- `width: number`和`height: number`：图像的宽度和高度。
- `quant_function: number→number`：量化函数，默认为简单二值化函数。

### 数字盲水印

**▶ [可视化演示](https://bd4sur.com/html/watermark.html)**

定义在`watermark.js`中。可视化页面`watermark.html`。

算法简述：

- 对图像（RGB或者YUV）作DCT变换，得到变换域图像。
- 在变换域图像上叠加水印，水印强度可以根据应用场景调整。
- 对叠加水印后的变换域图像作IDCT，即得添加水印的图像。
- 再次进行DCT，就可以看到叠加在变换域上的水印图案。

该算法非常朴素，健壮性很差，有待更进一步的改进。

2019年7月30日首次实现；2020年6月重构并纳入本仓库。

### 运动估计

**▶ [可视化演示](https://bd4sur.com/html/inter.html)**

定义在`inter.js`中。可视化页面`inter.html`。

采用三步搜索法作运动估计。

2019年8月7日首次实现；2020年6月重构并纳入本仓库。

依赖：

- `math.js`

### Otsu二值化

**▶ [可视化演示](https://bd4sur.com/html/harris.html)**

定义在`otsu.js`中。可视化页面`harris.html`。

2019年8月10日首次实现；2020年6月重构并纳入本仓库。

依赖：

- `math.js`


### Harris特征点

**▶ [可视化演示](https://bd4sur.com/html/harris.html)**

定义在`harris.js`中。可视化页面`harris.html`。

2019年8月7日首次实现；2020年6月重构并纳入本仓库。

依赖：

- `math.js`

## 视频编码（2019-08-04）

### 一 绪言

**1-1 研究动机**

- **时代背景** 毫无疑问，视觉是人类最重要的知觉。2020年的新冠疫情，网络摄像头价格飞涨，远程教育大发横财，远程办公迅速普及，让人们意识到音视频通信或许是下一个刚需。至于之前一直不瘟不火的AR/VR、4K~8K超高清、全景直播，乘着5G的东风也开始有了复兴的迹象。虽然影院KTV尚未复工复业，但红米的98寸大电视、B站大会员的4K视频也让人直呼真香。大片永远嫌不够多，大电视永远嫌不够大。底大一级压死人，如今依然是至理名言。
- **技术难题** 视频号称“最大的大数据”，其巨大的数据量是一切技术难题的万恶之源。因此，**高效**视频编解码，在整个视觉技术体系中作为底层基础，地位相当重要，是学界、业界重点关照的领域。视频编码和视频理解被业内公认为最有挑战性的两个领域。在这两方面，无论是技术研究还是产业落地，我国做得都还不错，至少不算落后。
- **技术自主** 视频编码地位重要、技术难度大、专利和标准壁垒高、先发优势效应明显，堪称视觉技术领域的光刻机，是**要不来、买不来、讨不来、也很难通过市场调节自发创造出来**的核心卡脖子技术。好在二十年前的DVD专利案给了国人当头一棒，如今才有了渐入佳境的AVS标准。还是那句话，我可以不用，但你不能没有。对于一个有抱负的国家来说，核心技术这块儿必须做到自主可控，没有任何捷径和退路。
- **个人兴趣** 个人知识管理需要维护大量的动画、电影等视频文件，涉及压制、转码、后处理、甚至检索等工作，有必要了解视频编解码的原理。另一方面一直对音视频技术感兴趣，因此愿意深入钻研下去。

**1-2 研究目标**

- 总的来说是抱着玩票的态度去研究。当然，以后靠这个吃饭也是说不定的事情。无论如何，满足自己的兴趣是首要目的。
- 输出一些有价值的材料，包括学习笔记、图表、文献综述等等，一方面是将自己学到的东西沉淀下来，另一方面有助于后来者入门。
- 写一些代码，知行合一。
- 掌握ffmpeg的用法。

**1-3 研究方法**

- **文献调研** 文献调研是研究工作的起点和重点，其目的是梳理本领域的概念体系、发展历程、经典文献、方家巨擘，以便后续可以纲举目张地进行深入研究。文献调研总的来说是个自举的过程，触发自举的源头有维基百科词条、知乎大佬文章、高校课程讲义、以及随缘看到的公号文章、业界资讯、博客笔记等等。从这些源头出发，通过引用关系递归地爬取文献资料，初步建立起能够反映本领域全貌的文献集合。当然，这个过程需要有选择地爬取价值较高、有代表性的材料，时刻注意去粗取精，不能无脑爬取而徒增负担。还需要注意的是，材料之间既要相互覆盖，又要尽可能从不同的角度去论述问题。不同材料之间相互对照，有助于去伪存真、获得相对可靠的知识。另外，为了保证参考资料随时可考，所有材料都必须保留自有备份。
- **输出驱动** 以输出倒逼输入，在创作中发现问题。
- **多提问题** 一个好的问题，胜过一百个现成的知识。
- **知行合一** 多写代码。需要遵守“代码即文档”的选择，保证可读性第一。

**1-4 结构和内容** 本文从视频编码的基本原理出发，展开介绍主流标准所使用的三大核心工具——变换、预测和熵编码，以及率失真优化等关键技术。随后分析主流视频编码标准，包括MPEG系、ITU系和AVS系，按照时间顺序分成三代，重点放在横向纵向比较，勾勒出视频编码标准的进化脉络。最后回归实践，介绍FFmpeg的使用方法。

------

**2020-06-12**

从0101二进制串→位图的过程，各位已经讲得很清楚了。我主要补充一点有关图像/视频压缩的东西。

如果没有视频压缩，那么很容易计算出，一个时长为1分钟、帧率25fps、分辨率为1920×1080逐行、每个像素RGB共24bit的简单视频，它的尺寸大约是9.3GB。然而实际见到的这类视频一般只有几MB～几十MB左右，这意味着编码器可以将原始视频以成百上千倍的效率对视频进行压缩，大大节约硬盘容量和带宽。以下是视频压缩的基本原理。

首先要知道，由于视网膜上感受亮度的杆细胞远多于感受色彩的锥细胞，因此人眼对亮度的感觉比色度更敏感。根据色彩理论，RGB并非表示颜色的唯一方式，还有一种方式称为YUV。出于以下的优势，图像/视频编码一般使用YUV表示。

Y指亮度分量，可以理解成黑白电视信号。而UV指两个色度分量。因此用YUV方式传输视频信号，可以非常方便地实现黑白电视与彩色电视的兼容。

更重要的是，人眼对Y比UV更敏感，因此可以在不损失画质的前提下，适当减少UV成分。普通的RGB图像中，RGB是同样多的，但是对于YUV来说，U和V的数量可以减半（指宽度或高度方向上），称为YUV422排列或者YUV420等等。

除了人眼生理特点决定的色彩冗余之外，视频内容本身也存在大量冗余。包括时空冗余、频域冗余、编码冗余等等。以下开始逐个消除这些冗余。

静止图像有大量的空间冗余。对于类似于框图、图书扫描的那种图像，它的背景有大片的空白，这部分空白内容就可以通过各种手段压缩掉。例如游程编码、例如帧内预测等等。

视频在时间上同样有更大的冗余。例如监控探头拍摄的画面，如果画面中没有物体运动，那么连续多帧画面之间的差异是很小的。此时，只需要编码传输第一帧（关键帧）和后面各帧与它的差异就可以了。如果画面中有物体运动，也是同样的思路，只需要编码传输画面中运动部分的位移信息即可。通过这种手段，可以消除掉绝大多数冗余。

以上两种压缩编码方法利用了视频在时空两个维度上的冗余，统称为预测编码。

还有一种重要的编码手段称为变换编码，用于消除图像的频域冗余。因为人眼对于图像中细微波动的细节并不敏感，所以可以想办法在人眼可接受的前提下，将一部分细节去除掉。方法是通过离散余弦变换（DCT）将直观上按照像素的空间位置排列的图像，转换为按照空间频率排列的频谱图。频谱图的特点是绝大多数信息都集中在一个角落里，而远离这些角落的零散信息，就是人眼难以感知到的冗余。

这一阶段的压缩效率如何呢？比如说对于一个8×8的原始图像，变换前需要保存所有64个像素，但变换后可能只剩下角落里的10个点比较重要（强度比较大），其余54个点都是可以被丢弃的冗余信息，因此只保留那10个重要的点就可以了。保留下来的少数几个点，再经过量化，可以简单理解成四舍五入，使这些频率点变成有限的几个值，实现更进一步的压缩。（注：实际上保留或者丢弃的操作是通过调整量化参数实现的，因为过小的值经过量化后就变成0没有了，相当于被丢弃掉了。）

变换编码/量化之后，还有一种强大的编码手段——熵编码。由于量化得到的值其取值范围是有限的，因此每个值（或多个值构成的元组）都可以看成是一个符号。熵编码的核心思想是根据原始信息中符号的概率分布，设计最佳的编码本，使得出现概率越大的符号编码越短，出现概率越小的符号编码越长。比如26个英文字母里A出现的概率远高于Z，因此A的编码长度要短于Z，才能保证总体编码长度较短。JPEG采用较为简单的哈夫曼编码，它的编码本是固定的。而H.264等视频编码标准采用更灵活的CABAC（上下文自适应算术编码）等方法，它会实时统计各个符号的出现概率，同时更新编码本，达到更高的压缩效率。

最后，为了保证视频信号不出错不失真，还会引入很多抗误码手段，例如校验码、例如灵活宏块排序等等，保证万一数据出错，要么可以通过校验码恢复，要么不会影响到太大的范围。

经过以上各种方法的处理，简单的二进制表示的图像，可以被压缩到原来尺寸的几百乃至上千分之一。无论是放在硬盘上慢慢欣赏，还是用4G流量看实时直播，都毫无压力了。

以上便是图像/视频压缩的大致原理。

------

**2020-06-19**

在《数字图像压缩编码》这本书中，用比较短的篇幅讲清楚了 ITU-T（VCEG）和 ISO/IEC（MPEG）两大门派制订视频编码标准的不同考虑。

ITU-T侧重“视频通信”，对于压缩效率、Codec成本和速度、在不可靠信道上如何靠谱地传输这类问题考虑得多一点。同时比较重视标准的体系化，试图将音视频通信嵌入到 ITU-T 的整个建议书体系中。

MPEG侧重“多媒体应用”，每一代标准都试图建立完整的多媒体应用系统的全套标准，相比于ITU-T，MPEG对于封装、交互、系统集成着墨较多。

- MPEG系标准是VCD/DVD/蓝光这类消费者产品的核心技术，正是其重应用思路的体现。
- ITU-T的G系列是重要的话音编码标准，H.261等标准至今仍在视频会议等场景应用广泛。
- MP3/AAC都是MPEG颠覆业界的杰作，但是这俩标准的理论编码延时都高得惊人，显然不是为了音频通信设计的。
- H.264提出的网络抽象层概念，就是为IP网上的视频通信设计的，充满了通信味儿。相比之下，MPEG从MPEG-1开始就引入了B帧，用术语来说就是非因果的，明摆着就是为了回放而不是实时通信设计的。
- MPEG-4非常超前非常大胆，它提出来的对象编码、视频交互、语义化等方向，即使在现在也很少可以完整实现。

------

**2020-06-20**

B站的视频编码码率控制：

- 离线的码率失真优化
- 引入机器学习预测编码参数
- 内容分类/切分场景
- 控制复杂度和视频上线时间
- 根据用户行为触发更高级的优化

------

**原型特性规划（2020-06-24）**

- 基于YUV420，逐行。
- 不需要条带
- 仅I帧和P帧两种。
- 仅支持8×8的宏块，不支持子块分割。
- 浮点DCT。量化参数？
- 帧内模式5种，同AVS。
- 仅前向参考。如何选择参考帧？
- 运动估计暂且采用1像素精度。
- 简单的游程编码，熵编码先不做。
- 率失真优化：拉格朗日优化，或不做。
- 码率控制：待调研。​​​

------

**2020-05-20**

- 包括子块划分、帧内预测模式、参考帧、量化参数在内的编码模式的决策，一方面是RDO，另一方面是启发式的选择。（参考：知乎文章H.265概念解析[OL]、深入理解视频编解码技术[M]）
- 去块滤波有两类，一类是作为解码后处理的 post filter，另一类是位于编码环路内部的 loop filter. （深入[M] p139）
- 问：为什么帧内预测的输入信号（相邻参考块）不需要去块滤波？猜想：去块滤波是针对整幅图像的，而帧内预测只能基于（一般位于左、上两个已编码方向）已编码的MB（更一般地，分析单元）。
- 问：帧内预测是以已编码块的边界作为预测依据，也就是说当前块的RDO实际上与相邻的每个块的RDO都相关。新的编码标准中是否有考虑到这一点？答：好像是有的。不过就这个问题而言，性价比似乎不高。
- 框出编码器里面的解码器。

### 二 视频编码的基本原理

![颜色降采样，原因可能与锥细胞和杆细胞数量比例有关。\[@中国科普博览20190823微博\]](./image/G3/video-encoding/颜色降采样.jpg)

![此图没有红色](./image/G3/video-encoding/颜色降采样-2.jpg)

![像素采样模式 \[B11\]](./image/G3/video-encoding/像素采样模式.png)

------

**视频信息冗余**

![信源编码示意图](./image/G3/video-encoding/信源编码.png)

上图中，信源编码器的任务是将信号分成两部分：一部分是有关信源性质的大量共性知识；另一部分是剥离掉这些共性知识的少量特性信息。对于感知编码而言，编码器还会有选择地去掉人类无法感知的信息。这样，信号就被分成了**共识**和**特征**两部分，其中“特征”部分的数据量远小于“共识”部分，这样就实现了数据的压缩。在信道上传输的，实际上是信号的“特征”；而“共识”部分，则通过标准和协议的形式，固化在每一个信源编解码器中。

例如，假设已经知道（约定）信源是一个标准的正弦波，那么只需要传输这个正弦波的相位、频率和幅度就足够了，不需要传输每一个采样点。再例如，假设已经知道（约定）信源是一个安防监控探头采集的图像，由于图像绝大多数时间都是固定不动的，因此只需要传输每帧之间的差异、或者画面中运动的内容就足够了。

作为信源编码的一类，视频编码就是从视频信号中剥离掉共性的知识，如时域/空域/频域相关性、编码的上下文特征、分形的纹理特征、特定的语义内容（如人脸、物体、几何形状）等，并（在可接受的前提下）抛弃掉人类难以感知的信息（如精细的色度信息、难以察觉的空域高频成分），只留下少量相关性很低的信息，实现视频信号的压缩。因此，视频编码，可以认为就是视频压缩。

民国时代曾经有一种简写日期的方式叫做“韵目代日”，例如长沙“文夕”大火，指的就是“文日”（1938年11月12日）夜间的大火。将日期缩写为某个单字，以减短电文长度。韵目代日的规则，就是上面所说的“共识”。如今已经几乎没有人懂得这个规则了。

信源编码分为**无损**和**有损**两类。有损编码会损失掉原信号的部分信息，解码后得到的信号并不完全等同于原信号，会有一定的失真。编码器的一个重要任务是，在失真可接受的前提下，尽可能提高数据压缩的效率；或者在数据率可接受的前提下，尽可能减小失真。这是一个带约束的优化问题，称为**码率-失真优化**（Rate-Distortion Optimization）问题。RDO实际上解决的是三方面问题

- 更好的失真度量方法
- 更高效的压缩编码
- 更好的RDO算法本身

信源编码，是对信号的内在性质、乃至这个世界的深刻再理解。

------

**视频编解码框架**

当今应用最广泛的视频编码框架，是基于预测-变换的混合式视频编码框架。它的特点如下：

- 总的来看是一个带反馈的差分预测编码框架
- 框架前向通路中引入了变换编码
- 残差信号经过量化、变换后，再进行熵编码
- 编码器包含完整的解码器

![差分脉冲编码调制（DPCM）框架](./image/G3/video-encoding/DPCM.png)

![视频编码器框图](./image/G3/video-encoding/video-encoder-arch.png)

![H.264的混合编码框架 \[B1\]](./image/G3/video-encoding/H264-framework.png)

------

**编码工具和编码策略**

------

**编码性能评价**

### 三 变换编码与量化

**变换编码的理论基础**

**x.1** 自然的空域图像信号往往有大片的平坦缓变区域，如蓝天、皮肤、白墙等等。也就是说，在图像的局部范围内，像素之间的差异是比较小的。

**x.1.1** 从频域的角度看，**x.1**意味着图像信号中直流和低频成分往往占据大部分能量。

**x.1.2** 从统计的角度看，**x.1**意味着图像信号的相近像素之间有较强的相关性。统计表明，像素间距离越近，相关性越强。

![灰度差值的概率分布 \[B7\]](./image/G3/video-encoding/灰度差值概率分布.png)

**x.2** 我们的目标是，在不损失过多信息的前提下，去除掉空域图像信号的冗余成分，实现信号的压缩。

**x.2.1** 从频域的角度看，根据**x.1.1**，我们可以设法去除能量较低的高频成分，保留能量较高的低频成分。

**x.2.2** 从统计的角度看，根据**x.1.2**，我们可以设法去除图像相近维度之间的相关性。

**x.3** 为了实现**x.2**所述的目标，我们可以通过某种变换，将图像的主要信息集中到少数维度上，这就是变换编码的核心思想。

**x.3.1** 为了实现**x.2.1**，可以将图像信号从空域变换到频域，将信号的能量集中到低频段，并去除高频段能量较少的成分，保留低频段能量较高的成分，达到数据压缩的目的。从1968年，人们[B6]首先使用FFT计算图像的频谱，但FFT是在复数上进行运算，显得有些冗余。后来人们提出了针对实信号的**离散余弦变换**（DCT），它本质上是FFT的导出形式，但是比FFT的能量集中性更好，并且同样具有快速算法。

![DCT比DFT有更好的能量集中性能 \[维基百科DCT词条\]](./image/G3/video-encoding/DFT-vs-DCT.jpg)

**x.3.2** 为了实现**x.2.2**，可以对图像向量作主成分分析，尽可能去除像素间的相关性（至少去除线性相关性），将图像信号降维到少数几个特征值上，达到数据压缩的目的。许多年(?)以前，人们提出**K-L变换**（Karhunen-Loève Transform，KLT），也叫**霍特林变换**（Hotelling Transform），它本质是一种正交线性变换，可以在均方误差最小的条件下实现随机信号的降维。但是，KLT的基底由图像的统计特征（如协方差矩阵）决定，计算复杂度较高且无快速算法，因而KLT只有理论上的意义，实用性很差，一般只用作性能比较的基准。（至于KLT和PCA的关系，可以认为PCA是KLT针对离散变量的特殊情况）

**x.4** 存在多种实用的变换算法，可以达到信号去相关的目的。变换算法需要满足几个特性：①可逆的正交变换；②性能近似KLT；③计算复杂度不能太高。

**x.4.1** 此类变换算法可分为正弦类变换和非正弦类变换。正弦类变换有FFT、DCT、离散正弦变换（DST）等，非正弦类变换有Haar小波变换、Walsh变换等。

**x.4.2** 非正弦类变换计算量较小，但效果较差，而且难以直观解释。正弦类变换尽管计算量较大，但是其效果更接近KLT，并且具有频率域变换的直观含义，因而成为主流的变换编码算法。

**x.5** DCT既是一种具有去相关特性的线性变换，也具有显然的频域分析的物理含义。尽管DCT去相关性能略输于KLT，但它①与输入图像无关，不需要计算输入图像的统计特征，且②有快速算法，并③具有易于理解的物理含义，实用性更强。不过，可以证明，当信号统计特性近似于一阶马尔可夫过程时，DCT的去相关性近似于KLT。

**x.5.1** 依据构造形式不同，DCT有8种变形，常用的是DCT-II，其反变换可以使用DCT-III计算得到。

**x.5.2** DCT具有快速算法，可实现$O(n \cdot \mathrm{log}(n))$的时间复杂度。一种是直接使用蝶形结进行递归分解，另一种是通过FFT来计算DCT。

**x.5.3** 二维图像信号的DCT可以通过两次一维DCT实现，即按行/列作一维DCT，再对变换结果按列/行作一维DCT。

**x.5.4** 近年来，随着算力的提升，自适应多核变换AMT[B3-p26]、基于在线训练的KLT[B8-p41]等更为复杂的变换算法被引入新一代的视频编码标准。音频信号由于涉及分帧和边界问题，使用经改进的DCT即MDCT实现变换编码。

**x.6** 理论上，DCT应针对整幅图像进行，以去除所有像素之间的相关性。但实际上都是先将图像分块，对每块单独计算DCT。这样做的原因是：①对整幅图作变换的计算量太大；②如果对整幅图作变换，变换编码传输中引入的误码在反变换后会分散到整幅图像，如果分块则会将误码局限在单个块内；③考虑到1所述的局部缓变性质，多数小块内的高频信息极少，因而对这类平坦小块单独计算DCT可以比对全图计算DCT更充分地去除掉冗余信息。（视频截图）

**x.6.1** 分块编码会导致块与块之间的边界不连续，形成可见的块边缘，这称为**块效应**。

**x.6.2** 根据图像内容自适应地选择块的大小和尺寸，不仅有利于减轻块效应，还有利于提高编码效率。

![更精细的宏块划分有助于提高编码效率 \[C1\]](./image/G3/video-encoding/宏块划分对比.jpg)

**x.7** DCT变换存在反变换（IDCT），变换后的信号经IDCT可以恢复出原始图像信号。但由于变换信号的高频冗余成分经后续的量化环节被压缩掉，恢复出的图像与原始图像并不完全一致，因而变换编码一般都是有损压缩编码。

------

**离散余弦变换**

$$ X[k] = \sum _{n=0} ^{N-1} {x[n] \cdot \mathrm{cos}( \frac{\pi}{N} k (n + \frac{1}{2}) )} $$

![8×8离散余弦变换的基模式 \[B11\]](./image/G3/video-encoding/DCT-basis-patterns.png)

![Zig-zag 排列方式 \[A2\]](./image/G3/video-encoding/zig-zag.png)

------

**H.264的整数变换**

### 四 预测编码

**帧内预测编码**

**帧间预测编码（20190807）**

### 五 熵编码

- CAVLC
- CABAC

### 六 码率-失真优化（RDO）

### 七 视频编码标准

![视频编码标准简史 \[C3\]](./image/G3/video-encoding/history-of-video-coding.png)

![主流视频编码标准发展史 \[B5\]](./image/G3/video-encoding/Video-Encoding-History-2.png)

- MPEG-1/2/4、H.261/2/3
- H.264/AVC、AVS
- H.265/HEVC、H.266/VVC、AVS2、AVS3

**HEVC的改进**

**VVC的改进**

- 块大小：128×128
- 更复杂的子块划分方式
- 67种亮度帧内预测模式
- 仿射运动矢量
- 自适应多核变换（AMT）

![图像分割方式的改进 \[C3\]](./image/G3/video-encoding/H266-piction-partitioning.jpg)

**AVS3的改进**

### 八 视频编码的发展趋势

### 参考资料

- 参考资料目录遵循 GB/T 7714-2005《文后参考文献著录规则》的要求编排。
- 如无说明，所有参考文献均有自有备份（纸质、电子文件或截图）。

**A.标准文档**

- A1 / **ITU-T H.264** 视听及多媒体系统/视听业务的基础设施/活动图像编码/通用视听业务的先进视频编码 [S]（H.264/AVC国际标准）
- A2 / **ITU T.81** [信息技术/连续色调静止图像的数字压缩和编码/要求和准则](https://www.w3.org/Graphics/JPEG/itu-t81.pdf) [S]（JPEG标准）
- A3 / **GB/T 33475.2-2016** 信息技术 高效多媒体编码 第2部分：视频 [S]（AVS2国家标准）

**B.论文、胶片、教材和专著**

- B1 / T Wiegand, G J Sullivan, G Bjontegaard, et al. **Overview of the H.264/AVC video coding standard**[J]. IEEE Transactions on Circuits & Systems for Video Technology, 2003, 13(7):560-576.
- B2 / 毕厚杰. **新一代视频压缩编码标准:H.264/AVC**[M]. 人民邮电出版社, 2005.
- B3 / G J Sullivan. **Versatile Video Coding – The Next-Generation Video Standard of the Joint Video Experts Team**[R]. 2018.
- B4 / 中科大《多媒体通信》课程讲义[R]
- B5 / 马思伟. **新一代AVS3视频编码标准**[R]. RTC2019.
- B6 / H C Andrews, W K Pratt. **Fourier transform coding of images**[?/找不到原始文章]. Hawaii International Conf. on System Sciences, pp. 677-679, 1968.
- B7 / 张春田, 苏育挺, 张静. **数字图像压缩编码**[M]. 清华大学出版社, 2006.
- B8 / 马思伟. **视频编码未来简史**[R]. RTC2017.
- B9 / 高文, 赵德斌, 马思伟. **数字视频编码技术原理（第二版）**[M]. 科学出版社, 2018.
- B10 / K R Rao, 金道年, 黄在静. **视频编码全角度详解**[M]. 机械工业出版社, 2017.
- B11 / I E Richardson. **The H.264 Advanced Video Compression Standard (second edition)**[M]. John Wiley & Sons, 2011.
- B12 / J Watkinson. **The MPEG Handbook (second edition)**[M]. Taylor & Francis, 2004.
- B13 / 陈靖, 刘京, 曹喜信. **深入理解视频编解码技术：基于H.264标准及参考模型**[M]. 北京航空航天大学出版社, 2012.

**C.视频、网课、博客**

- C1 / PanasonicBusiness. [**H.264 Compression Technology**](https://www.youtube.com/watch?v=PmoEsPWEdOA). 2013.
- C2 / [LiveVideoStack博客](https://blog.csdn.net/vn9PLgZvnPs1522s82g)
- C3 / Mickaël Raulet. [从HEVC到通用视频编码的下一代视频压缩技术](https://blog.csdn.net/vn9PLgZvnPs1522s82g/article/details/106152333)
- C4 / [雷霄骅博客](https://blog.csdn.net/leixiaohua1020)

## FFmpeg实践

**DVD或其他非AVC格式转码为HEVC**

```
ffmpeg -i "INPUT" -vcodec libx265 -b:v <比特率> -preset ultrafast -acodec aac -b:a 256k "output.mkv"
```

**注意**：输出比特率按照以下规则设定：

- DVD：2000k
- 其他非AVC格式：与原始视频的比特率接近（或略低）。

除非有特殊需要，伴音一律压成256k的AAC。

**AVC转码为HEVC**

**视频合并**

思路是先打包成可以直接合并的TS流，然后将合并后的TS转换成所需的封装格式。

```
ffmpeg -i 1.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 1.ts
ffmpeg -i 2.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 2.ts
ffmpeg -i "concat:1.ts|2.ts" -acodec copy -vcodec copy -absf aac_adtstoasc output.mp4
```

**硬件Codec**

## 音频编码（2019-11-03）

![音频编码效率比较 \[维基百科Opus\]](./image/G3/audio-encoding/音频编码效率比较.svg)

### 基于PCM的语音编码

**G.711**

![A律压扩 \[D2\]](./image/G3/audio-encoding/G711-A-law-pcm.gif)

|输入采样值|A律压扩编码|解码后的采样值|
|-------------------------------------|
|`s0000000abcdx`|`[~s]000abcd`|`s0000000abcd1`|
|`s0000001abcdx`|`[~s]001abcd`|`s0000001abcd1`|
|`s000001abcdxx`|`[~s]010abcd`|`s000001abcd10`|
|`s00001abcdxxx`|`[~s]011abcd`|`s00001abcd100`|
|`s0001abcdxxxx`|`[~s]100abcd`|`s0001abcd1000`|
|`s001abcdxxxxx`|`[~s]101abcd`|`s001abcd10000`|
|`s01abcdxxxxxx`|`[~s]110abcd`|`s01abcd100000`|
|`s1abcdxxxxxxx`|`[~s]111abcd`|`s1abcd1000000`|

XOR 01010101

### 基于CELP的语音编码

![CELP编码器一般结构 \[维基百科CELP\]](./image/G3/audio-encoding/CELP.svg)

**G.729**

## MP3音频编码

### 如何定义音乐的混乱程度？

撰写于2020年6月29日

从信号特征的角度是可以定义的，并且在音频编码的码率控制（率失真优化）方面有重要的作用。

在MP3标准（ISO/IEC 11172-3）中，使用感知熵（perceptual entropy，PE）来衡量一个音频帧的“混乱程度”。容易理解的是，一段声音越“混乱”，它的信息量就越大，编码这段声音所需的码长（下限）就越长。

MP3编码器在编码一个音频帧之前，会使用心理声学模型计算这一帧的PE。PE代表了编码一帧所需的码长下限，编码器通过预先计算出来的PE来分配码长预算，以维持码率恒定。（当然，是在在CBR模式下）

那么PE是如何计算出来的？简单来说，PE的度量方法基于这样一个假设：认为声音分为tone和noise两类，二者在时域和频域上分别有不同的特征。在时域上，Tone的波形是可预测的（自相关程度很高），而noise不然。在频域上，Noise的频谱是“平坦”的（想象白噪声的频谱）且可预测的，而tone的频谱是由多个孤立峰值构成的。

在MP3标准所使用的第二心理声学模型中，频谱平坦度是通过各频点的几何平均与算术平均之比所度量的。

而PE之所以称为“感知”熵，是因为它充分考虑了人耳的听觉特性。首先，它是在非线性的critical bands上提取频谱；其次，它利用人耳的掩蔽效应和绝对听阈特性，主要是同时掩蔽效应，对计算结果作修正。

越是“混乱”的声音，如语音中的摩擦音、打击乐器发出的声音等，其PE越大，编码所需的比特数越多。一旦编码结果的长度超出了PE的预估，MP3编码器就会从比特预算中取出一些额度。这种码率控制机制是MP3的专利技术之一，不过现在已经过期了。

### 1 概述

MP3音频编码标准，自从上个世纪80年代末提出至今，已经有三十余年的历史了。MP3是第一个针对“宽带”音频的压缩编码标准，在此之前的技术大多是针对语音压缩而提出的。MP3发展至今，已经一统江湖，成为最流行的数字音频压缩标准，没有之一；“MP3”这个词，俨然成为数字音乐的代名词。由于我本人对于音响技术一直很感兴趣，再加上近期知识管理工作中对于MP3文件管理的客观需要，都需要对MP3编解码原理做一些深入的研究，直至实现最基本的编解码器原型。

MP3编码标准所使用的技术，包括子带滤波、心理声学模型、变换编码和熵编码，都是非常具有代表性的技术，横跨众多领域，奠定了众多后续技术的基础。包括AAC、AC-3、MP3Pro等更为先进的音频编解码技术，都与MP3有着类似的思路。因此，掌握MP3标准之后再学习其他更为先进的标准，就轻松多了。

经过这段时间的研究，我的观念有所变化。那就是在个人数据管理工作中，一味追求无损是没有意义的。无损PCM相比于320K的MP3，并不会带来多高的音质提升，却浪费了几倍的空间，实在是不值当。这与高清视频不同：大尺寸、高帧率、高位深，是肉眼能实实在在感受到的。所以，以后不必一味追求PCM无损，省下来的空间多保存一些4K乃至8K的超高清视频，这是更划算的。

全称 MPEG-1 Layer Ⅲ，对应国际标准为 ISO/IEC 11172-3。

![MP3编码器框图](./image/G3/audio-encoding/aqua-overview.png)

![MP3量化循环](./image/G3/audio-encoding/aqua-rdo-loop.png)

### 2 心理声学模型


![等响度曲线 \[C2\]](./image/G3/audio-encoding/equal-loudness-curves.png)

![频域（同时）掩蔽，以及NMR、SNR、SMR之间的关系示意 \[A3\]\(由\[B1\]重制\)](./image/G3/audio-encoding/mp3-simultaneous-masking.png)

![时域掩蔽 \[A3\]\(由\[B1\]重制\)](./image/G3/audio-encoding/mp3-temporal-masking.png)


### 3 子带滤波

![分析子带滤波器组的频率特性 [B1]](./image/G3/audio-encoding/mp3-filterbank.png)

分析子带滤波器组，用于编码器。

首先明确输入和输出形式。对于每帧1152采样，划分成36组长度为32点的采样段，将每段**分别**输入滤波器，执行36次32点子带滤波，得到长度为32的滤波后结果。则36个采样段可以得到36组长度为32的滤波后结果。

滤波后结果的每一点对应32个子带之一，所以换一个视角，子带滤波器组的输出结果是：32个长度为36的子带滤波结果，每个结果对应一个频带。

至于子带滤波器，它的信号流图以及算法参数都在11172标准中有定义，直接照抄就可以。

### 4 变换编码

![MDCT使用的四种窗口 [B1]](./image/G3/audio-encoding/mp3-MDCT-windows.png)

![混叠消除的蝴蝶结运算 [A1]](./image/G3/audio-encoding/mp3-aliasing-butterfly.png)

### 5 量化与码率失真优化

+ 量化会引入量化噪声，包括同时噪声，也包括前回声（尤其是长窗口MDCT情况下）。
+ 人耳存在掩蔽效应，凡是低于掩蔽门限的声音，都是无法察觉的。掩蔽信号（Masker）与其某个掩蔽门限之比，称为信掩比（SMR）。
+ 量化位数越多，精度越高，量化噪声越低，即信噪比（SNR）越高，代价是码率开销越大。
+ 因此，对于某个信号，在某个量化精度下，如果其量化噪声恰好低于掩蔽门限，则量化噪声无法察觉，那么此时的量化精度就刚刚好，再提高量化精度也没有感知上的意义了。
+ 定义掩噪比MNR=SNR-SMR（标准如此），显然，如果某个量化位数使得MNR等于0，则此量化位数就是能够不引起可察觉量化失真的最低位数。
+ 编码器通过内部的循环，确定每帧的最佳量化位数，以实现VBR。

量化和熵编码模块是MP3编码的核心之一，具体算法按下不表，这里只需要知道：主控喂给此模块一个576点的频域granule，同时指定编码长度的“预算”，此模块就可以给出granule的二进制编码，并保证其长度不超过预算。

每个granule分配多少码长预算，取决于比特储备的余量，以及其自身的“复杂”程度。具体的分配机制是本文的核心，这个放在后面讲，现在来简单介绍一下，如何衡量granule的“复杂”程度。

音频信号的“复杂”程度是不同的。直观上可以这样理解：例如大镲的声音在各个频率上都有比较强的成分，而钢琴的声音只在离散的几个泛音上比较强，且越往高频越弱，所以编码大镲的声音所耗费的比特数，要多于钢琴。

每个granule的“复杂”程度，由感知熵（PE）衡量。感知熵由编码器的心理声学模型（PAM）计算得到，它衡量一个granule在不产生人耳可闻噪声的前提下，能够压缩到的信息量下限。PE乘以3.1，就是一个granule不产生可闻失真的压缩编码长度下限（单位bit），下文称“最小无噪压缩码长”。

在128kpbs及以上的码率下，常见的音乐信号中只有少数（320kbps下往往不足一成）granule的最小无噪压缩码长比平均码长要大，绝大多数granule的最小无噪压缩码长都不会超过平均码长。

### 6 熵编码

### 7 码流合成

> 20191223：比特储备池派发（make available）的比特数，再加上mean_bits，即为量化循环可用的比特数（max_bits）。
之前一直没搞明白这个因果关系：是比特储备池给量化循环派发比特额度，然后量化循环去利用这个额度，而不是反过来。
然后，量化循环完成后，一般都会剩下一些没有用完的额度，这个剩余的额度就会捐献给比特储备池。
如果一顿操作后，比特储备池还是超过了可用容量，那么就需要给量化完的Granule增加填充比特，这实际上是修改Granule的part23Length，同时比特储备池的容量会相应地减少，恢复到正常容量。

一言以蔽之，比特储备（bit reservoir）机制可以在恒定码率（CBR）的前提下，允许帧长度在一定的范围内弹性地变动。这样，既能保证比特流的码率不变（从而方便地实现定位、裁剪等处理），又可以赋予比较“复杂”的音频帧以足够的编码空间额度，保证编码质量。

如无说明，以下只考虑CBR模式。因为只有CBR模式才需要用到比特储备机制。

MP3比特流是由一连串的帧（frame）构成的。每个帧的结构如下图。

![ ](./image/G3/audio-encoding/mp3-frame.png)

图中Header是帧头，描述了比特率、采样率、声道数等基本信息。之所以每一帧都要带上这些信息，一方面是应用场景决定的，比如流媒体、实时通信场景，一般不会事先约定这些信息。另一方面是对于VBR模式，每一帧的实际码率都可能不一样，因此需要单独声明。

Header的开头是连续12个1组成的同步字。解码器在比特流中搜索同步字，以得到一帧的开头。在某些极端情形下，在不是帧开头的位置上也可能出现连续12个（或以上的）1，这一般是由于辅助信息的格式设计不当，因此11172标准中特别提醒，尽量不要在辅助信息中出现同步字，妨碍帧头的定位。标准规定的哈夫曼码表不会出现连续12个或以上的1。

CRC是循环冗余校验码，可有可无。

Header和CRC在MP3比特流中的位置是固定的，这有利于比特流的定位（同步）。连续两个帧头的间距frameLength为（单位bits）

```
frameLength = (bitRate * 1152) / sampleRate + (paddingSlot * 8)
```

在44100Hz采样率下，计算出来的间距是小数。因此为了保证码率不会因为小数部分的偏差而漂移，需要在个别帧中插入额外的填充字节，以抵消累积的误差。以320kbps/44100Hz举例，按照上面的公式，每帧长度约为1044.9字节，而帧的长度只能是整数个字节[注]。如果采用1044作为帧长度，那么若干个帧累积起来，就会有相当可观的误差——实际码率小于320kbps。因此，每隔几个帧，就有一个长度为1045字节的帧，以抵消每帧0.9字节的短少累积起来的总的码率误差。这与历法置闰非常类似。

> **注**：MPEG-1规定的是帧头之间的「槽」（slot）数相等，且 Layer 3 每个槽长度为1个字节，因此说“帧的长度必须为整数个字节”。

式中1152代表每帧有1152个时域（频域）采样点。这是标准规定的常数，所有编解码算法的参数（例如窗口大小等）都取决于这个数。

每帧1152采样点被分为两个granule（颗粒，或者称子帧，下文直接用英文指称之），每个granule有576个采样点。granule是编码的最小单位。由于音频分帧相当于截断，因此为了避免截断带来的噪声，需要对相邻帧作50%的重叠。使用考虑重叠的改进的DCT（即MDCT）对时域数据进行处理，输入是经重叠的2n个点，输出是n个点。因此MP3设定每帧1152点包含2个576点的granule，无论是子带滤波还是量化、熵编码，都是以granule为最小的编码单位。解码时，可从576点的granule经过IMDCT恢复出1152点的时域帧，经重叠-相加算法即可恢复出分帧前的信号。

由于帧头间隔由码率和采样率决定，因此可以认为是平均帧长。鉴于每帧有2个granule，因此每个granule的平均长度就等于平均帧长的一半，且仅由采样率和码率（以及声道数和是否“置闰”）决定。

实际编码后的granule的码长往往少于平均长度，但也有比平均长度长的情况出现。这与granule自身的性质有关，将在下文介绍。

图中竖直的红线代表 MainData 的起始点，之所以特别标注，是因为这个起始点一般位于本帧帧头的前面，也就是上一帧的位置。MainData起始点距离边信息SideInfo结束位置的向左的偏移量（字节），记录在边信息的main_data_begin字段，其最大值为511，意味着MainData最多偏移511字节。这个偏移量的确定方法，就是本文所讲的比特储备机制。

![ ](./image/G3/audio-encoding/mp3-main-data-begin.png)


### 8 元数据

### 9 LAME使用方法

### 10 开发笔记

**20191217**

> 这次研究MP3编解码算法，从11月初开始到现在已经有一个半月，进度还是很快的。
总结了Scheme解释器开发的经验教训，这次从一开始就有比较客观、宽松的心理预期，做好了打持久战的准备。
比较深刻的感受是自己的工程能力确实不行。越是到精深微妙之处，越感觉自己在驾驭复杂的系统上捉襟见肘、顾此失彼，更遑论优化和规范了。
工程的事情，经验很重要。做过和没做过，完全不一样。MP3编码器这个东西，不是个小东西。dist10编码器部分有1万多行C，自己实现的截至目前大概有2千行有效js代码。客观地说，它门槛不低，一方面是算法上的复杂性，光是信号与系统的知识就足以劝退大多数人；另一方面是工程上的复杂性，因为是底层技术，它本身就是一个库，所以要亲手处理很多细节问题。这都是很大的挑战。
那么做这件事的价值是什么呢，有很多方面。能对一个项目保持一个多月的稳定的学习输入，对我来说算是比较难得，根本原因在于兴趣。一直对音频技术感兴趣。所以第一个价值就是满足好奇心，做一个编码器出来也算是给自己一个交代。第二个价值就是功利方面的，我觉得这东西是足够牛逼，足够硬核，足够写进简历的。并且，音视频这个领域我觉得未来还是会有比较重要的地位。
一点简单的复盘，希望我能坚持做完这件事。

> 昨晚爆肝一夜，把噪声控制循环做完了。
预加重暂时不做，混合块模式不打算做。
用EncSpot观察了一些MP3文件的详细信息，发现几乎所有现有的文件里，长块都占97%以上，短块极少，混合块想必更少了。
验证计划：Hybrid filterbank 是无副作用的模块，其结果是相对值得信赖的。量化循环则比较复杂，也是最不靠谱的部分。所以解码器部分，除了已经基本完成的哈夫曼解码、频谱重排之外，反量化、前端也要做一下，验证MDCT输出的结果在编解码后是否一致就可以了。长块短块都要验证。
还有PAM2，照葫芦画瓢是可以实现的，目前看来没什么实现上的难度。PAM2算是比较紧迫的需求，因为现在对噪声容限的值没有任何直观概念，不利于调试。
比特池（所谓的reservoir）目前不理解，需要研究dist10源码。
scfsi放在最后实现，可以直接抄dist10。重在理解原理。
最后要强调一点就是MVP导向的开发，可视化是一定要有的。代码即文档。

## WAV文件格式

WAVE文件格式是RIFF格式的其中一种，一般用来存储PCM音频数据，也可以存储其他格式。以下是典型的WAV文件布局。

![WAV文件格式](./image/G3/audio-encoding/wav-format.png)

- 标红的部分是固定不变的部分。
- 所有数值字段均为小端模式，即高位字节在高地址，低位字节在低地址。
- 格式块字节数：指的是fmt块（蓝色）除了前两个字段以外的其余部分的字节数，固定为16。
- 音频格式：固定为1，代表PCM。
- 声道数：1为单声道；2为双声道。
- 采样率：指每秒钟PCM采样数。
- 每秒字节数：等于采样率×每个采样的字节数。
- 采样字节数：每个采样的字节数，包含所有声道，等于采样位深×声道数÷8。
- 采样位深：每个声道每个采样的位数，一般为8、16。
- 数据块字节数：即所有PCM数据的总的字节数。
- 0x24处可能有一可选块“fact”，PCM格式中一般不会出现。

## 音频编码参考资料

### MP3

**说明**：首先推荐一个MP3资料收集网站，有许多论文和技术资料可供下载：[http://www.mp3-tech.org/]()。以下参考资料中，凡是此网站有收录，或者很容易找到的，不再给出链接；不太容易找到的，将给出链接，方便读者溯源。部分文献资料可联系本文作者索取。

标准文档及其相关解读，标准文档当然是最高依据：

- A1 / **ISO/IEC 11172** - Coding Of Moving Picture And Associated Audio For Digital Storage Media At Up To About 1.5 Mbit/s - Part 3: Audio
- A2 / Pan D . **Tutorial on MPEG/audio compression**[J]. IEEE Multimedia, 1995, 2(2):60-74.
- A3 / Noll, P. [**MPEG digital audio coding**](https://www.csd.uoc.gr/~hy438/lectures/MPEGAudioCoding.pdf)[J]. IEEE Signal Process Mag, 1997, 14(5):59-81.
- A4 / Raissi R. **The theory behind MP3**[J]. MP3’Tech, 2002.

较好的中文论文，原理与实现并重，可供快速入门：

- B1 / 張芷燕. **MP3編碼法之研究與實現**[D]. 国立交通大学（台湾省）, 2002.

有关心理声学和感知编码的文章，可供参考：

- C1 / Painter T, Spanias A. **Perceptual coding of digital audio**[J]. Proceedings of the IEEE, 2000, 88(4): 451-515.
- C2 / Robinson D J M. **The human auditory system**[C]//107th convention of the Audio Engineering Society. 1999: 1-13.

其他：

- https://rarewares.org/

### 语音编码

- D1 / 赵力. **语音信号处理（第3版）**[M]. 机械工业出版社, 2017.
- D2 / [G.711 codec process](http://www.en.voipforo.com/codec/codecs-g711-alaw.php)


## 彩蛋：Lena原图

![ ](./image/G3/Lena原图.jpg)


# 机器学习理论

![机器学习模型训练的一般框架](./image/G4/ml-model-training.png)

![ ](./image/G4/揶揄人工智能-2.jpg)

## 最优化

> 2024-02-21

> 现在遵循控制论思想原则的人工智能实现方法，如机器学习、强化学习等等，都可以归结为两个字：寻优。

> “优”反映出人类对于问题域的理解，为解决方案注入人类的主观和先验，是问题的最终目标。机器学习我们叫损失函数、距离度量等等，概率统计我们叫似然函数、交叉熵一类的东西，理论物理我们把它称为“作用量”、哈密顿量等等。甚至我们常说的奥卡姆剃刀、正则化等等，它们反映了人类对于问题性质的某种信念，因而也属于“优”的范畴。

> 而“寻”，则是达到“优”的途径和方法。最优化理论就是“寻”优的理论。而落实到实践上，除了少数定义明确、性质良好的问题类，例如凸优化，有相当优雅高效的理性算法，大多数寻优方法都是所谓启发式的、出于人类对于物理世界的模仿的直觉。例如梯度下降，现在深度学习大模型的训练“寻”优手段，实质上就是梯度下降，其核心思想十分朴素，却相当有效。这种有效性可能反映出问题域的某种内在的结构。再例如遗传算法模仿生物进化、模拟退火模仿晶体退火过程中粒子各归其位等等。每种寻优算法在某些问题上相当有效，却不见得在任何问题上都有效，因而寻优方法和具体问题之间如何良好匹配，就构成了元寻优问题，因而相应出现了元启发式寻优算法。

> 进一步地，很多、甚至绝大多数情况下，我们甚至很难定义或者验证这个所谓的“优”，具体来说，就是找不到能够良好定义的目标函数，或者目标函数计算成本太高。既然“优”难以定义，那怎么寻呢？黑盒优化试图解决这类问题。极其粗糙地讲，黑盒优化的诸多手段，可以概括为“有根据地猜”。这也符合我们一般人在决策和判断时的行为策略。

> 在我个人看来，从可计算性理论的视角来看，所有的“计算”问题，都可以分为“判定”和“寻优”两大类。布尔可满足性（SAT）问题就是一个沟通了判定和寻优两大类问题的经典问题。经典问题之所以经典，是因为它具体而微。但是小不意味着简单喔。考虑到问题可以依复杂度归约，研究个别经典问题，有助于推广到一大类问题。而这个画头像的小demo，虽然简单而无用，但是我觉得它的内部深不可测。这种感觉完全出于我的无知。因此我时常提起这句话：

> **“实现了一个东西，不等于你理解了它。”**

### 优化稳定性与MIMO（2024-03-20）

① softmax的上溢问题。前段时间手写的多层感知机可视化，使用了sigmoid激活函数，没有遇到softmax上溢的问题。然而，如果把激活函数换成ReLU，则网络输出NaN。这是因为自己实现的softmax没有处理数值上溢的问题，从ReLU层输出到指数函数的数值过大，致使softmax层的输出变成infinity，导致后面各层变成NaN。

② 梯度下降的稳定性。自己手搓的梯度下降是最为朴素的恒定学习率小批量梯度下降，没有引入任何动量机制或者学习率调节机制。事实上，在神经网络的优化过程中，常常会在目标函数中遇到诸如鞍点、局部极小值、深而长的峡谷等“地形”，在这些位置上，简单的梯度下降优化往往会失效。在数学上，这些“病态”的情况可以用目标函数的Hessian矩阵来描述，而Hessian矩阵可以被分解成一组特征值，这些特征值足以描述目标函数在某点处的“地形”。具体地说，每个特征值都描述了目标函数在特定方向上的“曲率”。因而，如果目标函数某点Hessian矩阵的所有特征值都是正的，意味着此处是局部极小值。如果特征值有正有负，则此处可能是鞍点。而如果特征值的绝对值大小差异很大，直观上意味着此处“地形崎岖”，梯度下降容易迷路。[此处](https://en.wikipedia.org/wiki/Test_functions_for_optimization)有一些优化算法的测试函数，大多相当“崎岖”。

③ 梯度下降的改进。下图是自制的梯度下降可视化，手工绘制了一个狭长的深谷，这就是梯度下降的目标函数。在深谷峭壁上Hessian矩阵的各个特征值，也就是地形在各个方向上的“曲率”，差异很大，这意味着深谷的地形相当崎岖，因而在图中可以看到呈Z字形的梯度下降轨迹。这是因为朴素的梯度下降算法无法感知到地形的崎岖程度，只顾着找眼下最陡峭的方向猪突猛进，这样做的后果就是走弯路，甚至错过真正的极/最值点。因此，诸如AdaHessian之类的优化算法，可以感知到地形的崎岖程度，选择最合适的动量和学习率，尽可能少走弯路。而衡量“地形崎岖程度”的量，被称为Hessian矩阵的“条件数”，它是Hessian矩阵绝对值最大和最小的特征值的比值。Hessian矩阵的条件数越大，意味着目标函数越崎岖，梯度下降就要更加小心，以免迷路。

![ ](./image/G4/gd-ill.jpg)

④ 条件数与病态问题。在机器学习模型训练过程中，如果目标函数非常“崎岖”，意味着较小的位置变化可能会造成较大的上升和下降，这是一种不稳定的“病态”的情形。举一个更加形式化的例子来说明这个问题，线性方程组的系数，仅仅1%的微小扰动，就导致方程的解产生百倍的波动。如果矩阵的条件数非常大，意味着较小的误差会被放大成较大的波动。特别地，正交矩阵的条件数等于1，这意味着它在处理信息的时候，既没有损失，也没有冗余。直观来看，W矩阵的各行的相关性是很强的，意味着较大的条件数可能意味着较大的信息冗余（或者说线性变换的信息瓶颈），在W矩阵上损失的信息，就会在x矩阵中体现为巨大的不确定性。

$$ 令 \textbf{W}_0 = \begin{bmatrix} 100 & 100 \\ -100 & -101 \\ \end{bmatrix} \quad , \quad \textbf{b} = \begin{bmatrix} 1 \\ 1 \\ \end{bmatrix} $$

$$ 解方程 \textbf{W}_0 \textbf{x} = \textbf{b} \quad , \quad 得 \textbf{x} = \mathbf{W}_{0}^{-1} \textbf{b} = \begin{bmatrix} \color{blue}{2.01} \\ \color{blue}{-2.00} \\ \end{bmatrix}$$

$$ 令 \textbf{W}_1 = \begin{bmatrix} \color{red}{99} & 100 \\ -100 & -101 \\ \end{bmatrix} \quad , \quad \textbf{b} = \begin{bmatrix} 1 \\ 1 \\ \end{bmatrix} $$

$$ 解方程 \textbf{W}_1 \textbf{x} = \textbf{b} \quad , \quad 得 \textbf{x} = \mathbf{W}_{1}^{-1} \textbf{b} = \begin{bmatrix} \color{blue}{-201} \\ \color{blue}{199} \\ \end{bmatrix}$$

⑤ 深度学习中的训练稳定性问题。上周末用MLP解决Q问题（数字数圈圈找规律）时，一开始没有打乱训练数据集，每一批次参与训练的数据可能有较强的相关性，训练迟迟无法收敛。这可能就是因为样本之间相关性过强，样本矩阵的条件数过大，导致网络权值矩阵大幅震荡，从而难以收敛。将训练数据打乱后，网络很快就收敛了。另一方面，在深度学习的工程实践中，尽管诸如FP16、BF16甚至整数精度的训练可以有效节约显存、提升运算效率，但是由于数值精度的下降，在某些病态情况下，运算结果可能会大幅波动，导致训练失败。因此，在成熟的训练框架中，引入了自动混合精度（AMP）策略，可以通过梯度缩放等手段，避免因数值精度问题导致的训练不稳定性。不过，大模型提醒我，低精度训练也可以视为一种正则化手段，这大概就是无处不在的辩证法吧。

⑤ MIMO系统的预编码。在无线通信技术当中，对于多收多发MIMO系统，简而言之，多个收发天线之间的无线信道可以表示成矩阵H，它描述了多个收发天线两两之间的信道特性，这跟用矩阵来描述全连接神经网络的权值有相似之处。假设发射信号为x，经MIMO系统接收到的信号为y，那么对于MIMO接收机而言，它接收x的过程，实际上就是已知y、求解矩阵方程y=Hx的过程。然而，在Massive-MIMO系统中，H很大，并且未必可逆，因此求解过程的计算复杂度非常高。因此，工程上可以对H作奇异值分解，将H分解成UΣV，其中UV都是正交矩阵，而Σ是对角矩阵。U和V是收发两端通过CSI（信道状态信息）协商得到，在数学上（如图），可以通过简单的对角矩阵Σ来求取发射信号x，这就是MIMO的预编码技术。

![MIMO预编码原理，[来源](https://www.sharetechnote.com/html/BasicProcedure_LTE_MIMO.html)](./image/G3/mimo-precoding.png)

⑥ MIMO信道的层和条件数。MIMO信道的层，等于H的秩，也就是奇异值矩阵Σ对角线上非0值的个数。如果H是满秩的，意味着所有的信道都可以用于通信。然而，满秩不意味着最高的通信效率。如上文所述，MIMO信道的条件数定义为Σ对角线数值的差异，如果H的条件数约等于1，意味着所有信道都被利用、都是正交的、互不干扰，因而可以更高效地利用信道带宽，在较低的信噪比上实现较高的通信速率。

### 梯度下降演示（2018-05-24）

<iframe class="MikumarkIframe" src="./html/sgd.html" height="450px"></iframe>

- 多个同参数高斯函数叠加形成峰谷
- 点击坐标区域，添加极小值
- 点击**选择起点**，可选择迭代起始点
- 再点一次**选择起点**按钮，退出起点选择
- 点击**切换为极大/小值**，可切换极大/小值
- 参考：[Test functions for optimization](https://en.wikipedia.org/wiki/Test_functions_for_optimization)

### 模拟退火演示（2020-07-16）

<iframe class="MikumarkIframe" src="./html/simulated-annealing.html" height="450px"></iframe>

- 多个同参数高斯函数叠加形成峰谷
- 点击坐标区域，添加极小值
- 点击**选择起点**，可选择迭代起始点
- 再点一次**选择起点**按钮，退出起点选择
- 点击**切换为极大/小值**，可切换极大/小值

- 参考：[模拟退火](https://zh.wikipedia.org/wiki/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB)
- 参考：[HSV色彩空间](https://zh.wikipedia.org/wiki/HSL%E5%92%8CHSV%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4)

### 遗传算法求解旅行商问题（2019-04-29）

<iframe class="MikumarkIframe" src="./html/genetic-tsp.html" height="460px"></iframe>

### 遗传算法拟合任意图像（2024-02-21）

受到fwjmath的博客文章《[遗传算法：内存中的进化](https://fwjmath.wordpress.com/2009/03/02/genetic-algorithm-evolution-in-memory/)》启发而制作。以下引用该文：

> 从前在海岸边有一群扇贝在悠哉游哉地生活繁衍着。它们自然是衣食不愁，连房子也有了着落。它们担忧的只有一件事：每隔一段时间，总有一个人来挖走它们之中的一部分。当然啦，挖回去干什么这大家都知道。但扇贝们不知道的是，这人的家族图腾是Firefox的图标，所以他总是选择那些贝壳花纹长得比较不像Firefox图标的扇贝。这种状况持续了好几十万代。大家应该也猜到扇贝们身上发生什么事情了：它们的贝壳上都印着很像Firefox图标的图案。

<iframe class="MikumarkIframe" src="./html/genetic-image.html" height="200px"></iframe>

太史公曰：经常看到有人惊呼生命是多么神奇（进而引申到神创论的观点）等等。生命之所以神奇，不是因为生命多么神奇，实在是因为人类少见多怪。

### 迷宫生成与A*寻路算法（2024-02-26）

<iframe class="MikumarkIframe" src="./html/maze.html" height="400px"></iframe>

## 折中与约束

**各领域的“不可能定律”**：2021年夏天所做的关于智能通信的技术报告，最后表达了我的一个观点，就是在科学技术研究中，要注意那些[根本的限制和约束](https://zh.wikipedia.org/wiki/%E6%82%96%E8%AE%BA%E5%88%97%E8%A1%A8)，它一方面框定了解空间的范围，提醒研究者不要做无用功；另一方面其实也在启发研究者跳出局部，实现从exploitation向exploration的跃迁。工程的肮脏，其深层次原因就是必须在这些不可能因素之间寻求权衡。某些问题暂未发现或者确认“不可能定律”，说不定有最优解，可以尝试；而对于有“不可能定律”的问题，则应放弃执念，寻求权衡。这就进入了工程的领域。比这些跷跷板更本质的原因，是否是某种守恒？即某种指标（如本质复杂度）只会转移而不会消失。这里就来总结一些表述了某事“不可能”的定律。

- 哥德尔不完备定理、阿罗不可能定理、香农三大定理、热力学定律、NFL（没有免费午餐）、CAP（一致、可用、分区容错）
- [布雷斯悖论](https://zh.wikipedia.org/wiki/%E5%B8%83%E9%9B%B7%E6%96%AF%E6%82%96%E8%AE%BA)

**工程上的折中**

- Cache地址映射的方式：直接映射和全相联映射是两个极端，而组相联是折中。
- 实时频谱仪处理实时性和扫宽之间的矛盾：扫宽不大于实时带宽，则实时；否则将扫宽按照某个适当的实时带宽分成几段进行扫描；极端情况是实时扫宽为最小，退化为完全的扫频式频谱仪。
- 实现Scheme解释器/虚拟机的时候，处理异步事件的方式：一个极端是每执行一个同步指令就检查一次异步回调，另一个极端是不管异步回调以保证同步指令的性能。折中方案是每次执行一组若干个同步指令后，再去检查异步回调。（也可参见Python的GIL）
- 《微波工程》第四版P106：空气填充的77Ω同轴线衰减最小，30Ω同轴线功率容量最大。因此通信领域选择50Ω作为折中，而广播电视领域选取75Ω。
- 码率降低可以节省带宽，避免发射端吞吐不足；但是低码率又势必会提高编码延迟，反过来拖累吞吐，合着横竖都不好过，唉，还是对编码器开刀吧，编码速度没有三五十倍实时还好意思叫编码器吗。（2022-07-08：首次实现基于自研MP3编码器和GNURadio的端到端数字音频无线传输）

## 分类、回归

### 单层感知机演示（2018-05-26）

<iframe class="MikumarkIframe" src="./html/perceptron.html" height="450px"></iframe>

若数据集线性不可分，则会陷入无穷迭代。此例二维空间上线性可分的判定，可采用凸包+扫描线算法解决。
左下角(-160,-160)，右上角坐标(160,160)
感知机是简单的线性二分类模型，是神经网络和SVM的基础。这里使用随机梯度下降方法对其进行训练。

### 支持向量机演示（2018-06-02）

<iframe class="MikumarkIframe" src="./html/svm.html" height="500px"></iframe>

: 左下角为原点，右上角坐标(300,300)

+ SVM核心使用第三方实现：[SVMJS](https://cs.stanford.edu/people/karpathy/svmjs/demo/)（[GitHub](https://github.com/karpathy/svmjs/)）
+ 参考资料：[支持向量机：理论、算法与拓展](https://book.douban.com/subject/3927560/)

## 聚类、变换、压缩

## 集成学习

## 概率图模型

## 神经网络和深度学习

### 多层感知机可视化（2024-03-06）

<iframe class="MikumarkIframe" src="./html/mlp.html" height="600px"></iframe>

### 可微分计算

- [矩阵求导术（上）](https://zhuanlan.zhihu.com/p/24709748)、[矩阵求导术（下）](https://zhuanlan.zhihu.com/p/24863977)
- [Matrix Calculus](https://www.matrixcalculus.org/)

### Transformer

![Transformer语言模型](https://raw.githubusercontent.com/bd4sur/Nano/refs/heads/master/doc/nano-llm.png)

![多头注意力](./image/G4/multi-head-attention.svg)

![[来源](https://www.youtube.com/watch?v=-9vVhYEXeyQ)](./image/G4/bert-3d.png)

- https://arxiv.org/abs/1706.03762
- https://jalammar.github.io/illustrated-transformer/
- https://nlp.seas.harvard.edu/2018/04/03/attention.html
- https://erdem.pl/2021/05/introduction-to-attention-mechanism

# 人类知觉智能(NLP/CV/多模态)

2023-12-03：前几年，在人工智能领域，有一种学科划分方式，将NLP、知识工程划分为“认知智能”，将计算机视觉、语音、触觉等涉及视听和感觉的，划分为“感知智能”。然而，近几年的研究表明，“认知”和“感知”不应分家，多模态融合才是正确的发展路线。因此本章的标题是“人类智能”，将自然语言处理、机器视觉、语音等学科，统统纳入“人类智能”的范畴，或者称为“知觉智能”。此处再次强调我关于所谓人工智能的观点：**人工智能的使命是理解世界，而不是理解人类。**

## 大规模语言模型

![ ](./image/G4/揶揄人工智能-3.jpg)

### LLM调查研究笔记

**2025-02-17**：在vswd1上基于自己改写的llama2.c运行DeepSeek-R1-Distill-Qwen-1.5B，可以达到4.5tokens/s。

![ ](./image/G4/deepseek-v9200.jpg)

**2025-02-15**：在4卡P40/128GB-DDR4机器上跑DeepSeek-R1（1.56bit量化），可以达到1.5token/s的速度。不过调整GPU/CPU分配比例和其他参数，应该还有提升的可能性。从监控看，算力利用严重不足，看来确实是memory-bound。

**2025-02-14**：在联想拯救者R9000P（96GB内存）上运行ollama推理量化的DeepSeek-R1-Distill-Qwen-32B（Q4_k_m）可以达到8token/s以上。

**2025-02-13**：在RK3588上仅CPU运行DeepSeek-R1-Distill-Qwen-14B（Q4_k_m），回答射频常识题，非常好。速度大概2token/s。

**2024-12-30**：经过一轮预训练（256亿词元）和一轮监督微调（1288万条QA对）之后的效果：终于知道澳洲的首府在哪里了。但仅此而已，还是很蠢，没有实用价值。计划在2025年1月中旬结束训练。届时将完成两轮监督微调。后续不再折腾了。

**2024-12-24**：评[Meta的工作](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)：调教电子鹦鹉的时候就觉得词表这东西很没有智慧，没有智慧就在于它掺杂了人类对于“token”的理解。所以我自己的电子鹦鹉就没有词表的概念，就是unicode进unicode出。是时候把碳基彻底赶出硅基了。

**2024-12-20**：预训练持续40天，吃掉256亿个词元，验证集损失2.1，到此为止。今天开始，在12000000条问答语料上进行监督微调，至少一轮。

**2024-12-19**：魔改AK大佬的llama2.c，使其支持Qwen2(.5)的推理。Qwen2(.5)的模型结构，相比Llama2，只是在QKV上加了偏置，所以改动很小。

**2024-12-16**：自己拍脑袋设计词表的一个严重缺陷是…我忘了还有日本新字形…我只是简单地把两个国标里面的简繁体汉字塞进词表里。当然了，训练语料里想来也没有日文。

**2024-12-14**：自制168M电子鹦鹉在BBU上跑起来了，速度可以达到20个token/s。①上面那块VSWd1比下面那块更快，可以达到40token/s，估计是因为上面跑的任务比较少，看样子应该是2020年出厂的早期产品。②交换板手动关机后似乎会自动重启，重启后会重置系统，系统应该是存储在板子上的ROM里，每次上电都会写到SSD里，从SSD启动。

**2024-12-12**：虽然现在大家的兴趣都在1G以下的小模型上，但它的价值确实是要打个问号的。既然是说人话，那么“好”还是要比“快”重要一些。鹦鹉虽然会说人话，但是没人真的想跟鹦鹉唠嗑吧。归根结底，菜是原罪。不过，在封闭世界中的某些场景下，处理一些不那么严肃的任务，比如______，我觉得还是挺有价值的。等我的____模型炼成之后，第一件事就是郑重声明：电子鹦鹉所说的任何话，本人一概不承认。

**2024-12-09**：是从头训练小模型？还是把大模型压缩成小模型？根据[这篇文章](https://arxiv.org/pdf/2412.04315)，看来，依靠剪枝蒸馏量化把大模型压缩成小模型，还是把太多的人类自作聪明的理性因素引入了大模型，反而导致模型能力密度下降。而从头训练小规模模型，让小模型自己在大规模高质量语料中学习，更有利于得到知识密度更高的小模型，接近率失真优化的极限。此中得失，需要权衡。

**2024-12-04**：这个VAR前段时间注意到了，还clone下来跑了起来，效果挺好的。因为以我的数理水平很难理解扩散模型，另外近期也有在玩自回归语言模型在NLP以外问题上的推广，所以对这个“下一分辨率生成”的自回归生成范式很感兴趣。我个人觉得这个思路很有前途，它是在频域上操作，比空域上生成patch的思路更合乎直觉。

**2024-11-28**：近年来大模型业界习惯用“B”指代十亿，而我打算杜绝这种不规范的新用法。以后凡是看到“B”这种用法，在我这里一律改成SI词头“G”（吉咖）。另外，将严格区分SI十进制词头和二进制词头。例句：168G参数的语言模型，其检查点的大小约为1.9GiB。在64GiB的显存上训练时，批大小最大可以设置为0.08k个样本。

**2024-11-28**：上月训练的56M语言模型，成本高于1克黄金，智力低下，几乎无法解决任何实际问题。终端部署是能行的，性能是可以接受的，唯一有潜力的是瑟琴机器人这种恰好需要幻觉但是创造力又很有限的场景。而现在正在训练的168M模型，已经训练了半个多月，吃掉了100多亿词元，但是远远不够。目前看到的各种数百数千M级小规模模型，包括Qwen、Phi、SmolLM、MiniCPM等等，在解决事实性问题上都不是很堪用，但是在解决通用的语义理解类问题上还是有一定用处的。监督微调归根结底还是体力劳动，我个人没法承担这么大的工作量。

**2024-11-18**：Nano的WASM推理引擎现已支持LoRA插件…因为我戴了__色的眼镜，所以看什么都是__色的～但更令人感叹的是NLP的奇妙的发展史。下图左边是2018年整的一个小活，只是一个ChatBot的静态前端界面而已，无论输入什么都只会返回一个meme图，唯一值得说的就是上古时代的RAG——点击图片会以上一句话为关键字去搜索百度。虽然当时AttentionIsAllUNeed已经发表，但是绝对无法预料人类究竟何时/能否通过图灵测试。短短6年后，我有了右边那个东西。能否通过图灵测试不知道，但是当一只____也很令人称奇了。**打造电子鹦鹉，是我试图回答“人类的本质是什么”这个难题的一个现象学的进路**。

![ ](./image/G4/nano-neko-lora.jpg)

**2024-11-17**：Nano浏览器推理现已支持WASM加速！按需加卸载LoRA模块机制稍后加入。最近研究了一点WASM，像这种栈式虚拟机长得都差不多，有意思的是适配各种运行时环境的部分，也就是对于标准库的实现那部分。今天完成的WASM推理引擎，最重要的改进是将C语言代码中手写的朴素Trie树改成了以内存池形式实现的高效实现，消除了运行时malloc，而这对于提升WASM性能是决定性的：因为观察到malloc在浏览器上性能很差，并且大部分时间都花费在GC上。看了这么久WASM汇编代码，基本上看不懂。尤其是开了最高优化之后的汇编代码，完全看不懂。我也算是一个编译器爱好者，但是我自己的编译器生成的汇编代码我自己也看不懂。另外汇编这个层面到底要侵入宿主环境多深，到底要跨越多少个抽象层次，是个很有趣的问题。我自己搞的虚拟机/指令集是可以感知到λ闭包的，这有利于减少编译器的工作量，但是虚拟机要考虑的就很多了，访问变量的开销非常大。WASM与WASI和emscripten这两层东西是分开的，这很好。很大一部分精力花在了搞清楚这两层之间的边界在哪里。浏览器上的 WASM API 给开发者留了很大的自由度，开发者可以自由操作所谓的线性内存，所以我说绝对的权力意味着绝对的责任。malloc帮我们干了很多脏活累活，但是作为他们一切的主人，我们也不能越俎代庖，专业的事情交给专业的人干。而问题在于，我是那个专业的人吗？

**2024-11-16**：现有的56M大模型，是一只合格的电子鹦鹉。它可以创造性地说出流利通顺的人话，但是并不能保证事实正确。其语言能力大概相当于刚学会说话不久的小朋友。但这并非没有价值。在______场景下，它的确能复读出语料中那些____的话，但同时因为模型规模较小、以及训练不充分的缘故，它似乎不能完全沉浸在____的语境中，往往会说出很出戏的话。所以与其说是____，不如说是搞笑，有种难以言喻的诙谐感🤣。作为极小规模的胡言乱语机器人是合格的。如果训练得当，放在某些大型游戏里，用Lua这类胶水语言直接嵌入进去，作为NPC的电子大脑是完全可行的。现在正在训练168M规模的语言模型，已经训练一个星期了。至于要训练多久，只能说多多益善吧。希望它的智商比起56M模型能有一些提升。

**2024-11-10**：正在给Nano的推理后端适配RKNPU的API。但是RK给的示例代码是不是有问题…主要是算余弦相似度的部分，不对。但某些设置下会出来全0的结果，不知道怎么回事。

**2024-11-07**：评大模型结构收敛到Transformer：确实…我那个Nano里面的DeepSpeed适配基本上可以删掉了。现有主流分布式训练框架，要么是糟糕的抽象，要么是见啥吃啥的貔貅。业界在这方面浪费了大量的资源。模型结构既然收敛到llama，框架也就不必要了。接下来重要的是大模型和后端、硬件、网络的深度适配。

**2024-11-05**：RK3588到手后，把Nano小模型部署上去。不过他家的RKNN好像不开源，不太好搞。只能用官方工具转几个开源模型玩玩。那就用CPU推理也成。在JetsonOrinNX的A78AE上，各种优化统统开启，观察到最高达100token/s的生成速度。后续打算文火慢炼，炼个100M(0.1B)量级的语言模型。甚至可以把CLIP缝合上去，做个VLM，结合微调，解决一些有实际价值的机器视觉类问题。其实现在已经炼成的56M模型也不是一无是处，在____方面有很大的玩头。

**2024-11-03**：Nano的C语言推理验证成功。主要还是抄袭Karpathy大佬的大作，我只是狗尾续貂而已。几十年没有写过C了，非常的恶心非常的头疼……又来手动管理内存了我的电脑爹

**2024-10-17**：有人说我的大模型只是把题库背下来而已，其实没有思考的能力。没错，能把题库背下来就很了不起啦！别看我是个老B灯，实际上B类题库大多数问题，我也是不知其所以然的，所以说我的本质就是个复读机。让大模型真正具备思维能力大概只有一个办法，那就是加钱，加钱，加钱！

**2024-10-13**：租用1块A800(80GB)训练58M参数的语言模型，词元吞吐率大约200000token/s，大约是 AGX Orin 的十倍。因此我只能说我玩过的最离谱的东西就是用每小时x元的价格训练电子鹦鹉。现在想想，社会把很大一部分注意力放在电子鹦鹉上，这真的好吗？

**2024-10-12**：电子🦜的训练进展。50M参数，3.2B数据集，AGX Orin 盒子，预训练跑3天了，还不到两个epoch，已经没有耐心了= = 昨晚改进了词元编码，无非是覆盖了所有的CJK字符和绘文字（没错，我认为绘文字才是真正的世界语，给它分配1800个位置是值得的）。另外，考虑到英文的编码效率，加了几千个英文词根词缀和高频词，凑出32768个词，作为默认词表。构建词元编码器是基础工作，它如果不稳定，就不要贸然开展训练。语料还是要持之以恒地做，但是要有个基线。对于个人爱好者来说，数据算力算法，数据是最难搞的，算力倒还在其次（只需要付出亿点点钞能力）。数据难，还是因为把知识从人脑迁移到电脑这件事很难。

**2024-10-11**：用魔鬼辞典来微调正在炼制的50M语言模型。

**2024-10-09**：用假期前训练的预训练模型，在通用SFT数据集上做了小量的SFT，效果比预期好很多。看来没日没夜的长期连续充分预训练是非常有必要的。因此又准备了1.6B词元的预训练语料，计划再花几天时间，再训练一个基础预训练模型出来。

**2024-10-06**：关于大模型的监督微调究竟应该怎么调，现在依然是众说纷纭。机器学习喜欢谈“泛化能力”，但是我请问呢，从“短波业余波段有7MHz、14MHz、21MHz”能泛化出“18MHz”吗？恐怕是不能的。知之为知之，不知为不知。“泛化”是不存在的。“泛化”这个词就体现出一种人类特有的傲慢和自信，我都学会骑自行车了，开车有啥难的。不信你来刺桐城试试？一切训练的实质都是记忆，在封闭世界假设中，一定要让模型见到过全部的世界，最好还有它的反面。除此之外“泛化”出来的，都只能说是幻觉，而不是事实。如果说其中有符合事实的部分，那也只能说是巧合而已。

**2024-09-30：电子🦜训练随笔**

- **目标任务**：最近趁着假期前夕的宝贵空闲，从0927晚上9点开始，训练一个35M参数量的Llama-like语言模型，目标有二：①假装学会回答业余无线电操作证考试问题；②知道自己是BD4SUR训练的大模型和“人类的本质是复读机”这个事实。最终的目标是：对大模型祛魅。
- **模型结构**：模型的实现是基于Karpathy大佬的nanogpt魔改的，自己实现了数据预处理、数据加载器、简单的词元编码器、训练和微调流程等外围组件，并且对原有的GPT模型结构做了简单的魔改，使其兼容RoPE、RMSNorm两个重要技术（因而是Llama-like而不是GPT-like）。整个实现完全不依赖HF的各种框架，如Transformers、Tokenizers等等，也不打算兼容它们，仅依赖torch。具体而微，是我自己搞这些玩具的一个最重要的方针。模型的上下文窗口是256个词元，参数量35M。词元编码器采用最简单的方法，也就是将Unicode字符映射到一个整数。词典大小19000多，模型的词典大小设为20000整，为特殊词元预留一些空间。为了让模型更清楚地认识它的主人，将BD4SUR设置为特殊词元。
- **硬件和环境**：训练硬件使用 Jetson AGX Orin (64GB)，这个小盒子唯二的优势是显存大、功耗低（最高60W），通宵训练不用太担心消防安全问题。至于计算性能嘛，只能说“又不是不能用”，在BF16精度加自动混合精度加FlashAttn的设置下，粗略估计大概可以跑到23TFLOPS的速度（词元吞吐率没有计算）。从单位显存价格角度来看，AGX(64GB)比起 Mac Studio 其实是更划算的，但算力远逊于 Mac Studio，不过生态上的优势又弥补了这一点。虽说显存多多益善，但训练35M参数的语言模型，并不需要多少显存。大显存可以使用更大批大小，批大小设置为256时，显存占用大概30多GB。
- **预训练**：预训练语料的准备极端重要，其重要性无论如何强调都不为过。垃圾进垃圾出的铁律，会惩罚每一个不做数据工程的人。但是我并没有那么多的时间精力，只草草找了些百科、医学、xxqg、精神分析、无线电和经济类语料，利用GPT-4o等商用模型做了简单的、局部的清洗，也生成了一些语料，最终得到大约400M词元的预训练语料。按照所谓的规模扩展定律（Scaling law），预训练词元数至少要是模型参数量的20倍以上。虽然没有这么夸张的倍数，但是这次准备的语料，对于模型参数量来说，也不少了。模型的检查点文件（pickle）大概400MB左右，为了方便使用，将词元编码器（的配置文件，也就是词典）也包括在里面了。预训练持续到0929晚上，迭代了大概100000步，经历8到9个epoch，损失下降到2.1左右。数据的调度（或者叫“数据课程”）不太合理。6类语料是按顺序预训练的，这导致模型的损失曲线呈现出非常明显的分段，损失值低者达到2以下，高者依旧3点多，不过整体趋势是下降的。不知道这样做的负面影响是什么。为了弥补这一点，在第9个epoch将所有语料混合在一起，最终达到2.1的交叉熵损失。预训练阶段，还有一个很重要的技术细节，那就是分块的因果自注意力。预训练语料的分块（chunking）是个特别讲究的环节，如果不相关的句子出现在同一个上下文窗口中，需要用特殊词元将他们隔离开，并且训练阶段最好不要让他们互相看到，因此需要在注意力掩模上动手脚。或者直接在数据预处理阶段将不相关文本分配到不同的窗口中，代价是会浪费很多填充词元的长度，降低训练过程的有效吞吐率。但是我在数据处理阶段并没有很重视数据块的切分，甚至连bos/eos这样的分隔符都没加，所以这个是后续改进的一个点。
- **监督微调和偏好优化**：为了尽快看到效果，从0930凌晨开始，开始用业余无线电操作证考试QA数据集，对预训练模型作监督微调。为了提升SFT的效果，避免微调阶段过分过拟合，用商用大模型扩充了官方的操作证考试题库，每个QA扩充了3条不同的问法，试图激发出模型更一般的语义理解能力。最终得到大约5000条QA对，其中包括“我是BD4SUR训练的大模型”和“人类的本质是复读机”。今天（9月30日）早上8点左右看，损失下降到0.03，实际效果尚未验证。像这种损失值，恐怕早已发生了灾难性遗忘，模型原有的语言知识可能已经基本上被破坏殆尽，但是换来了在操作证考试QA上的精确（然而也是呆板的过）拟合。另外，SFT数据集中，加入大量“我是BD4SUR训练的实验性大模型”和“人类的本质是复读机”数据样例，试图为其注入自我认知。在这种场景下，RLHF的价值就体现出来了。一方面，BD4SUR作为特殊词元，特别容易跟填充词元、指令标记词元等特殊词元混淆，这可能是因为训练不充分，也有可能是SFT中缺乏惩罚机制（当我冒出惩罚的想法时，就意味着该上RL了）。另一方面，模型还不能很有效地学习到指令遵循能力，这会导致问答（chat）场景下，回答部分会出现特殊词元，也就是没有完全遵循指令模板的格式。无论如何，今晚回去看看，经过几百轮SFT的模型，在无线电领域上回答问题的效果如何。
- **推理及其优化**：推理优化，实际上是一个极其有价值且困难的课题，但这暂时不在我现阶段的考虑范围内。一个最基础的优化就是KV-Cache，以空间换时间，基本的实现并不复杂，但是它的优化非常高深，vLLM在这方面做了很多工作。
- **开源的考虑**：至于要不要发布在HF上，我看，还是不要了。拍个视频，留作证明就好了，不必什么东西都开放出去。最重要的考虑，当然是安全性问题。训练语料里面有大量的xxqg内容，所以模型会生成什么东西，我是完全无法控制的。另外，虽然这个模型并没有什么实用价值，但从成本的角度来看，应该说还是相当昂贵的。人力成本（不是什么阿猫阿狗都能搞定这个过程的，请给我技术咨询费！）、时间成本（它在炼丹的时候我在看杨旭游记）、能源成本（连着开了三天三夜的空调，并且是21度！冻死我了）、以及炼丹炉本身的成本（价格不菲的炼丹炉高负载连续运行三天三夜的折旧费估计也不便宜吧），都不可忽视。所以说，自己玩的东西不见得对他人有价值，但可能是相当昂贵的，只为博君一笑，拍个视频证明一下，天空没有留下痕迹，但我已经飞过，这就够了。

**2024-09-24**：词元编码（tokenizer）是传统NLP在大模型时代的最后堡垒之一。但是我觉得相比于其他模态的信源编码算法，自然语言文本的codec，诸如BPE之类的，并没有太多人类的智慧在里面。或者说，人类还是不要自作聪明地去做什么tokenize，直接交给模型去学就好了。因此，不想在词元编码上花费太多精力，直接使用一个Unicode字符一个词元的香草味方法。不过，对于英文这样的字母语文来说，BPE这样的词元编码还是有必要的。但我觉得这主要是经济性的考虑，里面是没有智慧的。当然工程实现层面很有智慧。

**2024-09-21**：我最近在尝试在nanogpt上进行预训练和监督微调，试图让小规模的GPT-like模型能够回答业余无线电操作技术能力验证的问题。不求正确，只求“说人话”。但是我总觉得现在的大模型与人脑的运作机制大不相同。具体来说就是LLM是“刚性”的、开环的，它不具备在推理时反馈重塑自身的能力。像语境学习（In-context learning）这样的能力，无非是搜索空间巨大，以至于给人一种举一反三的错觉而已，实际上模型并没有真的泛化出什么内在规律，也没有根据输入信息对自己进行“实时的重新训练”。像预训练、监督微调、人类反馈强化学习一类的手段，似乎都没有做到这一点。RLHF虽然体现了“实时训练”“训推闭环”这样的思想，但还是觉得不够到位。在自省和自我重塑这个视角看来，LLM作为一个庞大的开环控制系统，它甚至不如某些编程语言及其实现更有“智慧”。而人脑是一坨肉，它的结构是柔性的，神经元是会动的，在宏观的稳定中，存在着微观的噪声和易变。虽然诸如NAS（神经网络结构搜索）一类的技术试图将网络结构作为一种超参数甚至网络参数纳入优化的范围，但是如何在推理时实现这一点，可能还有很大的想象空间。归根结底，还是那种熟读唐诗三百首式的智能。LLM掌握了大量的“司机知识”，给人一种充满智慧的表象。至于 OpenAI o1 这类最新的工作，还需要进一步了解。最后还是要关注那个问题：**从表象到逻辑的模式之间，究竟是如何演化的。伽罗瓦式的智慧和拉马努金式的智慧，究竟有什么样的区别和联系。**

**2024-03-14**：72B量级的LLM所拥有的强大的指令跟随能力让她在某个十分逆天的提示语的激励下获得了极其逆天的瑟琴能力，这在2022年以前是绝对无法想象的。出于公序良俗的考虑，我不能提供进一步的情报了，请兄弟们自由探索吧，桀桀桀~

**2023-12-28**：众所周知，变压器类大模型有3种流派，分别是BERT（编码器解码器）、GPT（仅解码器）和T5（仅编码器）。历史表明，GPT类架构在大模型流派的竞争中暂时胜出，生成任务可以囊括各类语义理解任务。随着GPT的优势逐渐显现，对于OpenAI当初为何决策采用decoder-only架构，业界展开了马后炮式的研究。2021年4月，我发表了一篇长篇暴论，其中提到了变换矩阵的秩与变换的描述能力的问题。此时我还不了解注意力机制的原理。然而，在此一个月前，2021年3月，有人发表了一篇论文，标题大概是 Attention Is Not All U Need ...，其中一个主要观点是注意力矩阵的秩限制了注意力机制的表达能力。而我们在技术路线选择上，也是凭借大就是好的直觉，选择了BERT。在苏剑林的博客文章评论中，有人提到，双向注意力实际上是给注意力矩阵增加了更多的约束，导致注意力的有效自由度反而降低。

**2023-04-25**：我理解的机器学习算法有三个要素：模型、失真测度和优化方法。大家都很关心前两个，因为注入了人的价值观；而第三个需要神谕般的智慧。力学的核心原理是最小作用量原理，大自然知道如何最小化，但是人类很难知道。至于SAT是所谓NP完备问题，此问题可帮助人类领悟神谕。

**2023-03-27**：人类的感官就是一种有显著非线性特性和严重失真的糟糕的传感器，早期音视频编码大部分复杂度就在处理此类问题。而人脑早已学习出基本的去噪/校正算法，剩下的就要每个人自己努力去克服偏见和错觉了。

**2023-03-22**：感知智能发展，借用教育学术语，总结为三个范式或三个阶段：①职业教育：人工设计特征和领域专用小模型，解决领域问题，例如语音特征+HMM解决语音识别问题。②博雅教育：预训练+微调，训练一个高中生，再给他赋予专业能力。③主体教育：RLHF，强化学习，在互动中学习，培养智能体的主体间性。

**2023-03-19**：好多coser面对AI已经表现出显著的恐慌和愤怒了。其实我觉得如果真正喜欢cos本身的话，应该是心如止水岿然不动的才对。当然，很多coser靠这个赚钱，我理解。就像我玩无线电，其实是纯消费纯消耗，我不会因为它是个不时髦的小众东西就不喜欢它，也不会因为没有投资收益而远离它。我只是单纯喜欢这东西，与任何外人外物都无关。所以我预期我最近辛辛苦苦做的视频应该也不会有人看，这从立项一开始就完全在我的预料之内。我不会因为没人看就不做了，我更多的是给自己做，给自己看。最重要的是，**不能因为自己如何如何而产生某种优越感**，否则就不是就事论事了。另外我觉得我们人类的知识产权制度确实是越来越落后了…我们人类今天引以为傲的很多东西，过几年可能都是要变成非遗被保护起来的。我的暴论就是，德赛两先生，可能是第一个进非遗博物馆的。这是人类思想史的里程碑，但也只是里程碑而已。

**2023-03-15**：RNN和注意力之间的区别，很难不让人想到数字逻辑电路里面有关加法器的两种实现方式，一种是行波进位，另一种是超前进位。


### LLM性能测试备忘录

机器配置如下：

```
OS: Ubuntu 20.04.6 LTS x86_64
Host: NF5568M4
Kernel: 5.15.0-100-generic
CPU: Intel Xeon E5-2686 v4 (72) @ 3.000GHz
GPU: NVIDIA Tesla P40
GPU: NVIDIA Tesla P40
GPU: NVIDIA Tesla P40
GPU: NVIDIA Tesla P40
Memory: 128767MiB
```

Llama.cpp测试，测试输入“频谱仪的分辨率带宽和扫描速度之间是什么关系？”，无系统提示。

- `./llama.cpp/main -m Qwen15-72B-Chat-q2_k.gguf   -n 512 --color -i --chatml --numa distribute -t 36 --mlock -ngl 81`：5.55 tokens/s
- `./llama.cpp/main -m Qwen15-72B-Chat-q4_k_m.gguf -n 512 --color -i --chatml --numa distribute -t 36 --mlock -ngl 81`：4.79 tokens/s

```
from llama_cpp import Llama

MODEL_PATH = "/home/bd4sur/ai/_model/DeepSeek/DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf"

SYSTEM_PROMPT = ""

llm = Llama(
    model_path=MODEL_PATH,
    seed=1337,
    n_ctx=16384,
    n_gpu_layers=-1,
    verbose=False
)

history = []

def typewriter(delta):
    print(delta, end="", flush=True)

def predict(message, callback):
    messages = []
    messages.append({"role": "system", "content": SYSTEM_PROMPT})
    for user_message, assistant_message in history:
        messages.append({"role": "user", "content": user_message})
        messages.append({"role": "assistant", "content": assistant_message})

    messages.append({"role": "user", "content": message})

    response = llm.create_chat_completion(
        messages=messages,
        temperature=0.9,
        top_p=0.95,
        top_k=20,
        repeat_penalty=1.11,
        stream=True
    )

    text = ""
    for chunk in response:
        choices = chunk["choices"]
        first_choice = choices[0]
        delta = first_choice["delta"]
        if "content" in delta:
            content = delta["content"]
            text += content
            if callback is not None:
                callback(content)
    history.append([message, text])


if __name__ == "__main__":
    print(f"使用模型：{MODEL_PATH}")
    while True:
        try:
            prompt = input("User > ")
        except EOFError:
            break

        if not prompt:
            continue
        if prompt == "stop":
            break
        if prompt == "restart":
            history = []
            continue
        print(" Bot > ", end="")
        predict(prompt, typewriter)
        print("\n")
```

### 在安捷伦N9020A频谱仪上部署Qwen

视频：[在2007年的频谱仪上部署AI大模型](https://www.bilibili.com/video/BV1du4m1P7iU)

### 在松下SV8便携电脑上部署ChatGLM

<details>

<summary>2023-11-28 chatglm.cpp</summary>

以下完全基于[chatglm.cpp](https://github.com/li-plus/chatglm.cpp)（V0.3.0）部署。在松下SV8洋垃圾笔记本（Core i5-8365U，16GB内存，512GB的NVMe固态硬盘，Ubuntu 20.04 LTS）上测试，体验良好，运行速度尚在能够容忍的范围内，作为人工智障小玩具是堪用的。以下是部署过程备忘，连同环境在内的所有文件已备份到物理硬盘上。

首先，需要将全精度模型转换为低精度的GGML格式的模型。模型转换和量化过程对算力要求不大，但是对于内存需求巨大。6-7B模型需要至少16GB内存，13B模型需要至少64GB内存。由于SV8物理内存只有16GB，因此需要临时扩充交换空间，至少需要50GB的交换空间。临时交换空间的设置方式如下：

```
sudo fallocate -l 50G /swap
sudo sudo chmod 600 /swap
sudo mkswap /swap
sudo swapon /swap
# 查看交换空间
sudo swapon --show
# 查看所有内存
free -h
```

安装各种依赖。

```
python3 -m pip install -U pip
python3 -m pip install torch tabulate tqdm transformers==4.33.0 accelerate sentencepiece
```

拉取chatglm.cpp代码仓库和子模块：

```
cd ~/ai
git clone --recursive https://github.com/li-plus/chatglm.cpp.git && cd chatglm.cpp
git submodule update --init --recursive
```

去HuggingFace或者ModelScope下载模型权重文件。由于涉及巨大模型文件，需要先安装git-lfs：`sudo apt install git-lfs`。然后拉取模型仓库：

- `git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git`
- `git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b-32k.git`
- `git clone https://www.modelscope.cn/ZhipuAI/codegeex2-6b.git`
- `git clone https://www.modelscope.cn/baichuan-inc/Baichuan2-7B-Chat.git`
- `git clone https://www.modelscope.cn/baichuan-inc/Baichuan2-13B-Chat.git`

将模型转换为GGML格式并量化。其中`ModelRepoDir`是模型仓库的目录，量化类型建议取`q4_0`节约内存或者`q8_0`效果和速度折中。

```
cd ~/ai
python3 chatglm.cpp/chatglm_cpp/convert.py -i ModelRepoDir -t q8_0 -o ggml/xxx-ggml.bin
```

编译并安装chatglm.cpp。编译选项可以开启OpenBLAS：`-DGGML_OPENBLAS=ON`，但是实测发现性能似乎并未有提升，因此暂且不开启。

```
cmake -B build
cmake --build build -j4 --config Release
```

此时可以用编译好的可执行文件测试LLM推理：

```
chatglm.cpp/build/bin/main -m ggml/xxx-ggml.bin -p "执行JavaScript代码：Math.sqrt(2)"
# 交互式
chatglm.cpp/build/bin/main -m ggml/xxx-ggml.bin -i
```

还可以安装Python的接口库`pip install -U chatglm-cpp`，使用以下的简单代码进行测试：

```
#encoding=utf-8
from typing import List
import chatglm_cpp

SYSTEM_PROMPT = ""

MODE = "chat" # "chat" or "generate" for code generation
MODEL_INDEX = 1
MODEL = [
    "ggml/chatglm3-6b-32k-chat-i8-ggml.bin",
    "ggml/chatglm3-6b-chat-i8-ggml.bin",
    "ggml/baichuan2-13b-chat-i8-ggml.bin",
    "ggml/codegeex2-6b-i8-ggml.bin",
]

MAX_LENGTH         = 32000 # max total length including prompt and output
MAX_NEW_TOKENS     = -1 # max number of tokens to generate, ignoring the number of prompt tokens
TEMPERATURE        = 0.9
TOP_K              = 0
TOP_P              = 0.7
REPETITION_PENALTY = 1.0 # penalize repeat sequence of tokens
THREADS            = 8 # number of threads for inference

def main() -> None:

    print("""Input "restart" to restart conversation
      "stop" to quit\n""")

    generation_kwargs = dict(
        max_length = MAX_LENGTH,
        # max_new_tokens = MAX_NEW_TOKENS,
        # max_context_length = 512,
        do_sample = (TEMPERATURE > 0),
        top_k = TOP_K,
        top_p = TOP_P,
        temperature = TEMPERATURE,
        repetition_penalty = REPETITION_PENALTY,
        stream = True,
        num_threads = THREADS,
    )

    pipeline = chatglm_cpp.Pipeline(MODEL[MODEL_INDEX])

    system_messages: List[chatglm_cpp.ChatMessage] = [] # 注意：百川2-13B不要加入这两行
    system_messages.append(chatglm_cpp.ChatMessage(role="system", content=SYSTEM_PROMPT))

    messages = system_messages.copy()

    while True:
        try:
            prompt = input("User:\n")
        except EOFError:
            break

        if not prompt:
            continue
        if prompt == "stop":
            break
        if prompt == "restart":
            messages = system_messages.copy()
            continue

        if MODE == "generate":
            for chunk in pipeline.generate(prompt, **generation_kwargs):
                print(chunk, sep="", end="", flush=True)
        elif MODE == "chat":
            messages.append(chatglm_cpp.ChatMessage(role="user", content=prompt))
            print(f"{pipeline.model.config.model_type_name}:", sep="", end="")
            chunks = []
            for chunk in pipeline.chat(messages, **generation_kwargs):
                print(chunk.content, sep="", end="", flush=True)
                chunks.append(chunk)
            print()
            messages.append(pipeline.merge_streaming_messages(chunks))

    print("Bye~")


if __name__ == "__main__":
    main()

```

</details>

## 视觉和图文跨模态理解

### Qwen-VL

### 应用案例：表情包搜索引擎

视频演示：[自制表情包搜索引擎演示](https://www.bilibili.com/video/BV1vJ4m1e7MN)

2024-02-25：正在用Qwen-VL给六百多个表情包梗图做captioning。速度太慢了太慢了，按照这个速度，估计要做到明天去。并且视觉语言模型在meme理解上还是有很大的缺陷，最可笑的是，可能是由于价值观对齐的缘故，诸如“装逼”这样的词是无法出现的，会识别成“装通”。我们仍未知道视觉语言模型是如何实现OCR的。看来必须挂一个传统的OCR模型了，用来忠实提取画面上的文字，保证召回率。另外，提示语的设计仍然是个开放问题。

## 图像和视频合成（文生图等）

### Stable Diffusion

## 语音识别和理解

### Qwen-Audio

### FunASR

<details>

<summary>在低性能CPU机器上部署FunASR</summary>

2023年，阿里巴巴达摩院开源了[FunASR](https://github.com/alibaba-damo-academy/FunASR)语音识别工具包，可以在低资源CPU计算机上运行。以下参照实时语音转写的[官方安装指导](https://github.com/alibaba-damo-academy/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md)，整理出一份安装部署检查单。

**首先安装docker**

```
curl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.sh
sudo bash install_docker.sh
```

**拉取镜像并保存为本地tar包**

```
sudo docker pull registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.5
sudo docker image save -o ~/ai/funsar/funsar.tar registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.5
```

**启动容器和实时语音转写服务**

```
sudo docker run -p 10096:10095 -it \
  --rm \
  --privileged=true \
  --name funasr \
  --volume ~/ai/funasr/models:/workspace/models \
  --workdir /workspace/FunASR/runtime \
  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.5 \
    bash run_server_2pass.sh \
      --download-model-dir /workspace/models \
      --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \
      --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx  \
      --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  \
      --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx \
      --itn-dir thuduj12/fst_itn_zh \
      --hotword /workspace/models/hotwords.txt
```

以上命令中的镜像ID也可以替换为`docker image load -i ~/ai/funasr/funasr.tar`之后得到的镜像ID。

**启动客户端**

首先下载[客户端](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/sample/funasr_samples.tar.gz)。在启动Python客户端之前，需要针对音频做一些设置。首先安装工具和库：

```
sudo apt install pavucontrol portaudio19-dev
pip install websockets pyaudio
```

然后启动Python客户端：

```
python3 ~/ai/funasr/client/python/funasr_wss_client.py --host "127.0.0.1" --port 10096 --mode 2pass
```

</details>

## 语音/音乐合成和音色转换

### RVC

