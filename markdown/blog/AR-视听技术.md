
#!title:    视听技术
#!date:     2023-02-23

#!content

> 说明：本文《视听技术》建立于2023-02-23，由2019-08-04起草的《视频编码》和2019-11-03起草的《音频编码》两篇文章合并。

![城市天际线（其实是Aqua在64kbps(CBR)下压缩某一首歌得到的声谱图。由于还没有实现心理声学模型，编码器随缘量化，中高频部分不仅损失，而且引入了很强的可闻量化噪声，能量分散在很宽的频段上，表现在声谱图上就是这个样子了。2023-08-30截图](./image/G3/audio-encoding/aqua-noisy-spectrum.jpg)

# 视听技术体系

- 采集·感知：成像和摄影 / HVS / 光学 / 光机电系统
- 处理·通信：图像处理 / [[#ff0000:**高效编解码**#]] / 视频通信广播 / 存储 / 版权
- 建模·认知：计算机图形学 / AR/VR / 计算机视觉 / 语义化和信息检索
- 呈现·交互：显示和打印 / 图形计算 / Web前端/GUI / 人机交互 / 字体排印

# 成像技术

CMOS尺寸：**所谓“1英寸CMOS”的对角线长度为16mm，而不是25.4mm。**这大概要追溯到二十世纪五、六十年代电子成像技术刚开始的时代吧。那时早期的电视摄像机使用的感光元件是真空管。真空管的外面是有个玻璃罩子的，真空管外径是把玻璃厚度也算进去的，玻璃管当然是不能用于成像的，外径1英寸的真空管，实际成像区域只有16mm左右，于是16mm就成了电子摄像照相行业一个约定俗成的度量单位。这种度量方式一直继承了下来，我们现在所接触到的CCD尺寸的说法是参考传统摄像机内的真空摄像管的对角线长短来衡量的，它严格遵守了 Optical Format 规范，其数值称为OF值，单位为英寸。（整理于2020.5.2，[来源](https://blog.csdn.net/u010653400/article/details/78050494/)，[参考](https://en.wikipedia.org/wiki/Image_sensor_format)）

# 显示技术

**显像管电视的高频声音**

- 国内的电视机遵循PAL-D制式标准。
- PAL制式的视频分辨率是720×576，每秒25帧。由于隔行扫描的原因，每一帧画面分为两场进行扫描，每场耗费312.5行扫描时间，正程287.5行扫描时间，逆程25行扫描时间，所以扫描575行需要625行扫描时间，场频正好是两倍帧率。所以对于PAL制式CRT电视来说，行频=312.5×2×25=15.625kHz。
- 由于线圈等元件的工艺问题，在这个频率下免不了发生一些机械振动，听到的便是这个频率的声音。
- 电脑CRT显示器也会有类似的噪音，但显示器分辨率和刷新率较高，比如17寸CRT普遍的1024×768@85Hz，行频已经超过了65kHz，人耳是不可能听到的。（[来源](https://www.zhihu.com/question/55612873/answer/151940491)）

# 计算机图形学

## 水波纹特殊效果

<iframe class="MikumarkIframe" src="./html/ripple.html" height="350px"></iframe>

水波纹特效是比较常见的一类特效。前段时间突然想知道这个特效是怎样实现的，便有了这篇文章。

最开始看到这个问题的时候，第一想法是使用正弦波的传播去模拟水波的传播，同时加上一定的衰减（指数衰减）去模拟真实的物理过程。然而运行的时候就傻眼了——动画极其卡顿，完全没有流畅的感觉。这显然是因为正弦、指数、乃至乘法操作耗费了大量的时间，如果是做实时渲染的特效，断然不可以使用这些复杂的函数。

这次失败的尝试，充分暴露出我毫无图形学素养的事实。因此，我开始搜索现成的解决方案[[1]](#参考资料)。

**建模**

水波是日常生活里司空见惯的物理现象，但实际上是非常复杂的。像我最开始那样，简单使用衰减的正弦波去模拟水波，尽管效果上比较类似，但那并不是物理事实。

从运动的效果入手去模拟物理现象，我认为这是一种运动学的方法。这只是在结果的层次上进行模拟，并不具有通用性，甚至在有些情况下很可能是错误的。

因此，合理的方法应该是对需要模拟的物理现象进行动力学的建模，从“原因”的层面上去模拟。由于只是模拟视觉效果，因此可以忽略许多物理细节，在保证效果的前提下，减少实现上的复杂性。

水波效果可以分成两个相继的问题去研究：一是对水波本身进行模拟，二是对水波折射水底景物的效果进行模拟。

**水波的动力学**

首先需要明确的一个常识是：波是能量的传播，而不是水中质点的传播（实际上水波的质点会沿着波的方向运动）。水中振动着的质点，由于液体内部的作用力，会带动其周围的质点一起运动，导致振动的能量以波源为中心向外传播。从宏观角度看，就体现为一圈水波。

如果把宽度为$W$的Canvas画布上的每个像素看成是振动着的质点，那么可以用$P_i$来表示从左上角开始的第$i$个质点，其对应的振幅记为$A_i$。$P_i$上下左右四个临点的振幅记为$A_{i1}$、$A_{i2}$、$A_{i3}$和$A_{i4}$，其中$i1 = i - W$、$i2 = i - 1$、$i3 = i + 1$、$i4 = i + W$。

假定①：经过一个时间单位后，$P_i$的振幅可以由以下递推方程所确定（其中$a$和$b$是待定系数）：

$$A_i' = a(A_{i1} + A_{i2} + A_{i3} + A_{i4}) + bA_i$$

假定②：不考虑阻尼的情况下，水波传播过程满足振幅守恒：

$$\sum{A_i'} = \sum{A_i}$$

需要注意的是，这并非能量守恒，而是为了计算方便而做的合理简化。实际上，按照以上两个假设，计算出的水波振幅实际上是不收敛的，即振幅虽然会上下振动，但绝对值会越来越大，直至无穷。

（待续）

**性能优化**

- 化乘除为移位。
- 避免频繁读写全局变量。
- 使用WebGL。

**参考资料**

+ [Canvas实现水波纹效果](http://uusama.com/643.html) | [GitHub](https://github.com/youyouzh/water_ripple)，本文主要借鉴了这篇文章，文章开头的演示沿用此文作者的代码。

# 光盘技术

![光盘光斑尺寸对比](./image/G3/光盘光斑尺寸对比-Wiki.png)

![光盘激光头光路](./image/G3/激光头光路图.png)

2015年VCD改造：详见`image/G3/vcd-hacking`目录下图片。

# 音乐盒（2018-05-20）

<iframe class="MikumarkIframe" src="./html/jukebox.html" height="300px"></iframe>

# Project Vita

## 公共数据类型定义

图像矩阵`Imat`：用于表示图像单个通道的像素矩阵。其结构为**由每一行构成的数组**，如果以左上角为坐标原点(0,0)，那么访问位于(x,y)的像素的方法是`Imat[y][x]`。矩阵的每个元素为`number`类型，一般为[0,255]的整数。

```
Imat: [
    [第0列, 第1列, ... , 第(width-1)列], // 第0行
    [第0列, 第1列, ... , 第(width-1)列], // 第1行
    ......
    [第0列, 第1列, ... , 第(width-1)列], // 第(height-1)行
]
```

## 静止图像Codec

**▶ [可视化演示](https://bd4sur.com/html/jpeg.html)**

定义在`jpeg.js`中。可视化页面`jpeg.html`。

编解码器由一个模块`JPEG_Codec`类构成，使用ES5编写，Node和浏览器皆可使用，但是浏览器使用之前要注释掉`require`相关的代码。

仅实现了变换编码和游程编码，未实现熵编码（哈夫曼编码）和码流封装部分，因此并非完整的JPEG编解码器。

2019年7月7日首次实现；2020年6月重构并纳入本仓库。

依赖：

- [fourier.js](https://github.com/bd4sur/Fourier)：提供离散余弦变换的实现
- image.js：用于可视化，提供RGB与YUV之间转换的函数
- canvas.js：用于可视化，提供绘图函数

使用方法：

```javascript
// 建立Codec实例
let jcodec = new JPEG_Codec(); // 无参数

// 编码
let streams = jcodec.encode(Y, U, V, width, height, quality);

// 解码
let yuv420 = jcodec.decode(stream_Y, stream_U, stream_V, quality);
```

接口定义：

`JPEG_Codec.prototype.encode`：编码函数。此函数将**YUV420**格式的三个图像矩阵编码为三个码流。参数如下：

- `matrix_Y: Imat`、`matrix_U: Imat`、`matrix_V: Imat`：YUV三个通道。输入的YUV矩阵**必须是YUV420格式的**，即U、V矩阵的长宽都是Y的一半。矩阵的元素为8位无符号整数，即[0,255]区间内的整数。
- `width: number`、`height: number`：输入图像的宽度和高度。这两个参数未必与输入矩阵的尺寸完全符合，如果小于实际尺寸，则只会处理一部分；如果大于实际尺寸，则范围外的部分可能是`NaN`。Codec不会检查这两个参数是否符合实际图像的尺寸。
- `quality: number`：图像质量参数，值域为(0,100]。该参数用于控制压缩程度。值越大，画质越高，压缩程度越低。值为50时相当于采用标准推荐的量化表。

`JPEG_Codec.prototype.decode`：字节码流解码函数。此函数将`encode`函数生成的字节码流解码为YUV420格式的三个图像矩阵。参数如下：

- `bytestream_Y: Array<number>`、`bytestream_U: Array<number>`、`bytestream_V: Array<number>`：三个通道的码流。
- `quality: number`：图像质量参数，需要与编码时使用的参数一致。

待办：

- 熵编码接口（待后续按需要实现）。
- 边缘补零会导致严重的振铃效应。
- 成员函数静态化（`JPEG_Codec`类仅仅起到名称空间的作用），并且尽可能去除语言相关的部分，以利于C/C++移植。

参考资料：

- [ITU T.81](https://www.w3.org/Graphics/JPEG/itu-t81.pdf)

## 抖动

**▶ [可视化演示](https://bd4sur.com/html/jpeg.html)**

定义在`dither.js`中。可视化页面`jpeg.html`。

实现了 [Floyd-Steinberg 抖动算法](https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering)。

接口格式：`dither(matrix, width, height, quant_function)`

- `matrix: Imat`：单通道图像矩阵。
- `width: number`和`height: number`：图像的宽度和高度。
- `quant_function: number→number`：量化函数，默认为简单二值化函数。

## 数字盲水印

**▶ [可视化演示](https://bd4sur.com/html/watermark.html)**

定义在`watermark.js`中。可视化页面`watermark.html`。

算法简述：

- 对图像（RGB或者YUV）作DCT变换，得到变换域图像。
- 在变换域图像上叠加水印，水印强度可以根据应用场景调整。
- 对叠加水印后的变换域图像作IDCT，即得添加水印的图像。
- 再次进行DCT，就可以看到叠加在变换域上的水印图案。

该算法非常朴素，健壮性很差，有待更进一步的改进。

2019年7月30日首次实现；2020年6月重构并纳入本仓库。

## 运动估计

**▶ [可视化演示](https://bd4sur.com/html/inter.html)**

定义在`inter.js`中。可视化页面`inter.html`。

采用三步搜索法作运动估计。

2019年8月7日首次实现；2020年6月重构并纳入本仓库。

依赖：

- `math.js`

## Otsu二值化

**▶ [可视化演示](https://bd4sur.com/html/harris.html)**

定义在`otsu.js`中。可视化页面`harris.html`。

2019年8月10日首次实现；2020年6月重构并纳入本仓库。

依赖：

- `math.js`


## Harris特征点

**▶ [可视化演示](https://bd4sur.com/html/harris.html)**

定义在`harris.js`中。可视化页面`harris.html`。

2019年8月7日首次实现；2020年6月重构并纳入本仓库。

依赖：

- `math.js`

# 视频编码（2019-08-04）

## 一 绪言

**1-1 研究动机**

- **时代背景** 毫无疑问，视觉是人类最重要的知觉。2020年的新冠疫情，网络摄像头价格飞涨，远程教育大发横财，远程办公迅速普及，让人们意识到音视频通信或许是下一个刚需。至于之前一直不瘟不火的AR/VR、4K~8K超高清、全景直播，乘着5G的东风也开始有了复兴的迹象。虽然影院KTV尚未复工复业，但红米的98寸大电视、B站大会员的4K视频也让人直呼真香。大片永远嫌不够多，大电视永远嫌不够大。底大一级压死人，如今依然是至理名言。
- **技术难题** 视频号称“最大的大数据”，其巨大的数据量是一切技术难题的万恶之源。因此，**高效**视频编解码，在整个视觉技术体系中作为底层基础，地位相当重要，是学界、业界重点关照的领域。视频编码和视频理解被业内公认为最有挑战性的两个领域。在这两方面，无论是技术研究还是产业落地，我国做得都还不错，至少不算落后。
- **技术自主** 视频编码地位重要、技术难度大、专利和标准壁垒高、先发优势效应明显，堪称视觉技术领域的光刻机，是**要不来、买不来、讨不来、也很难通过市场调节自发创造出来**的核心卡脖子技术。好在二十年前的DVD专利案给了国人当头一棒，如今才有了渐入佳境的AVS标准。还是那句话，我可以不用，但你不能没有。对于一个有抱负的国家来说，核心技术这块儿必须做到自主可控，没有任何捷径和退路。
- **个人兴趣** 个人知识管理需要维护大量的动画、电影等视频文件，涉及压制、转码、后处理、甚至检索等工作，有必要了解视频编解码的原理。另一方面一直对音视频技术感兴趣，因此愿意深入钻研下去。

**1-2 研究目标**

- 总的来说是抱着玩票的态度去研究。当然，以后靠这个吃饭也是说不定的事情。无论如何，满足自己的兴趣是首要目的。
- 输出一些有价值的材料，包括学习笔记、图表、文献综述等等，一方面是将自己学到的东西沉淀下来，另一方面有助于后来者入门。
- 写一些代码，知行合一。
- 掌握ffmpeg的用法。

**1-3 研究方法**

- **文献调研** 文献调研是研究工作的起点和重点，其目的是梳理本领域的概念体系、发展历程、经典文献、方家巨擘，以便后续可以纲举目张地进行深入研究。文献调研总的来说是个自举的过程，触发自举的源头有维基百科词条、知乎大佬文章、高校课程讲义、以及随缘看到的公号文章、业界资讯、博客笔记等等。从这些源头出发，通过引用关系递归地爬取文献资料，初步建立起能够反映本领域全貌的文献集合。当然，这个过程需要有选择地爬取价值较高、有代表性的材料，时刻注意去粗取精，不能无脑爬取而徒增负担。还需要注意的是，材料之间既要相互覆盖，又要尽可能从不同的角度去论述问题。不同材料之间相互对照，有助于去伪存真、获得相对可靠的知识。另外，为了保证参考资料随时可考，所有材料都必须保留自有备份。
- **输出驱动** 以输出倒逼输入，在创作中发现问题。
- **多提问题** 一个好的问题，胜过一百个现成的知识。
- **知行合一** 多写代码。需要遵守“代码即文档”的选择，保证可读性第一。

**1-4 结构和内容** 本文从视频编码的基本原理出发，展开介绍主流标准所使用的三大核心工具——变换、预测和熵编码，以及率失真优化等关键技术。随后分析主流视频编码标准，包括MPEG系、ITU系和AVS系，按照时间顺序分成三代，重点放在横向纵向比较，勾勒出视频编码标准的进化脉络。最后回归实践，介绍FFmpeg的使用方法。

------

**2020-06-12**

从0101二进制串→位图的过程，各位已经讲得很清楚了。我主要补充一点有关图像/视频压缩的东西。

如果没有视频压缩，那么很容易计算出，一个时长为1分钟、帧率25fps、分辨率为1920×1080逐行、每个像素RGB共24bit的简单视频，它的尺寸大约是9.3GB。然而实际见到的这类视频一般只有几MB～几十MB左右，这意味着编码器可以将原始视频以成百上千倍的效率对视频进行压缩，大大节约硬盘容量和带宽。以下是视频压缩的基本原理。

首先要知道，由于视网膜上感受亮度的杆细胞远多于感受色彩的锥细胞，因此人眼对亮度的感觉比色度更敏感。根据色彩理论，RGB并非表示颜色的唯一方式，还有一种方式称为YUV。出于以下的优势，图像/视频编码一般使用YUV表示。

Y指亮度分量，可以理解成黑白电视信号。而UV指两个色度分量。因此用YUV方式传输视频信号，可以非常方便地实现黑白电视与彩色电视的兼容。

更重要的是，人眼对Y比UV更敏感，因此可以在不损失画质的前提下，适当减少UV成分。普通的RGB图像中，RGB是同样多的，但是对于YUV来说，U和V的数量可以减半（指宽度或高度方向上），称为YUV422排列或者YUV420等等。

除了人眼生理特点决定的色彩冗余之外，视频内容本身也存在大量冗余。包括时空冗余、频域冗余、编码冗余等等。以下开始逐个消除这些冗余。

静止图像有大量的空间冗余。对于类似于框图、图书扫描的那种图像，它的背景有大片的空白，这部分空白内容就可以通过各种手段压缩掉。例如游程编码、例如帧内预测等等。

视频在时间上同样有更大的冗余。例如监控探头拍摄的画面，如果画面中没有物体运动，那么连续多帧画面之间的差异是很小的。此时，只需要编码传输第一帧（关键帧）和后面各帧与它的差异就可以了。如果画面中有物体运动，也是同样的思路，只需要编码传输画面中运动部分的位移信息即可。通过这种手段，可以消除掉绝大多数冗余。

以上两种压缩编码方法利用了视频在时空两个维度上的冗余，统称为预测编码。

还有一种重要的编码手段称为变换编码，用于消除图像的频域冗余。因为人眼对于图像中细微波动的细节并不敏感，所以可以想办法在人眼可接受的前提下，将一部分细节去除掉。方法是通过离散余弦变换（DCT）将直观上按照像素的空间位置排列的图像，转换为按照空间频率排列的频谱图。频谱图的特点是绝大多数信息都集中在一个角落里，而远离这些角落的零散信息，就是人眼难以感知到的冗余。

这一阶段的压缩效率如何呢？比如说对于一个8×8的原始图像，变换前需要保存所有64个像素，但变换后可能只剩下角落里的10个点比较重要（强度比较大），其余54个点都是可以被丢弃的冗余信息，因此只保留那10个重要的点就可以了。保留下来的少数几个点，再经过量化，可以简单理解成四舍五入，使这些频率点变成有限的几个值，实现更进一步的压缩。（注：实际上保留或者丢弃的操作是通过调整量化参数实现的，因为过小的值经过量化后就变成0没有了，相当于被丢弃掉了。）

变换编码/量化之后，还有一种强大的编码手段——熵编码。由于量化得到的值其取值范围是有限的，因此每个值（或多个值构成的元组）都可以看成是一个符号。熵编码的核心思想是根据原始信息中符号的概率分布，设计最佳的编码本，使得出现概率越大的符号编码越短，出现概率越小的符号编码越长。比如26个英文字母里A出现的概率远高于Z，因此A的编码长度要短于Z，才能保证总体编码长度较短。JPEG采用较为简单的哈夫曼编码，它的编码本是固定的。而H.264等视频编码标准采用更灵活的CABAC（上下文自适应算术编码）等方法，它会实时统计各个符号的出现概率，同时更新编码本，达到更高的压缩效率。

最后，为了保证视频信号不出错不失真，还会引入很多抗误码手段，例如校验码、例如灵活宏块排序等等，保证万一数据出错，要么可以通过校验码恢复，要么不会影响到太大的范围。

经过以上各种方法的处理，简单的二进制表示的图像，可以被压缩到原来尺寸的几百乃至上千分之一。无论是放在硬盘上慢慢欣赏，还是用4G流量看实时直播，都毫无压力了。

以上便是图像/视频压缩的大致原理。

------

**2020-06-19**

在《数字图像压缩编码》这本书中，用比较短的篇幅讲清楚了 ITU-T（VCEG）和 ISO/IEC（MPEG）两大门派制订视频编码标准的不同考虑。

ITU-T侧重“视频通信”，对于压缩效率、Codec成本和速度、在不可靠信道上如何靠谱地传输这类问题考虑得多一点。同时比较重视标准的体系化，试图将音视频通信嵌入到 ITU-T 的整个建议书体系中。

MPEG侧重“多媒体应用”，每一代标准都试图建立完整的多媒体应用系统的全套标准，相比于ITU-T，MPEG对于封装、交互、系统集成着墨较多。

- MPEG系标准是VCD/DVD/蓝光这类消费者产品的核心技术，正是其重应用思路的体现。
- ITU-T的G系列是重要的话音编码标准，H.261等标准至今仍在视频会议等场景应用广泛。
- MP3/AAC都是MPEG颠覆业界的杰作，但是这俩标准的理论编码延时都高得惊人，显然不是为了音频通信设计的。
- H.264提出的网络抽象层概念，就是为IP网上的视频通信设计的，充满了通信味儿。相比之下，MPEG从MPEG-1开始就引入了B帧，用术语来说就是非因果的，明摆着就是为了回放而不是实时通信设计的。
- MPEG-4非常超前非常大胆，它提出来的对象编码、视频交互、语义化等方向，即使在现在也很少可以完整实现。

------

**2020-06-20**

B站的视频编码码率控制：

- 离线的码率失真优化
- 引入机器学习预测编码参数
- 内容分类/切分场景
- 控制复杂度和视频上线时间
- 根据用户行为触发更高级的优化

------

**原型特性规划（2020-06-24）**

- 基于YUV420，逐行。
- 不需要条带
- 仅I帧和P帧两种。
- 仅支持8×8的宏块，不支持子块分割。
- 浮点DCT。量化参数？
- 帧内模式5种，同AVS。
- 仅前向参考。如何选择参考帧？
- 运动估计暂且采用1像素精度。
- 简单的游程编码，熵编码先不做。
- 率失真优化：拉格朗日优化，或不做。
- 码率控制：待调研。​​​

------

**2020-05-20**

- 包括子块划分、帧内预测模式、参考帧、量化参数在内的编码模式的决策，一方面是RDO，另一方面是启发式的选择。（参考：知乎文章H.265概念解析[OL]、深入理解视频编解码技术[M]）
- 去块滤波有两类，一类是作为解码后处理的 post filter，另一类是位于编码环路内部的 loop filter. （深入[M] p139）
- 问：为什么帧内预测的输入信号（相邻参考块）不需要去块滤波？猜想：去块滤波是针对整幅图像的，而帧内预测只能基于（一般位于左、上两个已编码方向）已编码的MB（更一般地，分析单元）。
- 问：帧内预测是以已编码块的边界作为预测依据，也就是说当前块的RDO实际上与相邻的每个块的RDO都相关。新的编码标准中是否有考虑到这一点？答：好像是有的。不过就这个问题而言，性价比似乎不高。
- 框出编码器里面的解码器。

## 二 视频编码的基本原理

![颜色降采样，原因可能与锥细胞和杆细胞数量比例有关。\[@中国科普博览20190823微博\]](./image/G3/video-encoding/颜色降采样.jpg)

![此图没有红色](./image/G3/video-encoding/颜色降采样-2.jpg)

![像素采样模式 \[B11\]](./image/G3/video-encoding/像素采样模式.png)

------

**视频信息冗余**

![信源编码示意图](./image/G3/video-encoding/信源编码.png)

上图中，信源编码器的任务是将信号分成两部分：一部分是有关信源性质的大量共性知识；另一部分是剥离掉这些共性知识的少量特性信息。对于感知编码而言，编码器还会有选择地去掉人类无法感知的信息。这样，信号就被分成了**共识**和**特征**两部分，其中“特征”部分的数据量远小于“共识”部分，这样就实现了数据的压缩。在信道上传输的，实际上是信号的“特征”；而“共识”部分，则通过标准和协议的形式，固化在每一个信源编解码器中。

例如，假设已经知道（约定）信源是一个标准的正弦波，那么只需要传输这个正弦波的相位、频率和幅度就足够了，不需要传输每一个采样点。再例如，假设已经知道（约定）信源是一个安防监控探头采集的图像，由于图像绝大多数时间都是固定不动的，因此只需要传输每帧之间的差异、或者画面中运动的内容就足够了。

作为信源编码的一类，视频编码就是从视频信号中剥离掉共性的知识，如时域/空域/频域相关性、编码的上下文特征、分形的纹理特征、特定的语义内容（如人脸、物体、几何形状）等，并（在可接受的前提下）抛弃掉人类难以感知的信息（如精细的色度信息、难以察觉的空域高频成分），只留下少量相关性很低的信息，实现视频信号的压缩。因此，视频编码，可以认为就是视频压缩。

民国时代曾经有一种简写日期的方式叫做“韵目代日”，例如长沙“文夕”大火，指的就是“文日”（1938年11月12日）夜间的大火。将日期缩写为某个单字，以减短电文长度。韵目代日的规则，就是上面所说的“共识”。如今已经几乎没有人懂得这个规则了。

信源编码分为**无损**和**有损**两类。有损编码会损失掉原信号的部分信息，解码后得到的信号并不完全等同于原信号，会有一定的失真。编码器的一个重要任务是，在失真可接受的前提下，尽可能提高数据压缩的效率；或者在数据率可接受的前提下，尽可能减小失真。这是一个带约束的优化问题，称为**码率-失真优化**（Rate-Distortion Optimization）问题。RDO实际上解决的是三方面问题

- 更好的失真度量方法
- 更高效的压缩编码
- 更好的RDO算法本身

信源编码，是对信号的内在性质、乃至这个世界的深刻再理解。

------

**视频编解码框架**

当今应用最广泛的视频编码框架，是基于预测-变换的混合式视频编码框架。它的特点如下：

- 总的来看是一个带反馈的差分预测编码框架
- 框架前向通路中引入了变换编码
- 残差信号经过量化、变换后，再进行熵编码
- 编码器包含完整的解码器

![差分脉冲编码调制（DPCM）框架](./image/G3/video-encoding/DPCM.png)

![视频编码器框图](./image/G3/video-encoding/video-encoder-arch.png)

![H.264的混合编码框架 \[B1\]](./image/G3/video-encoding/H264-framework.png)

------

**编码工具和编码策略**

------

**编码性能评价**

## 三 变换编码与量化

**变换编码的理论基础**

**x.1** 自然的空域图像信号往往有大片的平坦缓变区域，如蓝天、皮肤、白墙等等。也就是说，在图像的局部范围内，像素之间的差异是比较小的。

**x.1.1** 从频域的角度看，**x.1**意味着图像信号中直流和低频成分往往占据大部分能量。

**x.1.2** 从统计的角度看，**x.1**意味着图像信号的相近像素之间有较强的相关性。统计表明，像素间距离越近，相关性越强。

![灰度差值的概率分布 \[B7\]](./image/G3/video-encoding/灰度差值概率分布.png)

**x.2** 我们的目标是，在不损失过多信息的前提下，去除掉空域图像信号的冗余成分，实现信号的压缩。

**x.2.1** 从频域的角度看，根据**x.1.1**，我们可以设法去除能量较低的高频成分，保留能量较高的低频成分。

**x.2.2** 从统计的角度看，根据**x.1.2**，我们可以设法去除图像相近维度之间的相关性。

**x.3** 为了实现**x.2**所述的目标，我们可以通过某种变换，将图像的主要信息集中到少数维度上，这就是变换编码的核心思想。

**x.3.1** 为了实现**x.2.1**，可以将图像信号从空域变换到频域，将信号的能量集中到低频段，并去除高频段能量较少的成分，保留低频段能量较高的成分，达到数据压缩的目的。从1968年，人们[B6]首先使用FFT计算图像的频谱，但FFT是在复数上进行运算，显得有些冗余。后来人们提出了针对实信号的**离散余弦变换**（DCT），它本质上是FFT的导出形式，但是比FFT的能量集中性更好，并且同样具有快速算法。

![DCT比DFT有更好的能量集中性能 \[维基百科DCT词条\]](./image/G3/video-encoding/DFT-vs-DCT.jpg)

**x.3.2** 为了实现**x.2.2**，可以对图像向量作主成分分析，尽可能去除像素间的相关性（至少去除线性相关性），将图像信号降维到少数几个特征值上，达到数据压缩的目的。许多年(?)以前，人们提出**K-L变换**（Karhunen-Loève Transform，KLT），也叫**霍特林变换**（Hotelling Transform），它本质是一种正交线性变换，可以在均方误差最小的条件下实现随机信号的降维。但是，KLT的基底由图像的统计特征（如协方差矩阵）决定，计算复杂度较高且无快速算法，因而KLT只有理论上的意义，实用性很差，一般只用作性能比较的基准。（至于KLT和PCA的关系，可以认为PCA是KLT针对离散变量的特殊情况）

**x.4** 存在多种实用的变换算法，可以达到信号去相关的目的。变换算法需要满足几个特性：①可逆的正交变换；②性能近似KLT；③计算复杂度不能太高。

**x.4.1** 此类变换算法可分为正弦类变换和非正弦类变换。正弦类变换有FFT、DCT、离散正弦变换（DST）等，非正弦类变换有Haar小波变换、Walsh变换等。

**x.4.2** 非正弦类变换计算量较小，但效果较差，而且难以直观解释。正弦类变换尽管计算量较大，但是其效果更接近KLT，并且具有频率域变换的直观含义，因而成为主流的变换编码算法。

**x.5** DCT既是一种具有去相关特性的线性变换，也具有显然的频域分析的物理含义。尽管DCT去相关性能略输于KLT，但它①与输入图像无关，不需要计算输入图像的统计特征，且②有快速算法，并③具有易于理解的物理含义，实用性更强。不过，可以证明，当信号统计特性近似于一阶马尔可夫过程时，DCT的去相关性近似于KLT。

**x.5.1** 依据构造形式不同，DCT有8种变形，常用的是DCT-II，其反变换可以使用DCT-III计算得到。

**x.5.2** DCT具有快速算法，可实现$O(n \cdot \mathrm{log}(n))$的时间复杂度。一种是直接使用蝶形结进行递归分解，另一种是通过FFT来计算DCT。

**x.5.3** 二维图像信号的DCT可以通过两次一维DCT实现，即按行/列作一维DCT，再对变换结果按列/行作一维DCT。

**x.5.4** 近年来，随着算力的提升，自适应多核变换AMT[B3-p26]、基于在线训练的KLT[B8-p41]等更为复杂的变换算法被引入新一代的视频编码标准。音频信号由于涉及分帧和边界问题，使用经改进的DCT即MDCT实现变换编码。

**x.6** 理论上，DCT应针对整幅图像进行，以去除所有像素之间的相关性。但实际上都是先将图像分块，对每块单独计算DCT。这样做的原因是：①对整幅图作变换的计算量太大；②如果对整幅图作变换，变换编码传输中引入的误码在反变换后会分散到整幅图像，如果分块则会将误码局限在单个块内；③考虑到1所述的局部缓变性质，多数小块内的高频信息极少，因而对这类平坦小块单独计算DCT可以比对全图计算DCT更充分地去除掉冗余信息。（视频截图）

**x.6.1** 分块编码会导致块与块之间的边界不连续，形成可见的块边缘，这称为**块效应**。

**x.6.2** 根据图像内容自适应地选择块的大小和尺寸，不仅有利于减轻块效应，还有利于提高编码效率。

![更精细的宏块划分有助于提高编码效率 \[C1\]](./image/G3/video-encoding/宏块划分对比.jpg)

**x.7** DCT变换存在反变换（IDCT），变换后的信号经IDCT可以恢复出原始图像信号。但由于变换信号的高频冗余成分经后续的量化环节被压缩掉，恢复出的图像与原始图像并不完全一致，因而变换编码一般都是有损压缩编码。

------

**离散余弦变换**

$$ X[k] = \sum _{n=0} ^{N-1} {x[n] \cdot \mathrm{cos}( \frac{\pi}{N} k (n + \frac{1}{2}) )} $$

![8×8离散余弦变换的基模式 \[B11\]](./image/G3/video-encoding/DCT-basis-patterns.png)

![Zig-zag 排列方式 \[A2\]](./image/G3/video-encoding/zig-zag.png)

------

**H.264的整数变换**

## 四 预测编码

**帧内预测编码**

**帧间预测编码（20190807）**

## 五 熵编码

- CAVLC
- CABAC

## 六 码率-失真优化（RDO）

## 七 视频编码标准

![视频编码标准简史 \[C3\]](./image/G3/video-encoding/history-of-video-coding.png)

![主流视频编码标准发展史 \[B5\]](./image/G3/video-encoding/Video-Encoding-History-2.png)

- MPEG-1/2/4、H.261/2/3
- H.264/AVC、AVS
- H.265/HEVC、H.266/VVC、AVS2、AVS3

**HEVC的改进**

**VVC的改进**

- 块大小：128×128
- 更复杂的子块划分方式
- 67种亮度帧内预测模式
- 仿射运动矢量
- 自适应多核变换（AMT）

![图像分割方式的改进 \[C3\]](./image/G3/video-encoding/H266-piction-partitioning.jpg)

**AVS3的改进**

## 八 视频编码的发展趋势

## 参考资料

- 参考资料目录遵循 GB/T 7714-2005《文后参考文献著录规则》的要求编排。
- 如无说明，所有参考文献均有自有备份（纸质、电子文件或截图）。

**A.标准文档**

- A1 / **ITU-T H.264** 视听及多媒体系统/视听业务的基础设施/活动图像编码/通用视听业务的先进视频编码 [S]（H.264/AVC国际标准）
- A2 / **ITU T.81** [信息技术/连续色调静止图像的数字压缩和编码/要求和准则](https://www.w3.org/Graphics/JPEG/itu-t81.pdf) [S]（JPEG标准）
- A3 / **GB/T 33475.2-2016** 信息技术 高效多媒体编码 第2部分：视频 [S]（AVS2国家标准）

**B.论文、胶片、教材和专著**

- B1 / T Wiegand, G J Sullivan, G Bjontegaard, et al. **Overview of the H.264/AVC video coding standard**[J]. IEEE Transactions on Circuits & Systems for Video Technology, 2003, 13(7):560-576.
- B2 / 毕厚杰. **新一代视频压缩编码标准:H.264/AVC**[M]. 人民邮电出版社, 2005.
- B3 / G J Sullivan. **Versatile Video Coding – The Next-Generation Video Standard of the Joint Video Experts Team**[R]. 2018.
- B4 / 中科大《多媒体通信》课程讲义[R]
- B5 / 马思伟. **新一代AVS3视频编码标准**[R]. RTC2019.
- B6 / H C Andrews, W K Pratt. **Fourier transform coding of images**[?/找不到原始文章]. Hawaii International Conf. on System Sciences, pp. 677-679, 1968.
- B7 / 张春田, 苏育挺, 张静. **数字图像压缩编码**[M]. 清华大学出版社, 2006.
- B8 / 马思伟. **视频编码未来简史**[R]. RTC2017.
- B9 / 高文, 赵德斌, 马思伟. **数字视频编码技术原理（第二版）**[M]. 科学出版社, 2018.
- B10 / K R Rao, 金道年, 黄在静. **视频编码全角度详解**[M]. 机械工业出版社, 2017.
- B11 / I E Richardson. **The H.264 Advanced Video Compression Standard (second edition)**[M]. John Wiley & Sons, 2011.
- B12 / J Watkinson. **The MPEG Handbook (second edition)**[M]. Taylor & Francis, 2004.
- B13 / 陈靖, 刘京, 曹喜信. **深入理解视频编解码技术：基于H.264标准及参考模型**[M]. 北京航空航天大学出版社, 2012.

**C.视频、网课、博客**

- C1 / PanasonicBusiness. [**H.264 Compression Technology**](https://www.youtube.com/watch?v=PmoEsPWEdOA). 2013.
- C2 / [LiveVideoStack博客](https://blog.csdn.net/vn9PLgZvnPs1522s82g)
- C3 / Mickaël Raulet. [从HEVC到通用视频编码的下一代视频压缩技术](https://blog.csdn.net/vn9PLgZvnPs1522s82g/article/details/106152333)
- C4 / [雷霄骅博客](https://blog.csdn.net/leixiaohua1020)

# FFmpeg实践

**DVD或其他非AVC格式转码为HEVC**

```
ffmpeg -i "INPUT" -vcodec libx265 -b:v <比特率> -preset ultrafast -acodec aac -b:a 256k "output.mkv"
```

**注意**：输出比特率按照以下规则设定：

- DVD：2000k
- 其他非AVC格式：与原始视频的比特率接近（或略低）。

除非有特殊需要，伴音一律压成256k的AAC。

**AVC转码为HEVC**

**视频合并**

思路是先打包成可以直接合并的TS流，然后将合并后的TS转换成所需的封装格式。

```
ffmpeg -i 1.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 1.ts
ffmpeg -i 2.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 2.ts
ffmpeg -i "concat:1.ts|2.ts" -acodec copy -vcodec copy -absf aac_adtstoasc output.mp4
```

**硬件Codec**

# 音频编码（2019-11-03）

![音频编码效率比较 \[维基百科Opus\]](./image/G3/audio-encoding/音频编码效率比较.svg)

## 基于PCM的语音编码

**G.711**

![A律压扩 \[D2\]](./image/G3/audio-encoding/G711-A-law-pcm.gif)

|输入采样值|A律压扩编码|解码后的采样值|
|-------------------------------------|
|`s0000000abcdx`|`[~s]000abcd`|`s0000000abcd1`|
|`s0000001abcdx`|`[~s]001abcd`|`s0000001abcd1`|
|`s000001abcdxx`|`[~s]010abcd`|`s000001abcd10`|
|`s00001abcdxxx`|`[~s]011abcd`|`s00001abcd100`|
|`s0001abcdxxxx`|`[~s]100abcd`|`s0001abcd1000`|
|`s001abcdxxxxx`|`[~s]101abcd`|`s001abcd10000`|
|`s01abcdxxxxxx`|`[~s]110abcd`|`s01abcd100000`|
|`s1abcdxxxxxxx`|`[~s]111abcd`|`s1abcd1000000`|

XOR 01010101

## 基于CELP的语音编码

![CELP编码器一般结构 \[维基百科CELP\]](./image/G3/audio-encoding/CELP.svg)

**G.729**

## MP3编码

### 如何定义音乐的混乱程度？

撰写于2020年6月29日

从信号特征的角度是可以定义的，并且在音频编码的码率控制（率失真优化）方面有重要的作用。

在MP3标准（ISO/IEC 11172-3）中，使用感知熵（perceptual entropy，PE）来衡量一个音频帧的“混乱程度”。容易理解的是，一段声音越“混乱”，它的信息量就越大，编码这段声音所需的码长（下限）就越长。

MP3编码器在编码一个音频帧之前，会使用心理声学模型计算这一帧的PE。PE代表了编码一帧所需的码长下限，编码器通过预先计算出来的PE来分配码长预算，以维持码率恒定。（当然，是在在CBR模式下）

那么PE是如何计算出来的？简单来说，PE的度量方法基于这样一个假设：认为声音分为tone和noise两类，二者在时域和频域上分别有不同的特征。在时域上，Tone的波形是可预测的（自相关程度很高），而noise不然。在频域上，Noise的频谱是“平坦”的（想象白噪声的频谱）且可预测的，而tone的频谱是由多个孤立峰值构成的。

在MP3标准所使用的第二心理声学模型中，频谱平坦度是通过各频点的几何平均与算术平均之比所度量的。

而PE之所以称为“感知”熵，是因为它充分考虑了人耳的听觉特性。首先，它是在非线性的critical bands上提取频谱；其次，它利用人耳的掩蔽效应和绝对听阈特性，主要是同时掩蔽效应，对计算结果作修正。

越是“混乱”的声音，如语音中的摩擦音、打击乐器发出的声音等，其PE越大，编码所需的比特数越多。一旦编码结果的长度超出了PE的预估，MP3编码器就会从比特预算中取出一些额度。这种码率控制机制是MP3的专利技术之一，不过现在已经过期了。

### 1 概述

MP3音频编码标准，自从上个世纪80年代末提出至今，已经有三十余年的历史了。MP3是第一个针对“宽带”音频的压缩编码标准，在此之前的技术大多是针对语音压缩而提出的。MP3发展至今，已经一统江湖，成为最流行的数字音频压缩标准，没有之一；“MP3”这个词，俨然成为数字音乐的代名词。由于我本人对于音响技术一直很感兴趣，再加上近期知识管理工作中对于MP3文件管理的客观需要，都需要对MP3编解码原理做一些深入的研究，直至实现最基本的编解码器原型。

MP3编码标准所使用的技术，包括子带滤波、心理声学模型、变换编码和熵编码，都是非常具有代表性的技术，横跨众多领域，奠定了众多后续技术的基础。包括AAC、AC-3、MP3Pro等更为先进的音频编解码技术，都与MP3有着类似的思路。因此，掌握MP3标准之后再学习其他更为先进的标准，就轻松多了。

经过这段时间的研究，我的观念有所变化。那就是在个人数据管理工作中，一味追求无损是没有意义的。无损PCM相比于320K的MP3，并不会带来多高的音质提升，却浪费了几倍的空间，实在是不值当。这与高清视频不同：大尺寸、高帧率、高位深，是肉眼能实实在在感受到的。所以，以后不必一味追求PCM无损，省下来的空间多保存一些4K乃至8K的超高清视频，这是更划算的。

全称 MPEG-1 Layer Ⅲ，对应国际标准为 ISO/IEC 11172-3。

![MP3编码器框图](./image/G3/audio-encoding/aqua-overview.png)

![MP3量化循环](./image/G3/audio-encoding/aqua-rdo-loop.png)

### 2 心理声学模型


![等响度曲线 \[C2\]](./image/G3/audio-encoding/equal-loudness-curves.png)

![频域（同时）掩蔽，以及NMR、SNR、SMR之间的关系示意 \[A3\]\(由\[B1\]重制\)](./image/G3/audio-encoding/mp3-simultaneous-masking.png)

![时域掩蔽 \[A3\]\(由\[B1\]重制\)](./image/G3/audio-encoding/mp3-temporal-masking.png)


### 3 子带滤波

![分析子带滤波器组的频率特性 [B1]](./image/G3/audio-encoding/mp3-filterbank.png)

分析子带滤波器组，用于编码器。

首先明确输入和输出形式。对于每帧1152采样，划分成36组长度为32点的采样段，将每段**分别**输入滤波器，执行36次32点子带滤波，得到长度为32的滤波后结果。则36个采样段可以得到36组长度为32的滤波后结果。

滤波后结果的每一点对应32个子带之一，所以换一个视角，子带滤波器组的输出结果是：32个长度为36的子带滤波结果，每个结果对应一个频带。

至于子带滤波器，它的信号流图以及算法参数都在11172标准中有定义，直接照抄就可以。

### 4 变换编码

![MDCT使用的四种窗口 [B1]](./image/G3/audio-encoding/mp3-MDCT-windows.png)

![混叠消除的蝴蝶结运算 [A1]](./image/G3/audio-encoding/mp3-aliasing-butterfly.png)

### 5 量化与码率失真优化

+ 量化会引入量化噪声，包括同时噪声，也包括前回声（尤其是长窗口MDCT情况下）。
+ 人耳存在掩蔽效应，凡是低于掩蔽门限的声音，都是无法察觉的。掩蔽信号（Masker）与其某个掩蔽门限之比，称为信掩比（SMR）。
+ 量化位数越多，精度越高，量化噪声越低，即信噪比（SNR）越高，代价是码率开销越大。
+ 因此，对于某个信号，在某个量化精度下，如果其量化噪声恰好低于掩蔽门限，则量化噪声无法察觉，那么此时的量化精度就刚刚好，再提高量化精度也没有感知上的意义了。
+ 定义掩噪比MNR=SNR-SMR（标准如此），显然，如果某个量化位数使得MNR等于0，则此量化位数就是能够不引起可察觉量化失真的最低位数。
+ 编码器通过内部的循环，确定每帧的最佳量化位数，以实现VBR。

量化和熵编码模块是MP3编码的核心之一，具体算法按下不表，这里只需要知道：主控喂给此模块一个576点的频域granule，同时指定编码长度的“预算”，此模块就可以给出granule的二进制编码，并保证其长度不超过预算。

每个granule分配多少码长预算，取决于比特储备的余量，以及其自身的“复杂”程度。具体的分配机制是本文的核心，这个放在后面讲，现在来简单介绍一下，如何衡量granule的“复杂”程度。

音频信号的“复杂”程度是不同的。直观上可以这样理解：例如大镲的声音在各个频率上都有比较强的成分，而钢琴的声音只在离散的几个泛音上比较强，且越往高频越弱，所以编码大镲的声音所耗费的比特数，要多于钢琴。

每个granule的“复杂”程度，由感知熵（PE）衡量。感知熵由编码器的心理声学模型（PAM）计算得到，它衡量一个granule在不产生人耳可闻噪声的前提下，能够压缩到的信息量下限。PE乘以3.1，就是一个granule不产生可闻失真的压缩编码长度下限（单位bit），下文称“最小无噪压缩码长”。

在128kpbs及以上的码率下，常见的音乐信号中只有少数（320kbps下往往不足一成）granule的最小无噪压缩码长比平均码长要大，绝大多数granule的最小无噪压缩码长都不会超过平均码长。

### 6 熵编码

### 7 码流合成

> 20191223：比特储备池派发（make available）的比特数，再加上mean_bits，即为量化循环可用的比特数（max_bits）。
之前一直没搞明白这个因果关系：是比特储备池给量化循环派发比特额度，然后量化循环去利用这个额度，而不是反过来。
然后，量化循环完成后，一般都会剩下一些没有用完的额度，这个剩余的额度就会捐献给比特储备池。
如果一顿操作后，比特储备池还是超过了可用容量，那么就需要给量化完的Granule增加填充比特，这实际上是修改Granule的part23Length，同时比特储备池的容量会相应地减少，恢复到正常容量。

一言以蔽之，比特储备（bit reservoir）机制可以在恒定码率（CBR）的前提下，允许帧长度在一定的范围内弹性地变动。这样，既能保证比特流的码率不变（从而方便地实现定位、裁剪等处理），又可以赋予比较“复杂”的音频帧以足够的编码空间额度，保证编码质量。

如无说明，以下只考虑CBR模式。因为只有CBR模式才需要用到比特储备机制。

MP3比特流是由一连串的帧（frame）构成的。每个帧的结构如下图。

![ ](./image/G3/audio-encoding/mp3-frame.png)

图中Header是帧头，描述了比特率、采样率、声道数等基本信息。之所以每一帧都要带上这些信息，一方面是应用场景决定的，比如流媒体、实时通信场景，一般不会事先约定这些信息。另一方面是对于VBR模式，每一帧的实际码率都可能不一样，因此需要单独声明。

Header的开头是连续12个1组成的同步字。解码器在比特流中搜索同步字，以得到一帧的开头。在某些极端情形下，在不是帧开头的位置上也可能出现连续12个（或以上的）1，这一般是由于辅助信息的格式设计不当，因此11172标准中特别提醒，尽量不要在辅助信息中出现同步字，妨碍帧头的定位。标准规定的哈夫曼码表不会出现连续12个或以上的1。

CRC是循环冗余校验码，可有可无。

Header和CRC在MP3比特流中的位置是固定的，这有利于比特流的定位（同步）。连续两个帧头的间距frameLength为（单位bits）

```
frameLength = (bitRate * 1152) / sampleRate + (paddingSlot * 8)
```

在44100Hz采样率下，计算出来的间距是小数。因此为了保证码率不会因为小数部分的偏差而漂移，需要在个别帧中插入额外的填充字节，以抵消累积的误差。以320kbps/44100Hz举例，按照上面的公式，每帧长度约为1044.9字节，而帧的长度只能是整数个字节[注]。如果采用1044作为帧长度，那么若干个帧累积起来，就会有相当可观的误差——实际码率小于320kbps。因此，每隔几个帧，就有一个长度为1045字节的帧，以抵消每帧0.9字节的短少累积起来的总的码率误差。这与历法置闰非常类似。

> **注**：MPEG-1规定的是帧头之间的「槽」（slot）数相等，且 Layer 3 每个槽长度为1个字节，因此说“帧的长度必须为整数个字节”。

式中1152代表每帧有1152个时域（频域）采样点。这是标准规定的常数，所有编解码算法的参数（例如窗口大小等）都取决于这个数。

每帧1152采样点被分为两个granule（颗粒，或者称子帧，下文直接用英文指称之），每个granule有576个采样点。granule是编码的最小单位。由于音频分帧相当于截断，因此为了避免截断带来的噪声，需要对相邻帧作50%的重叠。使用考虑重叠的改进的DCT（即MDCT）对时域数据进行处理，输入是经重叠的2n个点，输出是n个点。因此MP3设定每帧1152点包含2个576点的granule，无论是子带滤波还是量化、熵编码，都是以granule为最小的编码单位。解码时，可从576点的granule经过IMDCT恢复出1152点的时域帧，经重叠-相加算法即可恢复出分帧前的信号。

由于帧头间隔由码率和采样率决定，因此可以认为是平均帧长。鉴于每帧有2个granule，因此每个granule的平均长度就等于平均帧长的一半，且仅由采样率和码率（以及声道数和是否“置闰”）决定。

实际编码后的granule的码长往往少于平均长度，但也有比平均长度长的情况出现。这与granule自身的性质有关，将在下文介绍。

图中竖直的红线代表 MainData 的起始点，之所以特别标注，是因为这个起始点一般位于本帧帧头的前面，也就是上一帧的位置。MainData起始点距离边信息SideInfo结束位置的向左的偏移量（字节），记录在边信息的main_data_begin字段，其最大值为511，意味着MainData最多偏移511字节。这个偏移量的确定方法，就是本文所讲的比特储备机制。

![ ](./image/G3/audio-encoding/mp3-main-data-begin.png)


### 8 元数据

### 9 LAME使用方法

### 10 开发笔记

**20191217**

> 这次研究MP3编解码算法，从11月初开始到现在已经有一个半月，进度还是很快的。
总结了Scheme解释器开发的经验教训，这次从一开始就有比较客观、宽松的心理预期，做好了打持久战的准备。
比较深刻的感受是自己的工程能力确实不行。越是到精深微妙之处，越感觉自己在驾驭复杂的系统上捉襟见肘、顾此失彼，更遑论优化和规范了。
工程的事情，经验很重要。做过和没做过，完全不一样。MP3编码器这个东西，不是个小东西。dist10编码器部分有1万多行C，自己实现的截至目前大概有2千行有效js代码。客观地说，它门槛不低，一方面是算法上的复杂性，光是信号与系统的知识就足以劝退大多数人；另一方面是工程上的复杂性，因为是底层技术，它本身就是一个库，所以要亲手处理很多细节问题。这都是很大的挑战。
那么做这件事的价值是什么呢，有很多方面。能对一个项目保持一个多月的稳定的学习输入，对我来说算是比较难得，根本原因在于兴趣。一直对音频技术感兴趣。所以第一个价值就是满足好奇心，做一个编码器出来也算是给自己一个交代。第二个价值就是功利方面的，我觉得这东西是足够牛逼，足够硬核，足够写进简历的。并且，音视频这个领域我觉得未来还是会有比较重要的地位。
一点简单的复盘，希望我能坚持做完这件事。

> 昨晚爆肝一夜，把噪声控制循环做完了。
预加重暂时不做，混合块模式不打算做。
用EncSpot观察了一些MP3文件的详细信息，发现几乎所有现有的文件里，长块都占97%以上，短块极少，混合块想必更少了。
验证计划：Hybrid filterbank 是无副作用的模块，其结果是相对值得信赖的。量化循环则比较复杂，也是最不靠谱的部分。所以解码器部分，除了已经基本完成的哈夫曼解码、频谱重排之外，反量化、前端也要做一下，验证MDCT输出的结果在编解码后是否一致就可以了。长块短块都要验证。
还有PAM2，照葫芦画瓢是可以实现的，目前看来没什么实现上的难度。PAM2算是比较紧迫的需求，因为现在对噪声容限的值没有任何直观概念，不利于调试。
比特池（所谓的reservoir）目前不理解，需要研究dist10源码。
scfsi放在最后实现，可以直接抄dist10。重在理解原理。
最后要强调一点就是MVP导向的开发，可视化是一定要有的。代码即文档。

## WAV文件格式

WAVE文件格式是RIFF格式的其中一种，一般用来存储PCM音频数据，也可以存储其他格式。以下是典型的WAV文件布局。

![WAV文件格式](./image/G3/audio-encoding/wav-format.png)

- 标红的部分是固定不变的部分。
- 所有数值字段均为小端模式，即高位字节在高地址，低位字节在低地址。
- 格式块字节数：指的是fmt块（蓝色）除了前两个字段以外的其余部分的字节数，固定为16。
- 音频格式：固定为1，代表PCM。
- 声道数：1为单声道；2为双声道。
- 采样率：指每秒钟PCM采样数。
- 每秒字节数：等于采样率×每个采样的字节数。
- 采样字节数：每个采样的字节数，包含所有声道，等于采样位深×声道数÷8。
- 采样位深：每个声道每个采样的位数，一般为8、16。
- 数据块字节数：即所有PCM数据的总的字节数。
- 0x24处可能有一可选块“fact”，PCM格式中一般不会出现。

## 参考资料

### MP3

**说明**：首先推荐一个MP3资料收集网站，有许多论文和技术资料可供下载：[http://www.mp3-tech.org/]()。以下参考资料中，凡是此网站有收录，或者很容易找到的，不再给出链接；不太容易找到的，将给出链接，方便读者溯源。部分文献资料可联系本文作者索取。

标准文档及其相关解读，标准文档当然是最高依据：

- A1 / **ISO/IEC 11172** - Coding Of Moving Picture And Associated Audio For Digital Storage Media At Up To About 1.5 Mbit/s - Part 3: Audio
- A2 / Pan D . **Tutorial on MPEG/audio compression**[J]. IEEE Multimedia, 1995, 2(2):60-74.
- A3 / Noll, P. [**MPEG digital audio coding**](https://www.csd.uoc.gr/~hy438/lectures/MPEGAudioCoding.pdf)[J]. IEEE Signal Process Mag, 1997, 14(5):59-81.
- A4 / Raissi R. **The theory behind MP3**[J]. MP3’Tech, 2002.

较好的中文论文，原理与实现并重，可供快速入门：

- B1 / 張芷燕. **MP3編碼法之研究與實現**[D]. 国立交通大学（台湾省）, 2002.

有关心理声学和感知编码的文章，可供参考：

- C1 / Painter T, Spanias A. **Perceptual coding of digital audio**[J]. Proceedings of the IEEE, 2000, 88(4): 451-515.
- C2 / Robinson D J M. **The human auditory system**[C]//107th convention of the Audio Engineering Society. 1999: 1-13.

其他：

- https://rarewares.org/

### 语音编码

- D1 / 赵力. **语音信号处理（第3版）**[M]. 机械工业出版社, 2017.
- D2 / [G.711 codec process](http://www.en.voipforo.com/codec/codecs-g711-alaw.php)


# 彩蛋：Lena原图

![ ](./image/G3/Lena原图.jpg)
